## Sampling distribution of slope parameter

X <- rnorm(20)

new_b <- function(X, beta_0, beta_1){
  Y <- beta_0+beta_1*X+rnorm(20)
  mod <- lm(Y~X)
  mod$coefficients[2]
}
bs <- replicate(10000, new_b(X,2,2))
sd(bs)
hist(bs)

"
## Inference about the slope

In @sec-dataGeneration we generated data from a linear model
$$
y = \alpha + \beta x + \varepsilon,\qquad \varepsilon \sim \mathsf{Normal}(0, \sigma^2),
$$
with $\alpha = -3$, $\beta = 0.5$, $\sigma = 2$, and we plan to collect $n = 20$ observations. The following code defines those "true" values and a helper that can simulate a new sample whenever we need it.

```{r}
set.seed(2024)
alpha <- -3
beta <- 0.5
sigma <- 2
n <- 20
x_fixed <- rnorm(n, mean = 0, sd = 4)

simulate_sample <- function() {
  tibble(
    x = x_fixed,
    y = alpha + beta * x + rnorm(n, mean = 0, sd = sigma)
  )
}
```

The $x$ values are drawn once (stored in `x_fixed`) so each simulated dataset reuses the same design points while only the noise term changes.

With a single sample we obtain one set of parameter estimates. The plot below compares the fitted line from that sample to the (unknown) population line.

```{r}
sample_1 <- simulate_sample()
fit_1 <- lm(y ~ x, data = sample_1)

sample_1 %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(intercept = coef(fit_1)[1], slope = coef(fit_1)[2], colour = "#1b9e77", linewidth = 1) +
  geom_abline(intercept = alpha, slope = beta, colour = "#d95f02", linetype = "dashed") +
  labs(
    title = "Sample 1: fitted line vs. population line",
    x = "x",
    y = "y"
  )
```

Generating a *second* sample from exactly the same population already gives a noticeably different least squares line. Tabulating the estimates keeps us focused on how $a$ and $b$ change from sample to sample.

```{r}
sample_2 <- simulate_sample()
fit_2 <- lm(y ~ x, data = sample_2)

bind_rows(
  tibble(sample = "Sample 1", intercept = coef(fit_1)[1], slope = coef(fit_1)[2]),
  tibble(sample = "Sample 2", intercept = coef(fit_2)[1], slope = coef(fit_2)[2])
)
```

If we continue drawing samples we eventually build the *sampling distribution* of our estimators. Running the experiment 50 times produces the following empirical distribution for the slope $b$.

```{r}
set.seed(512)
n_sims <- 50
sampling_results <- map_dfr(
  1:n_sims,
  function(sim_id) {
    fit <- lm(y ~ x, data = simulate_sample())
    tibble(sim = sim_id, intercept = coef(fit)[1], slope = coef(fit)[2])
  }
)

sampling_results %>% 
  ggplot(aes(x = slope)) +
  geom_histogram(binwidth = 0.05, fill = "#377eb8", colour = "white") +
  geom_vline(xintercept = beta, linetype = "dashed", colour = "#d95f02") +
  labs(
    title = "Slope estimates from 50 simulated datasets",
    subtitle = "Dashed line marks the true population slope (Î² = 0.5)",
    x = "Estimated slope (b)",
    y = "Count"
  )
```

-   Repeating this process produces the sampling distribution of our parameters.

-   In the real world we will not have the luxury of generating new samples by running R commands, so we have to make do with the data we have. Thankfully, if our assumptions of the underlying model are true then we can expect the same process of convergence to a stable sampling distribution to hold true and our parameter estimates for $b$ to follow a t-distribution."