---
glossary-include: auto
glossary-sort: yaml
glossary:
  - term: Statistical model
    definition: |
      A probabilistic description of how a response relates to predictors, combining a systematic component with random variation.
    details: |
      **Common form**

      $$Y = f(X) + \varepsilon.$$

      **In R**

      ```r
      # Linear regression (Gaussian errors)
      m_lm <- lm(dist ~ speed, data = cars)

      # Generalised linear model (e.g., logistic regression)
      m_glm <- glm(vs ~ mpg + wt, data = mtcars, family = binomial())

      # Inspect / use fitted models
      summary(m_lm)
      coef(m_lm)
      predict(m_lm, newdata = data.frame(speed = c(10, 20)))
      ```
  - term: Random variable
    definition: |
      A quantity whose value varies across observations, described by a probability distribution.
    details: |
      **In R**

      ```r
      # Simulate random variables (make results reproducible)
      set.seed(1)
      x_norm <- rnorm(100, mean = 0, sd = 1)
      x_unif <- runif(100, min = 0, max = 1)
      x_binom <- rbinom(100, size = 1, prob = 0.3)
      ```
  - term: Distribution
    definition: |
      A description of how the values of a random variable are spread across possible outcomes.
    details: |
      **In R**

      ```r
      # Many distributions use the d/p/q/r pattern:
      # d* = density, p* = CDF, q* = quantile, r* = simulation
      dnorm(0, mean = 0, sd = 1)
      pnorm(0, mean = 0, sd = 1)
      qnorm(0.975, mean = 0, sd = 1)
      rnorm(5, mean = 0, sd = 1)
      ```
  - term: Normal distribution
    definition: |
      A symmetric, bell-shaped distribution often used to model random variation.
    details: |
      **Formula**

      $$X \sim N(\mu, \sigma^2).$$

      **In R**

      ```r
      # Normal N(mu, sigma^2): use mean = mu and sd = sigma
      mu <- 10
      sigma <- 2

      dnorm(10, mean = mu, sd = sigma)
      pnorm(12, mean = mu, sd = sigma)
      qnorm(0.95, mean = mu, sd = sigma)

      set.seed(1)
      rnorm(5, mean = mu, sd = sigma)
      ```
  - term: Variance
    definition: |
      A measure of spread; larger variance means values tend to be further from their mean.
    details: |
      **Formula**

      $$\mathrm{Var}(X) = E[(X - E[X])^2].$$

      **In R**

      ```r
      # Sample variance (uses n - 1 in the denominator)
      x <- c(1, 2, 3, 4, 5)
      var(x)

      # sd(x)^2 gives the same quantity
      sd(x)^2
      ```
  - term: Assumption
    definition: |
      A condition we adopt to justify a model or an inference procedure.
    details: |
      **In this course**

      - For linear regression, key assumptions are about the error term (mean zero, constant variance, normality, independence).
  - term: Response variable
    definition: |
      The variable you aim to explain or predict (sometimes called the outcome or dependent variable).
    details: |
      **In R**

      ```r
      # In a formula like y ~ x1 + x2, the response is on the left-hand side
      fit <- lm(dist ~ speed, data = cars)  # response = dist

      # The response used by a fitted model can be recovered like this
      y <- model.response(model.frame(fit))
      head(y)
      ```
    aliases:
      - Outcome variable
      - Response (outcome) variable
  - term: Predictor variable
    definition: |
      A variable used to explain, adjust for, or predict changes in the response (sometimes called a covariate).
    details: |
      **In R**

      ```r
      # In a formula like y ~ x1 + x2, predictors are on the right-hand side
      fit <- lm(dist ~ speed, data = cars)  # predictor = speed

      # The design matrix contains the (coded) predictors used by the model
      X <- model.matrix(fit)
      head(X)
      ```
    aliases:
      - Covariate
      - Explanatory variable
      - Predictor (explanatory) variable
  - term: Continuous variable
    definition: |
      A numeric variable that can (in principle) take any value on an interval.
    details: |
      **In R**

      ```r
      # Continuous variables are usually stored as numeric vectors
      is.numeric(cars$speed)

      # Quick numeric summaries
      summary(cars$speed)
      ```
  - term: Linear function
    definition: |
      A straight-line relationship between a variable and an outcome.
    details: |
      **Formula**

      $$f(x) = \alpha + \beta x.$$

      **In R**

      ```r
      # A straight-line function
      alpha <- 2
      beta <- 0.5
      f <- function(x) alpha + beta * x

      f(x = c(0, 1, 2))
      ```
  - term: Linear predictor
    definition: |
      The systematic (non-random) part of a regression model that combines predictors and coefficients.
    details: |
      **Formula**

      $$E[Y\mid X] = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Fitted values for the training data
      fitted(fit)[1:5]

      # Predictions for new cases
      predict(fit, newdata = data.frame(speed = c(10, 20)))
      ```
  - term: Intercept
    definition: |
      The baseline level of the response when predictors are at their reference values (often zero).
    details: |
      **In R**

      ```r
      # Included by default
      fit_with_intercept <- lm(dist ~ speed, data = cars)

      # Remove the intercept (forces line through the origin)
      fit_no_intercept <- lm(dist ~ speed - 1, data = cars)

      coef(fit_with_intercept)
      coef(fit_no_intercept)
      ```
  - term: Slope
    definition: |
      The expected change in the response for a one-unit increase in a predictor, holding other predictors constant.
    details: |
      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # The slope is the coefficient on the predictor
      slope_speed <- coef(fit)["speed"]
      slope_speed
      ```
  - term: Random error term
    definition: |
      The part of the response not explained by the predictors (random variation around the systematic part).
    details: |
      **In a model**

      - Often written as $\varepsilon$ in $Y = f(X) + \varepsilon$.
  - term: Mean-zero errors
    definition: |
      The assumption that errors average to zero, so the model is unbiased on average.
    details: |
      **Formula**

      $$E[\varepsilon \mid X] = 0.$$
  - term: Homoscedasticity
    definition: |
      The assumption that the variability of the errors is roughly constant across the predictor range.
    details: |
      **Formula**

      $$\mathrm{Var}(\varepsilon \mid X) = \sigma^2.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Residuals vs fitted values (look for constant spread)
      plot(fit, which = 1)

      # Scale-location plot (another constant-variance check)
      plot(fit, which = 3)
      ```

      **Remedies (when violated)**

      - Transform the response (e.g., log / square-root), or model the mean-variance relationship.
      - Use heteroskedasticity-robust standard errors for inference (where appropriate).
  - term: Heteroskedasticity
    definition: |
      Nonconstant error variance across predictor values (the opposite of homoscedasticity).
    details: |
      **Idea**

      - $\mathrm{Var}(\varepsilon \mid X)$ changes with $X$ (often increasing with the mean).

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Look for a funnel pattern in residuals vs fitted values
      plot(fit, which = 1)

      # Optional formal test (requires lmtest):
      # lmtest::bptest(fit)
      ```

      **Remedies**

      - Transform the response (log / square-root), especially for positive outcomes.
      - Consider weighted least squares if you can model the changing variance.
      - For inference, consider robust standard errors (e.g., `sandwich` + `lmtest`), but still inspect the fit.
    aliases:
      - heteroscedasticity
  - term: Normal errors
    definition: |
      The assumption that errors are approximately normally distributed (mainly important for small-sample inference).
    details: |
      **Formula**

      $$\varepsilon \sim N(0, \sigma^2).$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Q-Q plot of residuals
      plot(fit, which = 2)

      # Alternative base R approach:
      # qqnorm(resid(fit)); qqline(resid(fit))
      ```
  - term: Independence
    definition: |
      The assumption that errors from different observations are not correlated.
    details: |
      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # For time-ordered data: inspect autocorrelation
      acf(resid(fit))

      # Optional Durbin–Watson test (requires lmtest):
      # lmtest::dwtest(fit)
      ```
  - term: Estimation
    definition: |
      The process of using data to infer unknown model parameters (like regression coefficients and error variability).
    details: |
      **In R**

      ```r
      # Fit a linear regression model (estimates coefficients from data)
      fit <- lm(dist ~ speed, data = cars)

      # Extract the estimated coefficients
      coef(fit)
      ```
  - term: Estimate
    definition: |
      A value computed from data that approximates an unknown population quantity (a parameter).
    details: |
      **Notation**

      - Parameters are often written with Greek letters (e.g., $\beta$), and estimates with hats (e.g., $\hat{\beta}$).

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # The estimated coefficients (beta-hats)
      beta_hat <- coef(fit)
      beta_hat
      ```
    aliases:
      - estimate
  - term: Ordinary least squares
    definition: |
      Estimation method that chooses coefficients to minimise the sum of squared residuals.
    details: |
      **Criterion**

      $$\text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # lm() uses OLS (it chooses coefficients that minimise RSS)
      rss <- sum(resid(fit)^2)
      rss
      ```
    aliases:
      - OLS
  - term: Residual sum of squares
    definition: |
      The total squared discrepancy between observed values and fitted values.
    details: |
      **Formula**

      $$\text{RSS} = \sum_{i=1}^n e_i^2.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Compute RSS directly
      sum(resid(fit)^2)

      # For lm objects, deviance() equals RSS
      deviance(fit)
      ```

      **Notes**

      - Notation varies: many texts use `SSE` for this quantity; `SSR` is sometimes reserved for the *regression* sum of squares.
    aliases:
      - RSS
      - SSR
      - Sum of squared residuals
      - residual sum of squares
  - term: Total sum of squares
    definition: |
      The total variability in the response around its mean (a baseline benchmark for model fit).
    details: |
      **Formula**

      $$\text{TSS} = \sum_{i=1}^n (y_i - \bar{y})^2.$$

      **In R**

      ```r
      y <- cars$dist

      # Total variability around the mean
      sum((y - mean(y))^2)
      ```
  - term: Mean squared error
    definition: |
      An estimate of error variance based on residual size (often RSS divided by residual degrees of freedom).
    details: |
      **Formula**

      $$\text{MSE} = \frac{\text{RSS}}{n - p},$$

      where $p$ is the number of estimated parameters (including the intercept).

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # MSE = RSS / residual_df
      rss <- sum(resid(fit)^2)
      mse <- rss / df.residual(fit)
      mse

      # For lm, sigma(fit)^2 is the same value
      sigma(fit)^2
      ```
    aliases:
      - MSE
  - term: Residual standard error
    definition: |
      An estimate of the typical size of residuals (the estimated error standard deviation).
    details: |
      **Formula**

      $$\text{RSE} = \sqrt{\frac{\text{RSS}}{n - p}} = \sqrt{\text{MSE}},$$

      where $p$ is the number of fitted parameters (including the intercept).

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Residual standard error (estimated sigma)
      sigma(fit)
      ```
  - term: Degrees of freedom
    definition: |
      The amount of independent information remaining after estimating model parameters.
    details: |
      **Examples**

      - In simple linear regression (intercept + slope): residual df is $n - 2$.
      - More generally (linear regression): residual df is $n - p$.

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Residual degrees of freedom
      df.residual(fit)
      ```
  - term: Standard error
    definition: |
      An estimate of how much a statistic (like a coefficient) would vary across repeated samples.
    details: |
      **Formula (linear regression)**

      $$\mathrm{SE}(\hat{\beta}_j) = \sqrt{\hat{\sigma}^2\,(X^\top X)^{-1}_{jj}}.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Coefficient standard errors
      summary(fit)$coefficients[, "Std. Error"]

      # Covariance matrix of coefficient estimates
      vcov(fit)
      ```
  - term: t-test
    definition: |
      Tests whether a coefficient differs from zero using a t statistic and an assumed reference distribution.
    details: |
      **Test statistic**

      $$t = \frac{\hat{\beta}_j - 0}{\mathrm{SE}(\hat{\beta}_j)}.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # t values and p-values for coefficients
      summary(fit)$coefficients[, c("t value", "Pr(>|t|)")]
      ```
  - term: F-test
    id: f-test-overall-model-test
    definition: |
      Tests whether predictors jointly improve model fit compared with a simpler model (often intercept-only).
    details: |
      **Common form (overall regression test)**

      $$F = \frac{(\text{TSS} - \text{RSS})/(p - 1)}{\text{RSS}/(n - p)},$$

      where $p$ is the number of fitted parameters (including the intercept).

      **In R**

      ```r
      fit_small <- lm(dist ~ 1, data = cars)     # intercept-only
      fit_big <- lm(dist ~ speed, data = cars)   # add predictor

      # Overall F-test appears in summary() for lm
      summary(fit_big)

      # Compare nested models with anova()
      anova(fit_small, fit_big)
      ```
    aliases:
      - F-test (overall model test)
      - Overall F-test
  - term: p-value
    definition: |
      The probability, under the null hypothesis, of observing a result at least as extreme as the one observed.
    details: |
      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # p-values are part of the coefficient table
      summary(fit)$coefficients[, "Pr(>|t|)"]
      ```
  - term: Confidence interval
    definition: |
      A range of plausible parameter values at a chosen confidence level (e.g., 95%).
    details: |
      **Common form**

      $$\hat{\theta} \pm t^* \cdot \mathrm{SE}(\hat{\theta}).$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Confidence intervals for coefficients (default 95%)
      confint(fit)
      ```
  - term: Prediction interval
    definition: |
      An interval for a future observation at given predictors; wider than a confidence interval.
    details: |
      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      new <- data.frame(speed = c(10, 20))
      predict(fit, newdata = new, interval = "prediction")
      ```
  - term: Fitted value
    definition: |
      The model’s predicted mean response for an observation (given its predictor values).
    details: |
      **Notation**

      - Often written as $\hat{y}_i$.

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Fitted values for the training data
      head(fitted(fit))

      # Fitted value (mean) for new cases
      predict(fit, newdata = data.frame(speed = c(10, 20)))
      ```
    aliases:
      - Predicted value
  - term: Residual
    definition: |
      The difference between an observed value and the value predicted by the fitted model.
    details: |
      **Formula**

      $$e_i = y_i - \hat{y}_i.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Residuals for the training data
      head(resid(fit))
      ```
  - term: Simple linear model
    definition: |
      A model that relates a response to a single predictor using a straight-line mean function plus random error.
    details: |
      **Formula**

      $$Y = \alpha + \beta x + \varepsilon.$$

      **In R**

      ```r
      # Fit a simple linear model (one predictor)
      fit <- lm(dist ~ speed, data = cars)
      summary(fit)
      ```
    aliases:
      - simple linear model
  - term: Simple linear regression
    definition: |
      Linear regression with exactly one predictor (a straight-line relationship on average).
    details: |
      **Formula**

      $$E[Y\mid X] = \beta_0 + \beta_1 X.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Visualise the fitted line
      plot(dist ~ speed, data = cars)
      abline(fit, col = "blue")
      ```
  - term: Multiple linear regression
    definition: |
      Linear regression with two or more predictors, allowing adjustment for multiple variables at once.
    details: |
      **Formula**

      $$E[Y\mid X] = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k.$$

      **In R**

      ```r
      # Two or more predictors on the right-hand side
      fit_mlr <- lm(mpg ~ wt + hp + disp, data = mtcars)
      summary(fit_mlr)
      ```
  - term: Partial regression coefficient
    definition: |
      The effect of a predictor on the response after holding the other predictors constant.
    details: |
      **Idea**

      - Interprets the change in $E[Y\mid X]$ for a one-unit increase in $X_j$, with the other predictors held fixed.

      **In R**

      ```r
      fit_mlr <- lm(mpg ~ wt + hp, data = mtcars)

      # Each coefficient is a partial effect (holding the other predictors fixed)
      coef(fit_mlr)
      ```
  - term: Pearson correlation
    definition: |
      Measures linear association between two variables, ranging from -1 to 1.
    details: |
      **Formula**

      $$r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2\;\sum_{i=1}^n (y_i - \bar{y})^2}}.$$

      **In R**

      ```r
      # Pearson correlation (default)
      cor(mtcars$mpg, mtcars$wt)

      # Test for correlation (with CI)
      cor.test(mtcars$mpg, mtcars$wt)
      ```
  - term: R-squared
    definition: |
      The proportion of response variability explained by the fitted model (on the training data).
    details: |
      **Formula**

      $$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # R-squared
      summary(fit)$r.squared
      ```
  - term: Adjusted R-squared
    definition: |
      R-squared penalised for the number of predictors to discourage unnecessary terms.
    details: |
      **Formula**

      $$\bar{R}^2 = 1 - (1 - R^2)\frac{n - 1}{n - p},$$

      where $p$ is the number of fitted parameters (including the intercept).

      **In R**

      ```r
      fit_mlr <- lm(mpg ~ wt + hp + disp, data = mtcars)

      # Adjusted R-squared
      summary(fit_mlr)$adj.r.squared
      ```
  - term: Polynomial term
    definition: |
      A transformed predictor (like x²) used to capture curvature while keeping the model linear in coefficients.
    details: |
      **In R**

      ```r
      # Add a squared term (still linear in coefficients)
      fit_poly <- lm(mpg ~ wt + I(wt^2), data = mtcars)

      # Or use orthogonal polynomials
      fit_poly2 <- lm(mpg ~ poly(wt, degree = 2), data = mtcars)
      ```
  - term: Interaction
    definition: |
      A model term that allows the effect of one predictor to depend on another predictor.
    details: |
      **In R**

      ```r
      # Interaction only:
      fit_int_only <- lm(mpg ~ wt:hp, data = mtcars)

      # Main effects + interaction:
      fit_int <- lm(mpg ~ wt * hp, data = mtcars)
      ```
  - term: Overfitting
    definition: |
      When a model captures noise in the training data and generalises poorly to new data.
    details: |
      **In practice**

      - Symptoms include overly optimistic fit on training data and poor performance on new data.
      - Prefer validation (train/test split or cross-validation) over “fit on everything and hope”.
  - term: Stepwise regression
    definition: |
      An automated selection approach that adds/removes predictors based on a criterion (often AIC).
    details: |
      **Pitfalls**

      - Can be unstable: small data changes may lead to different selected models.
      - Post-selection $p$-values and confidence intervals can be misleading.

      **In R**

      ```r
      fit_full <- lm(mpg ~ wt + hp + disp + drat + qsec, data = mtcars)

      # Stepwise selection by AIC (automated; use with caution)
      fit_step <- step(fit_full, trace = 0)
      formula(fit_step)
      ```
  - term: Akaike Information Criterion
    definition: |
      Model comparison metric that balances fit and complexity; lower values indicate a preferred model among those compared.
    details: |
      **Formula**

      $$\mathrm{AIC} = -2\log(L) + 2k,$$

      where $k$ is the number of fitted parameters and $L$ is the maximised likelihood.

      **In R**

      ```r
      m1 <- lm(mpg ~ wt, data = mtcars)
      m2 <- lm(mpg ~ wt + hp, data = mtcars)

      # Lower AIC is preferred (among the models compared)
      AIC(m1, m2)
      ```
    aliases:
      - AIC
  - term: Bayesian Information Criterion
    definition: |
      Model comparison metric that penalises complexity more than AIC; lower values indicate a preferred model among those compared.
    details: |
      **Formula**

      $$\mathrm{BIC} = -2\log(L) + k\log(n).$$

      **In R**

      ```r
      m1 <- lm(mpg ~ wt, data = mtcars)
      m2 <- lm(mpg ~ wt + hp, data = mtcars)

      # Lower BIC is preferred (stronger penalty for complexity)
      BIC(m1, m2)
      ```
    aliases:
      - BIC
  - term: Parsimony
    definition: |
      Choosing the simplest adequate model that answers the scientific question.
    details: |
      **In practice**

      - Prefer simpler models unless extra complexity meaningfully improves interpretation or prediction.
  - term: Outlier
    definition: |
      An observation with an unusually large residual relative to the fitted model.
    details: |
      **Diagnostics**

      - Large standardised/studentised residuals suggest an outlying response value *given the predictors*.
      - Rules of thumb (context dependent): $|r_i| > 2$ (flag), $|r_i| > 3$ (strong flag).

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Standardised residuals and studentised residuals
      head(rstandard(fit))
      head(rstudent(fit))
      ```
  - term: Standardised residual
    definition: |
      A residual scaled by its estimated standard deviation to highlight unusually large deviations.
    details: |
      **Formula**

      $$r_i = \frac{e_i}{\hat{\sigma}\sqrt{1 - h_{ii}}}.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Standardised residuals
      rstandard(fit)[1:5]

      # Studentised residuals (often used for outlier detection)
      rstudent(fit)[1:5]
      ```
    aliases:
      - Standardized residual
  - term: Influential point
    definition: |
      An observation that substantially changes fitted coefficients or predictions when removed.
    details: |
      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Influence diagnostics
      cooks.distance(fit)[1:5]
      influence.measures(fit)
      ```
  - term: Leverage
    definition: |
      A measure of how unusual a case’s predictor values are; high leverage can increase influence.
    details: |
      **Formula**

      $$h_{ii} = x_i^\top (X^\top X)^{-1} x_i.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Hat-values are leverages (h_ii)
      hatvalues(fit)[1:5]
      ```

      **Rule of thumb**

      - High leverage often flagged by $h_{ii} > 2p/n$ (or $3p/n$), where $p$ is the number of parameters.
  - term: Cook's distance
    definition: |
      An influence measure combining residual size and leverage to flag points that strongly affect the fit.
    details: |
      **Formula**

      $$D_i = \frac{e_i^2}{p\,\hat{\sigma}^2}\frac{h_{ii}}{(1 - h_{ii})^2}.$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Cook's distance (larger values indicate more influence)
      cd <- cooks.distance(fit)
      head(cd)

      # Rule of thumb often used: 4/n
      4 / nrow(cars)
      ```

      **Rule of thumb**

      - Values above about $4/n$ are often flagged for inspection (not an automatic deletion rule).
  - term: Extrapolation
    definition: |
      Making predictions outside the observed predictor range, where the fitted relationship may not hold.
    details: |
      **In R**

      ```r
      # Compare new predictor values to the observed range
      range(cars$speed)

      new_speed <- c(5, 50)
      new_speed
      ```

      **Practical note**

      - Uncertainty grows quickly as you move away from the data cloud (often reflected in higher leverage).
  - term: Multicollinearity
    definition: |
      Strong correlation among predictors that inflates standard errors and destabilises coefficient estimates.
    details: |
      **Diagnostics**

      - Large standard errors and unstable coefficient signs/magnitudes.
      - High correlations among predictors; large VIFs.
      - Exact multicollinearity shows up as non-estimable coefficients.

      **In R**

      ```r
      fit_mlr <- lm(mpg ~ wt + hp + disp, data = mtcars)

      # Correlations among predictors
      cor(mtcars[, c("wt", "hp", "disp")])

      # Exact aliasing (perfect collinearity)
      alias(fit_mlr)

      # Condition number (rough collinearity diagnostic)
      kappa(model.matrix(fit_mlr))
      ```

      **Remedies**

      - Remove or combine redundant predictors (guided by your scientific question).
      - Centering can help when you include interactions/polynomials (reduces *induced* collinearity), but does not “fix” collinearity in general.
      - If prediction is the goal, consider regularisation (ridge/lasso) or dimension reduction (e.g., principal components).
  - term: Variance inflation factor
    definition: |
      A diagnostic that summarises how collinearity inflates the uncertainty of a coefficient estimate.
    details: |
      **Formula**

      $$\text{VIF}_j = \frac{1}{1 - R_j^2},$$

      where $R_j^2$ comes from regressing predictor $X_j$ on the other predictors.

      **In R**

      ```r
      fit_mlr <- lm(mpg ~ wt + hp + disp, data = mtcars)

      # Optional (if you have the car package):
      # car::vif(fit_mlr)

      # Base R: regress each predictor on the others, then VIF = 1 / (1 - R^2)
      predictors <- c("wt", "hp", "disp")
      vif <- sapply(predictors, function(p) {
        others <- setdiff(predictors, p)
        r2 <- summary(lm(reformulate(others, response = p), data = mtcars))$r.squared
        1 / (1 - r2)
      })
      vif
      ```
    aliases:
      - VIF
  - term: Box-Cox transformation
    definition: |
      A family of power transformations used to stabilise variance or improve linearity for positive responses.
    details: |
      **Formula**

      For $\lambda \neq 0$:

      $$g_\lambda(y) = \frac{y^\lambda - 1}{\lambda}.$$

      For $\lambda = 0$:

      $$g_0(y) = \log(y).$$

      **In R**

      ```r
      fit <- lm(dist ~ speed, data = cars)

      # Box-Cox search (requires MASS; often installed with R)
      # MASS::boxcox(fit)

      # Common special case (lambda = 0): log-transform a positive response
      fit_log <- lm(log(dist) ~ speed, data = cars)
      summary(fit_log)
      ```
  - term: Factor
    definition: |
      A categorical predictor with discrete levels; R stores these as factors.
    details: |
      **In R**

      ```r
      x <- c("low", "medium", "low", "high")
      f <- factor(x)

      levels(f)
      table(f)
      ```
  - term: Dummy variable
    definition: |
      An indicator (0/1) used to code categories in regression relative to a baseline.
    details: |
      **In R**

      ```r
      df <- data.frame(
        y = c(1, 2, 3, 4),
        group = factor(c("A", "A", "B", "B"))
      )

      # Factors expand to dummy/indicator columns in the design matrix
      model.matrix(y ~ group, data = df)
      ```
    aliases:
      - Indicator variable
  - term: Reference level
    definition: |
      The baseline category used to interpret coefficients for a factor.
    details: |
      **In R**

      ```r
      g <- factor(c("control", "treatment", "control"))
      levels(g)

      # Set the reference (baseline) level
      g2 <- relevel(g, ref = "control")
      levels(g2)
      ```
  - term: Contrast
    definition: |
      A coding scheme that maps factor levels to numeric columns (e.g., treatment coding).
    details: |
      **In R**

      ```r
      g <- factor(c("control", "treatment", "treatment"))

      # View or set contrasts (coding)
      contrasts(g) <- contr.treatment(nlevels(g))
      contrasts(g)
      ```
---

# Glossary {#sec-glossary .unnumbered}

::: {#glossary}
:::
