[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Regression and Model Selection with R",
    "section": "",
    "text": "Course Overview",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Linear Regression and Model Selection with R",
    "section": "What you will learn",
    "text": "What you will learn\n\nFormulate simple and multiple linear regression models and articulate their assumptions.\nInterpret regression coefficients, including partial, interaction, and categorical effects.\nEvaluate model utility using hypothesis tests, R-squared metrics, and prediction accuracy.\nIdentify when additive models fail and implement interaction or polynomial terms.\nIncorporate qualitative predictors using indicator (dummy) variables.\nApply principles of parsimony and evidence-based model building.\nUse AIC, multicollinearity diagnostics, and stepwise procedures for model selection.\nPerform residual analysis, identify influential observations, and apply transformations.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Linear Regression and Model Selection with R",
    "section": "How to use this book",
    "text": "How to use this book\n\nEach chapter corresponds to a module in the syllabus and ends with short practice prompts.\nR examples use base lm() and standard diagnostics; you can run code chunks directly if you enable execution.\nThe Assessments chapter outlines optional exercises and a capstone project that integrate exploration, model selection, and diagnostics.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "module00-lm.html",
    "href": "module00-lm.html",
    "title": "1  Introduction to linear models",
    "section": "",
    "text": "1.1 Statistical models\nA central aim of statistical modelling is to understand how one variable changes in relation to others. In your own work, these variables will have concrete meaning - perhaps plant growth, reaction time, exam score, or income - but for now we will simply call them \\(x\\) and \\(Y\\).\nIn regression, we choose one variable \\(Y\\) to treat as the outcome we want to explain or predict, and \\(x\\) as one or more predictors. Our goal is to describe how changes in \\(x\\) are associated with changes in \\(Y\\).\nA simple way to express this idea is\n\\[Y = f(x)\\]\nmeaning that the value of \\(Y\\) can be described by some function of \\(x\\). If we knew this function exactly, and if the world behaved perfectly, then knowing \\(x\\) would tell us everything about \\(Y\\). Many physical laws look like this—for example, \\(E = mc^2\\)—but real data rarely follow a perfectly deterministic relationship.\nIn practice, even when \\(x\\) is held constant, repeated observations of \\(Y\\) will vary. People respond differently, instruments fluctuate, biological systems are noisy, and experimental conditions change. To recognise this, statistical models include a random error term:\n\\[Y = f(x) + \\varepsilon\\].\nHere, \\(\\varepsilon\\) represents natural variability: the part of \\(Y\\) that our model does not or cannot explain.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to linear models</span>"
    ]
  },
  {
    "objectID": "module00-lm.html#linear-prediction",
    "href": "module00-lm.html#linear-prediction",
    "title": "1  Introduction to linear models",
    "section": "1.2 Linear prediction",
    "text": "1.2 Linear prediction\nTo make our model concrete, we need to choose a form for the function \\(f(x)\\). A natural starting point—because it is simple, interpretable, and surprisingly powerful—is a linear function:\n\\[ f(x) = \\alpha + \\beta x \\].\nThis allows us to describe the expected value of \\(Y\\) as\n\\[ E[Y] = \\alpha + \\beta x \\]\nThis is the familiar straight-line relationship:\n- \\(\\alpha\\) is the point where the line meets the vertical axis, and\n- \\(\\beta\\) is the slope, describing how we expect \\(Y\\) to change when \\(x\\) increases by one unit.\n\n\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β)\"})\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (α)\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\nPlot.plot({\n  x:{domain: [-10,10], label: \"x\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`E[Y] = ${b0} + ${b1}x`\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption 1Y and x have a linear relationship\n\n\n\nContinue\n\n\nExample 1: Salary growth over timeSuppose you have received a job offer from Company A, and you want to predict your salary after working there for 10 years. You are told that the average starting salary at this company is $50,000, and that salaries increase by $5,000 per year of employment.\nWe can represent this relationship using a simple linear predictor. For an employee with \\(x\\) years at the company, the expected salary is\n\\[\nE[Y] = 50{,}000 + 5{,}000 \\cdot x.\n\\]\n\nPlotCode\n\n\n\n\n\n\n\nExpected salary at Company A as a function of years employed.\n\n\n\n\n\n\n\nggplot() +\n  geom_abline(intercept = 5e4, slope = 5e3, colour = '#2196F3') +\n  lims(x = c(0, 15), y = c(4e4, 13e4)) +\n  labs(x = \"Years Employed\", y = \"Expected Salary ($)\")\n\n\n\n\nAfter 10 years of employment (\\(x = 10\\)), our linear predictor gives\n\\[\nE[Y] = 50{,}000 + 5{,}000 \\times 10 = 100{,}000.\n\\]\n\n\n\n\nExercise 1: A competing offerA second company also offers you a position. Their starting salary is higher—$70,000 on average—but their yearly pay increases are smaller. Employees who have been at the company for 6 years earn, on average, $18,000 more than when they started.\nWe model expected salary after \\(x\\) years as:\n\\[\nE[Y] = \\alpha + \\beta x.\n\\]\n\n1.2.0.1 Choosing parameters\n\nThe starting salary gives \\(\\alpha = 70{,}000\\).\nThe 6-year increase gives \\(6\\beta = 18{,}000\\), so \\(\\beta = 3{,}000\\).\n\nAssign these values in R:\n\n\n\n\n\n\n\n\n\n\n\nalpha &lt;- 70000\nbeta &lt;- 3000\nalpha &lt;- 70000\nbeta &lt;- 3000\n\n\n\n\n\n\n\n\n\n1.2.0.2 Linear prediction\nThus the linear predictor for Company B is\n\\[\nE[Y] = 70{,}000 + 3{,}000 \\cdot x.\n\\]\nBelow is a plot comparing salary trends for both companies:\n\nPlotCode\n\n\n\n\n\n\n\nLinear salary trends for two companies.\n\n\n\n\n\n\n\nggplot() +\n  geom_abline(aes(intercept = 7e4, slope = 3e3, colour = \"Company B\")) +\n  geom_abline(aes(intercept = 5e4, slope = 5e3, colour = \"Company A\")) +\n  lims(x = c(0, 15), y = c(4e4, 13e4)) +\n  labs(\n    x = \"Years Employed\",\n    y = \"Expected Salary ($)\",\n    colour = \"Company\"\n  ) +\n  scale_color_manual(values = c(\"Company B\" = \"#4CAF50\", \"Company A\" = \"#2196F3\"))\n\n\n\n\nNow compute the expected salary after 10 years:\n\n\n\n\n\n\n\n\n\n\n\nE_Y &lt;- alpha + (beta * 10)\nE_Y &lt;- alpha + (beta * 10)\n\n\n\n\n\n\n\n\n\n1.2.0.3 Using R functions\nEvaluating the expression in R:\n\n\n\n\n\n\n\n\nThis matches the calculation:\n\\[\nE[Y] = 70{,}000 + 3{,}000 \\times 10 = 100{,}000.\n\\]\nNow we can turn this into a reusable function:\n\n\n\n\n\n\n\n\nPredict salaries for these employment durations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimple_linear_prediction(X)\nsimple_linear_prediction(X)\n\n\n\n\n\n\n\n\nGood work!\nNext we will extend our linear predictor to form a full linear statistical model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to linear models</span>"
    ]
  },
  {
    "objectID": "module00-lm.html#random-errors",
    "href": "module00-lm.html#random-errors",
    "title": "1  Introduction to linear models",
    "section": "1.3 Random Errors",
    "text": "1.3 Random Errors\n\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(x\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line.\nAs mentioned in Section 1.x, to account for this variation we add an error term, \\(\\varepsilon\\), to our model\n\n\\[\nY=\\alpha + \\beta x + \\varepsilon\n\\]\n\n1.3.1 Mean Zero\n\nThe error term represents the *difference between the actual value of \\(Y\\) and the value predicted by the linear predictor, \\(E[Y]\\).\nOn average, we expect these error terms to balance out:\n\n\nAssumption 2The mean of the error term is zero:\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\ni.e. The linear predictor gives the correct value of \\(Y\\) on average\n\n\n\n\n1.3.2 Constant variance\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The amount of spread is measured by the variance of the errors.\nWe assume that this variance is constant - like \\(mu=0\\), it is the same for all values of \\(x\\) however we dont specify which particular value it takes:\n\n\nAssumption 3The variance of the error term is constant for all values of x: \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\n\n\n1.3.3 Normal distribution\nWhile the assumptions of mean 0 and constant variance describe the center and spread of the errors, they don’t fully specify the shape of their distribution. To model this more completely, we often assume that the errors follow a Normal distribution.\n\nAssumption 4The error is normally distributed (with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\) . \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\n\n\nviewof mu = Inputs.range([-5, 5], {\n  value: 0,\n  step: 0.1,\n  label: `Mean (μ):`\n})\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: 'Standard deviation (σ):'\n})\n\nSQRT2PI=Math.sqrt(2 * Math.PI)\n\nnormalDensity = (x, mean, sd) =&gt;\n  (1 / (sd * SQRT2PI)) * Math.exp(-0.5 * ((x - mean) / sd) ** 2);\n  \ndensityGrid_1 = d3.range((-4 * sigma)+mu, mu+(4 * sigma), sigma / 50).map(x =&gt; ({\n  x,\n  density: normalDensity(x,mu,sigma)\n}));\n\ntex.block`\\varepsilon \\sim \\text{Normal}(${mu}, ${sigma}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: \"ε\" },\n  marks: [\n    Plot.areaY(densityGrid_1, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.1: Normal distribution with adjustable mean and standard deviation.\n\n\n\n\n\n\n\n\nExample 2: Variation in salaryLets return to our simple linear model of salary at ~company A~,\n\\[\nE[Salary] = 50,000 + 5,000\\times Years\n\\]\nThis expresses how salary tends to increase with experience (i.e. on averaage). But in practice, not every employee with the same number of years earns the same amount — some earn a bit more, some a bit less. Suppose we know that most employees — about two-thirds of them — earn within roughly $4,000 of the average salary for their experience level. In other words, if the average salary after five years is $75,000, then about two-thirds of employees earn between $71,000 and $79,000, and almost everyone (about 95%) earns between $67,000 and $83,000.\nWe can capture this variability with a random error term, \\(\\varepsilon\\), assumed to follow a Normal distribution with mean 0 and standard deviation \\(\\sigma = 4,000\\).\n\\[\nSalary = 50,000 + 5,000\\times Years + \\varepsilon, \\quad{\\varepsilon \\sim \\mathcal{N}(0,4000^2)}\n\\]\nThis means that for a given number of years \\(x\\): - The expected salary is \\(50,000 + 5,000\\cdot x\\) - Actual salaries will vary around that average, typically within about ±$4,000\nFor example, after 5 years (x=5): \\[\nE[Y]=\\$50,000 + \\$5,000 \\times 5 = \\$75,000\n\\]\nThe distribution of salaries for employees with 5 years’ experience is\n\\[\nY\\sim \\mathcal{N}(75,000, 4,000^2)\n\\]\nSince we have a probability distribution over \\(Y\\), we can use R to evaluate the probability of any given salary after x years at the company.\nFor example, we want to know if employed at company A, what is the probabity after working there for 10 years I will have a salary of at least $110,000?\nFirst lets calculate the average salary after 10 years\n\n50000+(5000*10)\n\n[1] 1e+05\n\n\n$100,000.\nNow\n\npnorm(11e4,1e5, 4e3, lower.tail = FALSE)\n\n[1] 0.006209665\n\n\nThe following diagram tells us roughly the probability of observing a \\(Y\\) value in the given range:\n\n\nmu_salary = 75000\nsigma_salary = 4000\n\nviewof k = Inputs.range([0, 3], {step: 0.1, value: 1, label: \"Half-width k (so interval is μ ± k·σ)\"})\nviewof offset_z = Inputs.range([-5, 5], {step: 0.1, value: 0, label: \"Centre offset (in σ units)\"})\n\ncentre = mu_salary + offset_z * sigma_salary\na = centre - k * sigma_salary\nb = centre + k * sigma_salary\n\n// --- Numerical helpers ---\nerf = x =&gt; {\n  const a1 = 0.254829592, a2 = -0.284496736, a3 = 1.421413741,\n  a4 = -1.453152027, a5 = 1.061405429, p = 0.3275911;\n  const sign = Math.sign(x) || 1;\n  x = Math.abs(x);\n  const t = 1 / (1 + p * x);\n  const y = 1 - (((((a5*t + a4)*t + a3)*t + a2)*t + a1)*t) * Math.exp(-x*x);\n  return sign * y;\n}\nnormalCDF = (x, mean, sd) =&gt; 0.5 * (1 + erf((x - mean) / (sd * Math.SQRT2)))\n\n// Probability mass between a and b\nprob = Math.max(0, Math.min(1, normalCDF(b, mu_salary, sigma_salary) - normalCDF(a, mu_salary, sigma_salary)))\n\n// Density grid and shaded interval\ndensityGrid_salary = d3.range(mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary, sigma_salary/100)\n.map(y =&gt; ({ y, density: normalDensity(y, mu_salary, sigma_salary) }))\n\nshaded = densityGrid_salary.filter(d =&gt; d.y &gt;= a && d.y &lt;= b)\n\n// Display text summary\ntex.block`P(${Math.round(a).toLocaleString()} \\le Y \\le ${Math.round(b).toLocaleString()}) = ${prob.toFixed(3)} \\;\\;(\\approx ${(prob*100).toFixed(1)}\\%)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 300,\n  marginLeft: 56,\n  marginBottom: 40,\n  x: { label: \"Salary ($)\", grid: true, domain: [mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary] },\n  y: { label: \"Density\",  },\n  marks: [\n    // Full curve (light)\n    Plot.areaY(densityGrid_salary, {x:\"y\", y:\"density\", fill:\"#2a5599\", fillOpacity:0.12, stroke:\"#2a5599\"}),\n    // Shaded probability region\n    Plot.areaY(shaded, {x:\"y\", y:\"density\", fill:\"#FFD54F\", fillOpacity:0.35}),\n    // Vertical rules\n    Plot.ruleX([mu_salary], {stroke: \"black\", strokeDash: [4,4]}),\n    Plot.ruleX([a], {y1:0, y2:normalDensity(a, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    Plot.ruleX([b], {y1:0, y2:normalDensity(b, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    // Baseline\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1.2: Probability of salary falling within a chosen interval.\n\n\n\n\n\n\n\nHere \\(\\varepsilon\\) represents random deviations from the expected (average) salary for a given number of years x. - Same model as above - given standard deviation - Example Y values - Illustrate error with normal overay - Excersises: - which variance is larger? - (hard) probability of finding E[Y]+2sd observation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to linear models</span>"
    ]
  },
  {
    "objectID": "module00-lm.html#sec-simple_linear_model",
    "href": "module00-lm.html#sec-simple_linear_model",
    "title": "1  Introduction to linear models",
    "section": "1.4 The Simple Linear Model",
    "text": "1.4 The Simple Linear Model\nPutting these pieces together we are left with:\n\\[\nY=\\alpha+\\beta x+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\nThis ‘simple linear model’ is the starting place for conducting linear regression - in which we ‘fit’ (i.e. estimate the values of \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma^2\\)) from data.\n\n\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β)\"})\nviewof b0_2 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (α)\"})\nviewof sigma_2 = Inputs.range([0.2, 5], {step: 0.1, value: 2.5, label: \"Std. deviation (σ)\"})\n\nviewof n_cs = Inputs.range([10, 50], {step: 1, value: 50, label: \"Number of cross sections visualised\"})\n\ntex.block`Y = ${b0_2} + ${b1_2}x + \\varepsilon, \\quad \\varepsilon \\sim \\text{Normal}(0, ${sigma_2}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin = -10;\nxMax = 10;\nstep = (xMax - xMin) / (n_cs - 1);\nxSampleValues = d3.range(xMin, xMax + step/2, step);  \n\nySectionValues = d3.range(-10, 10.001, 0.1)\nwidthScale = Math.min(1.8, sigma_2 * 0.9)\n\ndensityCurveData = xSampleValues.flatMap(xVal =&gt; {\n  const mu = b0_2 + b1_2 * xVal;\n  const peakDensity = normalDensity(mu, mu, sigma_2);\n\n  const rightSide = ySectionValues.map(y =&gt; {\n    const density = normalDensity(y, mu, sigma_2);\n    const width = (density / peakDensity) * widthScale;\n    return {x: xVal + width, y, group: xVal};\n  });\n\n  return rightSide\n});\n\ncrossSectionTrendLine = xSampleValues.map(x =&gt; ({\n  x,\n  y: b0_2 + b1_2 * x\n}))\n\nPlot.plot({\n  x: {domain: [-10, 10], label: \"x\", grid: true},\n  y: {domain: [-10, 10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(densityCurveData, {\n      x: \"x\",\n      y: \"y\",\n      z: \"group\",\n      stroke: \"#2a5599\",\n      strokeWidth: 1.5,\n      curve: \"basis\"\n    }),\n    Plot.line(crossSectionTrendLine, {x: \"x\", y: \"y\", stroke: \"black\", strokeWidth: 2})\n  ]\n});\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\nFigure 1.3: Cross-sections of the simple linear model normal error density.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to linear models</span>"
    ]
  },
  {
    "objectID": "module00-lm.html#a-simple-linear-model-in-r",
    "href": "module00-lm.html#a-simple-linear-model-in-r",
    "title": "1  Introduction to linear models",
    "section": "1.5 A simple Linear model in R",
    "text": "1.5 A simple Linear model in R\n\nfinish this section and lead into the next on fitting models to data.\nsimulate observations of Y from a specified linear model\n\n\nto begin, we start with a collection of x values (we might imagine we measure these values in the wild)\n\n\nn &lt;- 100\nX &lt;- runif(n=100, min=0, max= 50)\nhead(X)\n\n[1] 40.805650  5.229895  3.720145 29.186672 12.397257 40.207871\n\n\n\nThis code randomly chooses n = 100 values uniformy at random from the interval \\([0,50]\\).\n\n\nDefine the model\n\nNext, we construct our simple linear model\n\nalpha &lt;- 4\nbeta &lt;- 1.2\nsigma &lt;- 4\n\nsimple_linear_model &lt;- function(X, alpha, beta, sigma) {\n  mu &lt;- alpha + (beta * X) \n  mu + rnorm(length(X), mean = 0, sd = sigma)\n}\n\n\nThis is function takes x values and returns the specified linear function with normally distributed random noise added.\nNow we can simulate observations of \\(Y\\) given our list of \\(x\\) values and out linear model:\n\n\nY &lt;- simple_linear_model(X, alpha, beta, sigma)\nhead(Y)\n\n[1] 55.670859  7.239089 10.029033 34.183736 19.646636 49.764588\n\n\nlets look at the joint distribution of x and Y:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nFigure 1.4: Data generated from the simple linear model \\(Y=4+1.2\\times x + \\varepsilon\\), with \\(\\varepsilon\\sim N(0,16)\\). Dashed line shows E[Y|x] = α + βx\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(X = X, Y = Y)\n\nggplot(df, aes(X, Y)) +\n  geom_point(alpha = 0.7) +\n  geom_abline(intercept = alpha, slope = beta, linetype = \"dashed\") +\n  labs(x = \"x\", y = \"Y\") +\n  theme_minimal()\n\n\n\n\nHeres the same code running in webR - try adjusting some of the parameters (e.g. the number of samples) and running to plot a different random sample!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to linear models</span>"
    ]
  },
  {
    "objectID": "module01-slr.html",
    "href": "module01-slr.html",
    "title": "2  Simple Linear Regression (SLR)",
    "section": "",
    "text": "2.1 When to use SLR",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "module01-slr.html#sec-LinearFit",
    "href": "module01-slr.html#sec-LinearFit",
    "title": "2  Simple Linear Regression (SLR)",
    "section": "2.2 Fitting models to data",
    "text": "2.2 Fitting models to data\nin the previous section, you were given a linear model - we knew the values of \\(\\alpha\\), \\(\\beta\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(x\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) by analysing the data.\nIndeed, at the end of the last session we generated data from a linear model. In preparation for this chapter, we’ve generated data from a similar such linear model. Here it is:\n\nPlotData\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            x          y\n1   3.8261210 -1.9910365\n2   0.1737806 -5.3003933\n3  -0.2337409 -5.0775049\n4  -2.7024000 -4.0923756\n5  -3.6091883 -3.0859508\n6  -4.5933553 -7.8052454\n7  -1.0023777 -0.9789296\n8  -0.9355944 -2.9067503\n9   5.3589767 -3.5705371\n10  0.8407398 -1.4427999\n11 -3.9394018 -5.2743670\n12  3.3378157 -0.7869733\n13  5.7782597 -2.3202003\n14 -2.2901022 -1.6892767\n15 -2.5082864 -3.0424795\n16 -0.5131353  0.7466187\n17  1.5955202 -2.2088350\n18  0.9387156 -3.2383479\n19 -0.8056076 -2.9267959\n20 -1.2136836 -2.0554007\n\n\n\n\n\nyour task for the remainder of the chapter (and in regression generally) is to try an make a (educated) guess about what model produced this data.\n\nContinue\n\n\n2.2.1 Estimating parameters\nWe call the process of trying to guess the parameters in the data generating model (i.e. \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\)), estimation, and our guesses are estimates. To avoid confusion, we’ll denote the estimated intercept by \\(a\\) and the estimated slope by \\(b\\). So, we have\n\\[\na: \\text{Estimated intercept}\n\\] \\[\nb: \\text{Estimated slope}\n\\] \\[\n\\hat{\\sigma}: \\text{Estimated standard deviation}.\n\\] We also define \\(\\hat{Y}\\) as the predicted value of \\(Y\\) given our estimated model: \\[\n\\hat{Y}=a+bx\n\\] Looking at some data, we try to find the values of our parameters that best ‘fit’ its distribution. Adjust the sliders below to find a line that you think fits the data:\n\n\nviewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept (asjust)`})\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})\n\nb0_1 = (-3 - 0.5*b1_1) + b0adj_1\n\ntex.block`\\hat{Y} = ${b0_1.toFixed(2)} + ${b1_1.toFixed(2)}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(linearData.x)\n\nlineData_1 = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\n\nPlot.plot({\n  marks: [\n    Plot.line(lineData_1, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10, 5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nWhat is your best estimate for the values of \\(\\alpha\\) and \\(\\beta\\) that best fit the given data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 \nOk great, so we have an estimate for our line-of-best-fit:\n\ntex.block`\\hat{Y} = ${my_a} + ${my_b}\\cdot x`\n\n\n\n\n\n\nbut what do we mean by ‘best’ fit here? How do we evaluate this estimate and maybe compare it to other estimates?\n\nContinue\n\n\n\n2.2.3 Residuals\nResiduals are the difference between observed values of \\(Y\\), and the value predicted by our estimated linear predictor \\(\\hat{Y}\\). We denote residuals with the letter \\(e\\): \\[\ne=Y-\\hat{Y} = Y - (a + bx).\n\\]\nResiduals, \\(e\\), are similar to the errors, \\(\\varepsilon\\), that we encountered in chapter 1 - but they are distinct. In many situations, we do not know the ‘acutual’ values of \\(\\alpha\\) and \\(\\beta\\) - so we cannot calculate the errors, \\(\\varepsilon= Y- E[Y]= Y-\\alpha + \\beta x\\). However, we do have our estimates, \\(a\\) and \\(b\\), so we can calculate residuals.\nIndeed - calculating residuals gives us a way to assess how well our estimated model fits the data. We can think of the residuals as being the ‘mismatch’ between our estimated linear predictor and each data point. By minimizing the size of residuals (minimisiong the mismatch of our model to the data), we can get a better fit of our line to data.\n\nContinue\n\n\n\n2.2.4 Estimating coefficients\n\n2.2.4.1 Optimising model fit by minimising (squared) residuals\nBelow, the plot now also displays residuals for each data point. Underneath the model equation is the Sum of Squared Residuals (SSR), which gives a measure of the absolute difference between our linear predictor and the observed outcomes.\n\\[\nSSR=\\sum_{i=1}^{n} e_i^2\n\\]\nwhere \\(e_i = Y_i - \\hat{Y}_i\\). This gives us a numerical measure of how well the line fits the data - see how low you can get the SSR by adjusting slope and intercept.\n\n\nviewof b0adj = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept`})\nviewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})\n\nb0 = -3 + b0adj\n\nresidualData = transpose(linearData).map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n  \nSSRes = d3.sum(residualData, d =&gt; d.residual ** 2)\n\ntex.block`\\hat{Y} = ${b0} + ${b1}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sum_{i} e_i^{2} = ${SSRes.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes}),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10,5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nBased on this visualisation, what do you think is the minimum possible \\(SSR\\) achievable?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want you can update your \\(a\\) and \\(b\\) estimates too (otherwise just proceed):\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 The least-squares estimate\nWe call the estimates \\(a\\) and \\(b\\) which minimise the sum of squares the least squares estimates. Some nice mathematics tells us that the least squares esimates for a simple linear model are unique - that is, there is one set of values for \\(a\\) and \\(b\\) which satisfy this property. Moreover, we don’t have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute \\(a\\) and \\(b\\) very efficiently. As a statistical programming language, this is something R does very easily..\n\n\n2.2.6 Fitting a model with the lm() function\nIn R the lm() function computes the least squares estimates \\(a\\) and \\(b\\) for our simple linear model (among other things) in a single command:\n\n\n\n\n\n\n\n\nWe can extract the coefficients from the lm by indexing:\n\n\n\n\n\n\n\n\nor by the coef() function:\n\n\n\n\n\n\n\n\n\nHow do these estimates compare with yours?\n\nLets compare the estimated linear fits graphically:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n2.2.7 Estimating variance\nOnce the line has been fitted (i.e. \\(\\alpha\\) and \\(\\beta\\) have been estimated as \\(a\\) and \\(b\\)), we also have to estimate the distribution of error terms (remember, as per ?sec-varAssumption, the error distribution has a constant variance defined by \\(\\sigma^2\\)). As you might expect, we again utilise the residuals from our least squares linear predictor to estimate \\(\\sigma\\). The spread of the error terms will be estimated by the spread of the residuals around our line of best fit.\nWe can extract the individual residual values from the fitted lm object by indexing (e.g. my_lm$residuals) or with the residuals() function.\n\n\n\n\n\n\n\n\nThe resulting R object is a vector of the residual for each observation. i.e. residuals(lm_1)[[3]] \\(= e_3\\)\n\nExercise 3: Extracting residuals from an lm object\nExtract the residuals from your least squares fitted model lm_1by indexing and assign them to the variable e\n\n\n\n\n\n\n\n\n\n\n\ne &lt;- lm_1$residuals\ne &lt;- lm_1$residuals\n\n\n\n\n\n\n\n\n\n\n2.2.8 Residual Standard Error\nDividing the residual sum of squares by \\(n-2\\) (one degree of freedom lost per fitted coefficient) gives the mean squared error, and taking the square root yields the residual standard deviation (aka. the Residual-Standard-Error) \\[\ns^2 = \\frac{1}{n-2}\\sum_{i=1}^{n} e_i^2= \\frac{SSE}{n-2}, \\quad s=\\sqrt{s^2}.\n\\] Interpreting \\(\\hat \\sigma\\) is often easier on the original \\(y\\) scale: a typical observation falls about \\(\\hat \\sigma\\) units away from the fitted line.\n\\(s^2\\) and \\(s\\) are our estimators for \\(\\sigma^2\\) and \\(\\sigma\\), respectively. ::: Exercise #### Calculate the Residual Standard Error using your e object defined in the previous exercise, calculate the residual standard error of the least squares model (hint - there are 20 observations in the linearData dataset)\n\n\n\n\n\n\n\n\n\n\n\nrse &lt;- sqrt(sum(e^2)/(nrow(linearData)-2))\nrse &lt;- sqrt(sum(e^2)/(nrow(linearData)-2))\n\n\n\n\n\n\n:::\n\n\n2.2.9 Correlation and \\(R^2\\)\nThe Pearson-Correlation coefficient, \\(r\\), measures how strongly \\(x\\) and \\(Y\\) move together along a straight line, taking values between \\(-1\\) (perfect negative linear relationship) and \\(1\\) (perfect positive linear relationship).\nIn simple linear regression with an intercept, the R-Squared value can be written as \\[\nR^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = r^2,\n\\] so \\(R^2\\) captures the proportion of the total variation in \\(Y\\) that is explained by the fitted line.\n\nExercise 4: Correlation and \\(R^2\\) in our example\nCalculate the correlation between x and y, then compute \\(R^2\\) using the residuals from lm_1.\n\n\n\n\n\n\n\n\n\n\n\nr &lt;- cor(linearData$x, linearData$y)\nr_squared &lt;- 1 - sum(e^2) / sum((linearData$y - mean(linearData$y))^2)\nr &lt;- cor(linearData$x, linearData$y)\nr_squared &lt;- 1 - sum(e^2) / sum((linearData$y - mean(linearData$y))^2)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "module01-slr.html#inference-for-slr",
    "href": "module01-slr.html#inference-for-slr",
    "title": "2  Simple Linear Regression (SLR)",
    "section": "2.3 Inference for SLR",
    "text": "2.3 Inference for SLR\nUp to this point, we have fitted a straight-line model to a sample of data, obtaining estimates \\(a\\) and \\(b\\) for the intercept and slope. These estimates describe the pattern visible in the sample, but they do not tell us how closely they reflect the true population parameters \\(\\alpha\\) and \\(\\beta\\). Because sampling introduces randomness, different samples would produce different fitted lines.\nThis raises the central question:\nHow much confidence can we place in the slope and intercept we estimated, and what do they tell us about the true relationship in the population?\nTo answer this, we rely on statistical inference, which quantifies the uncertainty in the estimates and evaluates whether the predictor genuinely influences the response.\nImportantly, the inferential procedures we use rest on the assumptions of the linear regression model:\n\nThe errors \\(\\varepsilon\\) have mean \\(0\\).\nThey have constant variance \\(\\sigma^2\\) (homoscedasticity).\nThey are independent.\nThey are Normally distributed.\n\nThe Normality assumption is what allows us to derive the sampling distributions of \\(a\\) and \\(b\\), leading directly to the t-tests and confidence intervals used for inference. Without these assumptions—especially Normality—the exact forms of these inferential tools would not hold.\n\nNote 1For a broader introduction to statistical inference, see the Inferential Statistics with R short course. If concepts such as the Normal distribution or t-tests feel unfamiliar, please review that material before continuing.\n\n\n\n\n2.3.1 Inference About the Slope, \\(\\beta\\)\nIn simple linear regression, the population relationship is modelled as\n\\[\nY = \\alpha + \\beta x + \\varepsilon.\n\\]\nTo determine whether \\(x\\) is genuinely associated with \\(Y\\), we test:\n\\[\nH_0: \\beta = 0\n\\qquad \\text{vs.} \\qquad\nH_a: \\beta \\ne 0.\n\\]\n\nUnder \\(H_0\\), changes in \\(x\\) do not affect the mean of \\(Y\\) (a change in \\(x\\) will lead to \\(\\beta\\cdot x = 0\\cdot x = 0\\) change in \\(Y\\)).\nUnder \\(H_a\\), there is evidence of a real linear effect (i.e. a change in \\(x\\) will lead to a non-zero change in \\(Y\\)).\n\nBecause the Normality assumption implies that the estimator \\(b\\) has a Normal sampling distribution (and hence a \\(t\\) distribution once \\(\\sigma\\) is estimated), we are able to quantify how unusual our observed slope would be if \\(H_0\\) were correct.\n\nExercise 5: Hypothesis testing revision (p-values)\n\n\n\n2.3.1.1 The t-Test for the Slope\nThe hypothesis test is carried out using the statistic\n\\[\nt = \\frac{b}{\\text{SE}(b)},\n\\]\nwhich follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom.\nInterpretation:\n\nA large value of \\(|t|\\) (small p-value) indicates evidence that \\(\\beta \\ne 0\\).\nA small value of \\(|t|\\) suggests the data are consistent with no linear effect.\n\nThe validity of this test relies on the Normality of the errors, which guarantees that this \\(t\\) statistic follows the appropriate reference distribution.\n\nNote 2While the assumption of Normality is what theoretically grounds the use of the t-test for parameter estimates, SLR is quite robust to violations of this assumption and inferences about our slope parameters may be of interest even with non-normal residual distributions.\n\n\n\nExample 1: t-test for slope\n\n\n\nExercise 6: t-test for slope\n\n\n\n\n2.3.1.2 Confidence Interval for the Slope\nA \\((1-\\alpha)100%\\) confidence interval for \\(\\beta\\) is\n\\[\nb ;\\pm; t_{\\alpha/2,,n-2},\\text{SE}(b).\n\\]\nInterpretation:\n\nAn interval excluding zero indicates a likely genuine relationship.\nAn interval including zero suggests weaker evidence.\n\nConfidence intervals complement hypothesis tests by communicating both the direction and plausible magnitude of the effect.\n\n\n\n2.3.2 Inference About the Response, \\(Y\\)\nOnce we have fitted a regression model, we often want to make statements about the value of the response at a given predictor value \\(x_0\\). There are two distinct quantities of interest:\n\nThe mean (average) response at \\(x_0\\): \\[\n\\mu_Y(x_0) = \\alpha + \\beta x_0.\n\\]\nA new individual response at \\(x_0\\): \\[\nY_{\\text{new}} = \\alpha + \\beta x_0 + \\varepsilon.\n\\]\n\nThese involve different uncertainties, and therefore require different intervals.\n\n2.3.2.1 Confidence Interval for the Mean Response\nLet \\(\\hat{y}_0 = a + b x_0\\) be the fitted value at \\(x_0\\). A confidence interval for the mean response is:\n\\[\n\\hat{y}*0\n;\\pm;\nt*{\\alpha/2,,n-2} ,\ns\\sqrt{\n\\frac{1}{n} +\n\\frac{(x_0 - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n}.\n\\]\nThis interval quantifies uncertainty in the average value of \\(Y\\) for units with predictor value \\(x_0\\).\npredict(fit, newdata = new_point, interval = \"confidence\")\n\n\n2.3.2.2 Prediction Interval for a New Observation\nTo predict an individual outcome at \\(x_0\\), we must include the additional uncertainty from the random error \\(\\varepsilon\\):\n\\[\n\\hat{y}*0\n;\\pm;\nt*{\\alpha/2, , n-2} ,\ns\\sqrt{\n1 +\n\\frac{1}{n} +\n\\frac{(x_0 - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n}.\n\\]\nBecause of the extra “1” term, prediction intervals are always wider than confidence intervals.\npredict(fit, newdata = new_point, interval = \"prediction\")\n\n\n2.3.2.3 Summary\n\nConfidence interval → uncertainty in the expected value at \\(x_0\\)\nPrediction interval → uncertainty in a new outcome at \\(x_0\\)\n\n\n\nContinue\n\nShould I include a (brief) section on residuals diagnostics here or save that for the dedicated chapter??",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html",
    "href": "module02-mlr.html",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "3.1 When to extend SLR\nMost practical applications of linear regression require models more complex than the simple linear regression(SLR) model introduced in Section 1.4. Multiple linear regression (MLR) extends simple linear regression by allowing \\(Y\\) to depend on more than one predictor variable. This enables richer models and allows us to estimate the partial contribution of each predictor while accounting for others.\nSLR is limited to one predictor. MLR becomes appropriate when:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#when-to-extend-slr",
    "href": "module02-mlr.html#when-to-extend-slr",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Multiple factors plausibly influence the response.\nExcluding predictors may bias results.\nYou wish to measure the unique contribution of each predictor while controlling for others.\n\n\nExample 1: Advertising salesLets consider a simple example using the Advertising dataset\n\n\n\n\n\n\n\n\nSuppose that we are statistical consultants hired by a client to investigate the association between advertising and sales of a particular product. The Advertising dataset (which we have imported as ads) consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for two different media: TV, and radio. 1\n\n\n\n\n\n\n\n\nWe might look at each bivariate (i.e. two variables) relationship between sales and advertising expendature separately:\n\n\\(\\text{Sales} \\sim \\text{TV}\\)\\(\\text{Sales}\\sim \\text{Radio}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever we might also look at the multivariate relationship:\n\nColoured Scatter3-D Scatter\n\n\nHere we encode the values of the second predictor (Radio) with colour\n\n\n\n\n\n\n\n\n\n\nIn the case of 3 variables we can also extend our visualisation to 3 dimensions\n\n\n\n\n\n\n\n\n\n\n\nUsing multiple predictors simultaneously gives us more information than using each variable separately - this is a great case for applying multiple regression!\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#the-multiple-linear-regression-model",
    "href": "module02-mlr.html#the-multiple-linear-regression-model",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "3.2 The multiple linear regression model",
    "text": "3.2 The multiple linear regression model\nThe multiple linear model extends the simple linear model from Section 1.4 straightforwardly by adding the extra variables to the deterministic linear predictor, each with its own parameter:\n\nKey-point: The MLR model\\[\nY = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\varepsilon, \\quad \\varepsilon \\sim N(0,\\sigma^2)\n\\]\n\n\nNow, instead of a single predictor \\(x\\), we may have several (\\(k\\)) predictors \\(x_1, x_2, \\ldots, x_k\\), each corresponding to a column in our dataset. We use an index to distinguish these predictors, and each predictor \\(x_i\\) has a corresponding parameter \\(\\beta_i\\) governing its effect on the response. Note that we have updated the intercept parameter \\(\\alpha\\) from the simple linear regression section to \\(\\beta_0\\), because it is simply another parameter in the model!\nThe random error term, \\(\\varepsilon\\), stays the same, so the assumptions of MLR are very similar to those of SLR:\n\nAssumption: Assumptions of the MLR model\n\nA linear predictor, e.g. \\(E[Y]=\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k\\).\nIndependent and identically distributed random error terms that\n\nHave a mean \\(\\mu=0\\), and a constant variance \\(Var(\\varepsilon)=\\sigma^2\\)\nFollow a normal distribution: \\(N(0,\\sigma^2)\\)\n\n\n\n\n\nNote: Multivariate LinearityAlthough this is still a linear model, the term “linear” no longer refers to a literal straight line in two dimensions as it did in simple linear regression. Instead, “linear” means that the model is linear in its parameters: each \\(\\beta_i\\) multiplies a predictor and enters additively. This linear model may live in a high-dimensional predictor space and cannot be visualised as a single line. For example, for the three dimensional data presented above (one outcome: Sales + two predictors: TV, Radio), a linear model may instead look like a plane - the three dimensional equivalent of a line in two dimensions.\n\n\n\n\n\n\n\n\nA further discussion of linearity will take place in ?sec-non_linearity, when we discuss higher order multiple regression models.\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#partial-regression-coefficients-beta_i",
    "href": "module02-mlr.html#partial-regression-coefficients-beta_i",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "3.3 Partial Regression Coefficients, \\(\\beta_i\\)",
    "text": "3.3 Partial Regression Coefficients, \\(\\beta_i\\)\nEven though our model is no longer just a straight line, the slope interpretation from SLR is still relevant in MLR. Now however, \\(\\beta_i\\) describes the expected change in the mean response for a one-unit increase in \\(x_i\\), holding all other predictors constant.\n\nExample 2: Interpreting a partial coefficient, \\(\\beta_i\\)\nWe may propose a model for Sales whereby the baseline sales (when no money is invested in advertising) is \\(\\beta_0=3\\), \\(\\beta_{TV}= 0.1\\) and \\(\\beta_{Radio}= 0.2\\), i.e.  \\[\nE[Sales]= 3 + 0.1\\cdot\\text{TV} + 0.2\\cdot \\text{Radio}\n\\] In this model: * A $1k increase in TV advertising expenditure is associated with an expected 100 unit (0.11000units - the scale of the outcome) increase in sales, holding Radio expenditure constant.  Converely, at a fixed level of TV advertising expenditure, a $1k increase in Radio advertisin expenditure corresponds to an expected 200 unit increase in sales.\n\nContinue\n\nFor example, lets fix one predictor. Say, we have already have invested $5k in Radio, then our linear predictor becomes\n\\[\nE[Sales]= 3 + 0.1\\cdot\\text{TV} + 0.2\\cdot 5= 4 + 0.1\\cdot\\text{TV}\n\\]\ni.e. only one variable (TV) remains (Radio no longer varies but is held constant - contributing \\(0.2\\cdot 5=1\\)k units to \\(E[Y]\\)), so the parameter \\(\\beta_1\\) can be interpreted as the slope of the SLR line:\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Calculating the expected outcome of a multiple regression model\nGiven this model of sales by TV and Radio advertising, \\[\nE[Sales]= 3 + 0.1\\cdot\\text{TV} + 0.2\\cdot \\text{Radio}\n\\]\n$12k has been spent on TV advertising due to a persisting contract. How much should the company invest in Radio advertising so that the expected sales matches their inventory of 8k units?\n\n\n\n\n\n\n\n\n\n\n\n3.3.1 Fitting MLR Models in R\nFitting a multiple linear regression uses the exact same conceptual process from ?sec-leastsquares: residuals are still defined \\(e = Y- \\hat{Y}\\), and minimising the sum of squared residuals provides optimal and unique least squares estimates for the model parameters \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_k\\).\nIn R we use the same lm() function as SLR (these are both ‘linear models’, after all). Now however, the model formula includes both predictors on the RHS, separated by a +:\n\n\n\n\n\n\n\n\n\nNote: R formulas(the first argument, identified by the ~ separating the LHS and RHS)\n\n\nLets visualise our fitted model:\n\n\n\n\n\n\n\n\nWe can extract our fitted model coefficients in the same way:\n\nUsing indexingThe coef() function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: extracting coefficients from an lm model\nUse the the least-squares coefficients from abs mod to complete these partial regression plots\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 broom::tidy()\nWe can also use the tidy() function from the broom package for a nice summary of the relevant inferential statistics for each parameter:\n\n\n\n\n\n\n\n\nThe hypotheses being tested for each coefficient is analagous to that in the SLR case ?sec-beta_inference: \\[\nH_0 : \\beta_i = 0\n\\qquad\\text{vs}\\qquad\nH_a : \\beta_i \\ne 0.\n\\] using t-statistic which takes into account the size of the estimate along with its standard error: \\[\nt_i = \\frac{\\hat\\beta_i}{\\operatorname{SE}(\\hat\\beta_i)}.\n\\]\nThe main difference in calculation here is that this test statistic has degrees of freedom that depend on the number of predictors in the model. Specifically, \\(t_i\\) has \\(n - (k+1)\\) degrees of freedom - where \\(n\\) is the number of observations and \\(k\\) is the number of predictor variables. Note that this is consistent with the df used for SLR.\nThe interpretation of these hypotheses is slightly more nuanced. As pointed out above, the value of \\(\\beta_i\\) here does not represent the relationship between \\(Y\\) and \\(X_i\\) without qualification - but only as the other variables are held constant. Analogously, the hypothesis being tested by \\(t_i\\) is whether \\(X_i\\) is related to to \\(Y\\) - once we account for the other predictors in the model.\n\nKey-point: Interpreting the t-test of \\(\\beta_i\\)\\(t_i\\) is a \\(n-k+1\\) statistic which tests the null hypothesis that \\(\\beta_i=0\\) in the multiple regression model. A significant p-value for this test entails that given the other predictors in the model.\n\n\nBecause of this:\n\nThe significance of a predictor can change when variables are added or removed.\nA small p-value for \\(\\beta_i\\) suggests a meaningful partial effect, not necessarily a marginal (unadjusted) one. ::: Warning ### Multicollinearity\nleast squares is an amazing method for fitting but it has a weakness\ncorrelated predictors can cause unstable parameter estimates\nCheck for correlation using a correlation matrix or using vif\nthis will be adressed more comprehensively in module 6: diagnostics an transformations. :::\n\n\nNote: Multiple t-tests and type I error inflationWhile it is easy to compute t-tests for each model coefficient individually, this is generally not a good way to determine whether the model as a whole is contributing useful information for the prediction of \\(Y\\). While the model above only has two predictors, larger datasets have the potential for dozens, hundreds, or thousands of predictors! If we were to rely on this series of \\(t\\)-tests alone to determine whether we have any useful predictors of the outcome, we would be compounding our chance of making a ‘type I error’ (i.e. concluding there is a effect where there is none) with each test. Instead of performing several small tests, we have reason to want a single global test - one that encompasses all the \\(\\beta\\) parameters.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#the-f-test-for-model-model-usefulness.-and-measures-of-model-fit",
    "href": "module02-mlr.html#the-f-test-for-model-model-usefulness.-and-measures-of-model-fit",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "3.4 The F-Test for model model usefulness. : and measures of model fit",
    "text": "3.4 The F-Test for model model usefulness. : and measures of model fit\nHaving multiple predictor variables expands the scope of the kind of questions we can ask about our linear model. Rather than looking at each coefficient separately, we can ask whether our model as a whole is effective in explaining the outcome \\(Y\\) In other words, is the combined contribution of the predictors enough to conclude that a linear relationship exists?\nFormally, the global hypothesis test is:\n\n\\(H_0\\): all slope coefficients are zero (no linear relationship between \\(Y\\) and the predictors)\n\n\\(H_a\\): at least one slope coefficient is non-zero (the model is useful)\n\nThis shift parallels the move from multiple t-tests to an ANOVA - instead of testing individual “effects,” we examine the overall variance explained by a model.\nThe F-statistic compares:\n\nvariation explained by the model (mean regression sum of squares, MSR),\n\nresidual variation (mean squared error, MSE).\n\nBecause each of these quantities is constructed from squared normal deviations, the ratio\n\\[\nF = \\frac{\\text{MSR}}{\\text{MSE}}\n\\]\nfollows an F-distribution under \\(H_0\\).\nIf the model truly explains some structure in the data, MSR will be noticeably larger than MSE.\n[#^1] The links between ANOVA and linear regression will be futher explored here in ?sec-qualitative_predictors.\n\nExample 3In the life expectancy example, the output might report\nF-statistic: 28.2 on 4 and 44 DF,  p-value: 1.19e-11\nThe very small p-value indicates strong evidence against \\(H_0\\). We conclude that at least one of the predictors (Population, Health, Internet, BirthRate) is useful for predicting life expectancy, and that the model is useful overall.\n\n\n\nExercise 3Given an F-statistic of 12.3 with p = 0.0004, state the null and alternative hypotheses and conclude whether the model is useful at the 5% level.\n\n\n\n3.4.0.1 global statistics with anova()\nWe can obtain the model F-statistic using the anova() function, which reinforces the connection between regression and the ANOVA framework introduced earlier.\n\n\n\n\n\n\n\n\nThe final row of the ANOVA table reports the global F-test for the model.\n\n\n3.4.0.2 global statistics with glance()\nThe glance() function from the broom package also reports the model F-statistic while providing several additional summary measures in one place.\n\n\n\n\n\n\n\n\nThis output includes several global model features besides the global test statistic and p-value.\n\n\n3.4.1 Adjusted \\(R^2\\)\nOnce we have established that the model is useful overall, we can quantify how much of the variation in \\(Y\\) it explains. The measure \\(R^2\\) was introduced in SLR and extends naturally to MLR In multiple regression, \\(R^2\\) plays the same descriptive role, but its interpretation changes subtly because the model now includes several predictors.\n\n\n3.4.2 \\(R^2\\) in Multiple Regression\nWe define \\[\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}},\n\\] exactly as before. The difference lies in what \\(R^2\\) represents:\n\nIn SLR, \\(R^2\\) reflects how well a single predictor explains variation in \\(Y\\).\nIn MLR, \\(R^2\\) reflects the combined explanatory power of all predictors working together.\n\nBecause adding a new predictor can never increase SSE, it follows that \\(R^2\\) can never decrease when more predictors are added, even if the new predictor has little or no real relationship with the response. For this reason, \\(R^2\\) is not reliable for comparing models with different numbers of predictors.\n\n\n3.4.3 Adjusted \\(R^2\\)\nTo address the fact that \\(R^2\\) is overly optimistic in larger models, we use the adjusted coefficient of determination: \\[\nR^2_{\\text{adj}} = 1 -\\frac{\\text{SSE}/(n - k - 1)}{\\text{SST}/(n - 1)}.\n\\]\nAdjusted \\(R^2\\):\n\npenalises the inclusion of additional predictors,\nincreases only when a predictor provides meaningful explanatory value,\nand may decrease when a predictor contributes little or nothing.\n\nThus,\n\n\\(R^2\\) is appropriate as a descriptive measure of how much variation the fitted model explains,\nadjusted \\(R^2\\) is more appropriate for comparing different models, especially those with differing numbers of predictors.\n\nThis completes our introduction to multiple linear regression: we now have a model with several predictors, an interpretation for its coefficients, and tools to judge whether the model is useful and how well it fits the data.\n\n\n3.4.4 Other model fit statistics: logLikelihood, AIC, and BIC\nThe glance() output also includes two additional quantities: AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\nAlthough they appear alongside the F-statistic and \\(R^2\\), they serve a different purpose.\nAIC and BIC are not measures of how well a single model fits the data in an absolute sense.\nInstead, they are designed for comparing multiple competing models, balancing goodness of fit against model complexity. Lower values indicate a preferable trade-off, but only relative comparisons are meaningful.\nBecause AIC and BIC are tools for model selection rather than model assessment, we do not interpret them here. They will be discussed in detail in Module 5 (Principles of Model Building), where they are used to guide decisions about which predictors to include in a regression model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#footnotes",
    "href": "module02-mlr.html#footnotes",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "This exmple is adapted from “Introduction to Statistical Learning (2)” - an excellent (and freely available) text on statistical modelling.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html",
    "href": "module03-model-building.html",
    "title": "4  Model Building and Selection",
    "section": "",
    "text": "4.1 Increasing model complexity to model complex relationships\nIn this module, we explore strategies for building effective regression models. We cover: - Increasing model complexity to capture non-linear relationships and interactions, - Assessing model fit while balancing complexity, - Techniques for selecting parsimonious models.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#increasing-model-complexity-to-model-complex-relationships",
    "href": "module03-model-building.html#increasing-model-complexity-to-model-complex-relationships",
    "title": "4  Model Building and Selection",
    "section": "",
    "text": "4.1.1 Why increase model complexity?\nWe began this course with the simple linear regression model: a single continuous predictor with a straight-line effect. We then expanded to multiple predictors, where each predictor still had a straight-line effect. For reasons that will become clear, these are examples of so-called first-order models:\n\nKey-term: First-order modelA linear regression where each predictor appears only once and to the first power (no squared terms or interactions). For one predictor: \\[\nE[Y] = \\beta_0 + \\beta_1 X.\n\\] With multiple predictors, \\(X_1, X_2, \\dots, X_i\\), this extends to \\[E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_i X_i\\]\n\n\nThis first-order structure is the simplest linear model we can make with \\(X\\). It is often a reasonable starting point, and can go a long way to modeling real-world phenomena (especially in the multiple regression case), but real data are often more complex. Real relationships between predictors and outcomes are often non-linear, or involve interactions between predictors. To capture these more complex relationships, we can increase model complexity by adding ‘higher-order’ and ‘interaction’ terms.\n\nExample 1: Non-linear relationships in data\nFor example, fitting a straight line to the following data is not ideal, as the relationship between \\(X\\) and \\(Y\\) is curved (U-shaped), so there is always part of the relationship the line cannot capture:\n\n\nquadPoints = transpose(quadData)\nquadXDomain = d3.extent(quadData.x)\n\nxRange = d3.extent(quadData.x)\nyRange = d3.extent(quadData.y)\n\nviewof b0adj_1 = Inputs.range([-1,16], {step: 1, label: html`${tex`\\beta_0`}: Intercept `})\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\beta_1`}: Slope`})\n\nb0_1 = b0adj_1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\hat{Y} = ${b0_1.toFixed(0)} + ${b1_1.toFixed(2)}x`\n\n\n\n\n\n\n\nlineData = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\nlineResiduals = quadPoints.map((d) =&gt; {\n  const fit = b0_1 + b1_1 * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\nlineMaxAbsResid = d3.max(lineResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\nPlot.plot({\n  marks: [\n    Plot.link(lineResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / lineMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(quadPoints, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: yRange, label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n4.1.2 Square, Cubic, and Higher-Order Univariate Models\nWe enrich the basic linear model by adding powers of a predictor, allowing the fitted relationship to bend. In the above case, the “U” shape suggests a quadratic (squared) term may be appropriate.\n\nKey-term: Second-order (Quadratic) univariate model\\[E[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2\\]\n\n\n\nExample 2: Seeing how \\(\\beta_2\\) bends the curve\nAdjust the slider for the squared term to see how changing \\(\\beta_2\\) adds curvature to the fitted relationship . Try to find a good fit - can you guess what model generated this data?.\n\n\nviewof b0_quad = Inputs.range([-4, 4], {step: 1, value: 0, label: html`${tex`\\beta_0`}: Intercept`})\nviewof b1_quad = Inputs.range([-3, 3], {step: 1, value: 0, label: html`${tex`\\beta_1`}: Linear term`})\nviewof b2_quad = Inputs.range([-1, 3], {step: 0.05, value: 0, label: html`${tex`\\beta_2`}: Squared term`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\hat{Y} = ${b0_quad.toFixed(0)} + ${b1_quad.toFixed(0)}x + ${b2_quad.toFixed(2)}x^2`\n\n\n\n\n\n\n\nquadCurve = d3.range(quadXDomain[0], quadXDomain[1] + 0.05, 0.05).map((x) =&gt; ({\n  x,\n  y: b0_quad + b1_quad * x + b2_quad * x * x\n}))\n\nquadResiduals = quadPoints.map((d) =&gt; {\n  const fit = b0_quad + b1_quad * d.x + b2_quad * d.x * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\nquadMaxAbsResid = d3.max(quadResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\n\nPlot.plot({\n  height: 320,\n  marginLeft: 50,\n  x: {label: \"Predictor (x)\", domain: xRange},\n  y: {label: \"Outcome (y)\", domain: yRange},\n  marks: [\n    Plot.link(quadResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / quadMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(quadPoints, {\n      x: \"x\",\n      y: \"y\",\n      r: 3,\n      title: (d) =&gt; `x = ${d.x.toFixed(2)}\\ny = ${d.y.toFixed(2)}`,\n    }),\n    Plot.line(quadCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2.1 Third-order (cubic) univariate model\n\\[\nE[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3\n\\]\nallows more complex curvature with two changes in direction (an “S”-shape) that a quadratic term cannot capture.\n\nExample 3: Seeing how \\(\\beta_3\\) twists the curve\nAdjust the slider for the cubic term to see how changing \\(\\beta_3\\) adds an inflection point to the fitted relationship. Can you tune the parameters to recover the curve that generated this data?\n\n\ncubicPoints = transpose(cubicData)\ncubicXDomain = d3.extent(cubicData.x)\n\nviewof b0_cub = Inputs.range([0, 4], {step: 1, value: 2, label: html`${tex`\\beta_0`}: Intercept`})\nviewof b1_cub = Inputs.range([-2, 2], {step: 1, value: -1, label:  html`${tex`\\beta_1`}: Linear term`})\nviewof b2_cub = Inputs.range([-1, 1], {step: 0.1, value: 0.5, label: html`${tex`\\beta_2`}: Squared term`})\nviewof b3_cub = Inputs.range([-0.6, 0.6], {step: 0.1, value: 0.0, label: html`${tex`\\beta_3`}: Cubic term`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\hat{Y} = ${b0_cub.toFixed(1)} + ${b1_cub.toFixed(2)}x + ${b2_cub.toFixed(2)}x^2 + ${b3_cub.toFixed(2)}x^3`\n\n\n\n\n\n\n\ncubicCurve = d3.range(cubicXDomain[0], cubicXDomain[1] + 0.05, 0.05).map((x) =&gt; ({\n  x,\n  y: b0_cub + b1_cub * x + b2_cub * x * x + b3_cub * x * x * x\n}))\n\ncubicResiduals = cubicPoints.map((d) =&gt; {\n  const fit = b0_cub + b1_cub * d.x + b2_cub * d.x * d.x + b3_cub * d.x * d.x * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\ncubicMaxAbsResid = d3.max(cubicResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\ncubicYDomain = d3.extent(cubicData.y)\n\nPlot.plot({\n  height: 320,\n  marginLeft: 50,\n  x: {label: \"Predictor (x)\", domain: cubicXDomain},\n  y: {label: \"Outcome (y)\", domain: cubicYDomain},\n  marks: [\n    Plot.link(cubicResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / cubicMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(cubicPoints, {\n      x: \"x\",\n      y: \"y\",\n      r: 3,\n      title: (d) =&gt; `x = ${d.x.toFixed(2)}\\ny = ${d.y.toFixed(2)}`,\n    }),\n    Plot.line(cubicCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2.2 N-th order univariate model\nWe can extend this process of deriving new terms by adding further powers of \\(X\\) - allowing ius to to fit arbitrarily complex curves: \\[\nE[Y] = \\beta_0 + \\beta_1X +  \\dots + \\beta_nX^n.\n\\]\nNote that each term gets its own parameter (\\(\\beta_i\\)). If we have \\(n\\) data points, a (n-1)^th-order model will have a parameter for each point, meaning it will fit the data perfectly!\n\nExample 4: Fitting higher-order polynomial models\n\nreg= require(\"d3-regression\")\npolyData=transpose(nonlinearData)\nviewof degree = Inputs.range([1, 9], {step: 1, label: \"Model Order\", value: 1})\n\npolyRegression = reg.regressionPoly()\n  .x((d) =&gt; d.x)\n  .y((d) =&gt; d.y)\n  .order(degree)\n  .domain(d3.extent(polyData, (d) =&gt; d.x));\n\npolyCurveRaw = polyRegression(polyData)\npolyCurve = polyCurveRaw.map(([x, y]) =&gt; ({ x, y }))\npolyPredict = (x) =&gt; {\n  if (typeof polyRegression.predict === \"function\") return polyRegression.predict(x);\n  if (polyCurveRaw.coefficients) {\n    return polyCurveRaw.coefficients.reduce((acc, coeff, i) =&gt; acc + coeff * x ** i, 0);\n  }\n  return NaN;\n}\npolyResiduals = polyData.map((d) =&gt; {\n  const fit = polyPredict(d.x);\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\npolyMaxAbsResid = d3.max(polyResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolyTerms = Array.from({length: degree + 1}, (_, k) =&gt; {\n  if (k === 0) return `\\\\beta_{0}`;\n  if (k === 1) return `\\\\beta_{1} x`;\n  return `\\\\beta_{${k}} x^{${k}}`;\n}).join(\" + \");\n\nhtml`&lt;div style=\"text-align:center; margin: 0.5rem 0;\"&gt;${tex`E[Y] = ${polyTerms}`}&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolyYDomain = d3.extent([\n  ...polyData.map((d) =&gt; d.y),\n  ...polyCurve.map((d) =&gt; d.y),\n  ...polyResiduals.map((d) =&gt; d.fit)\n])\n\nPlot.plot({\n  marginLeft: 50,\n  height: 320,\n  x: {label: \"Predictor (x)\"},\n  y: {label: \"Outcome (y)\", domain: polyYDomain},\n  marks: [\n    Plot.link(polyResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / polyMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(polyData, {\n      x: \"x\",\n      y: \"y\",\n      r: 4,\n    }), \n    Plot.line(polyCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2,\n    }),\n  ],\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher-order terms increase the model’s flexibility, but also increase the risk of fitting random noise rather than meaningful structure. We will return to this important issue later in the module.\n\nNote: The meaning of “linear” in linear modelsEven though higher-order models can model nonlinear relationships between the outcome and predictors, they are still linear models because each parameter enters additively. “Linearity” here refers to the parameters, not the shape of the fitted curve.\n\n\n\n\n4.1.2.3 Fitting higher-order univariate models in R\nIn R, higher-order polynomial terms can be included using the I() function to indicate ‘as is’ operations. For example, to fit a quadratic model:\n\nlm(y ~ x + I(x^2), data = nonlinearData)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = nonlinearData)\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n     2.6185      -3.3760       0.8218  \n\n\nor using the poly() function:\n\nlm(y ~ poly(x, 2), data = nonlinearData)\n\n\nCall:\nlm(formula = y ~ poly(x, 2), data = nonlinearData)\n\nCoefficients:\n(Intercept)  poly(x, 2)1  poly(x, 2)2  \n     0.3493       0.3941       3.0208  \n\n\n\nContinue\n\n\n\n\n4.1.3 Interaction Models with Continuous Predictors\nIn ?sec-slr we saw a multiple regression model with two continuous predictors:\n\\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2.\n\\]\nThis again is a first-order model - each predictor appears only once, ‘as is’. Of course, we can add higher-order terms for each predictor separately (e.g., \\(X_1^2\\), \\(X_2^3\\)) to capture curvature in their individual effects as we did above. e.g. A second-order multivariate model with two predictors might look like: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1^2 + \\beta_4X_2^2.\n\\]\nNow however, we can also consider a different form of higher order term: what happens when we multiply predictors together? This gives us an interaction term - a term that combines two (or more) predictors.\n\nKey-term: Interaction termAn interaction is a product of predictors that allows the effect of one predictor to depend on the level of another. For two continuous predictors, the second-order interaction model is: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2.\n\\] If \\(\\beta_3 = 0\\) (i.e. as in the first-order model), predictors act independently; if \\(\\beta_3 \\neq 0\\), the slope for \\(X_1\\) varies with \\(X_2\\) and vice versa.\n\n\n\nExample 5: Fitting an interaction model to the advertising dataset.If we recall our plot of the advertising dataset from ?sec-mlr, our fist-order linear model did not fit the data in particular ways:\n\nads &lt;- read.csv(\"Data/Advertising.csv\")\nads_mod &lt;- lm(Sales ~ TV + Radio, data = ads)\n\n\n\n\n\n\n\nThe plane does not fit the data at the edges - it overestimates sales when either Radio or TV advertising is low (separately), but underestimates sales when both are high. This suggests that the effect of increasing one type of advertising depends on the level of the other type - an interaction effect.\nWe can fit a second-order interaction model to capture this: \\[\nE[Sales] = \\beta_0 + \\beta_1TV + \\beta_2Radio + \\beta_3(TV \\times Radio).\n\\]\nThe fitted interaction model has a charateristic ‘saddle’ shape, which can better capture the data structure:\n\n\n\n\n\n\nThe interaction effect can also be visualised by fixing one predictor and plotting the relationship between the other predictor and the outcome. In this case, we can plot lines representing the relationship between TV advertising and Sales when Radio advertising is low ($0 spent), average (i.e. mean(ads$Radio)= 23.264 thousand dollars ), and high (i.e. max(ads$Radio)= $49.6 thousand dollars):\n\n\n\n\n\n\n\n\n\nWe can see that the relationship between TV advertising and Sales (represented by the slope of the regression lines) is different at different levels of Radio advertising. In particular, as radio advertising increases, the slope of the line increases, indicating that TV advertising has a larger effect on Sales when Radio advertising is also high.\n\n\n\nContinue\n\nWhen our first order model has more than two predictors, we can include interaction terms between any pair (or more) of predictors. For example, with three predictors \\(X_1\\), \\(X_2\\), and \\(X_3\\), a second-order interaction model would include interactions between each pair: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_1X_2 + \\beta_5X_1X_3 + \\beta_6X_2X_3.\n\\]\n\n4.1.3.1 Higher-order interaction models\nWe can also consider higher-order interactions - those that include combinations of three or more predictors. For example, a third-order interaction model with three predictors would include the ‘three-way’ interaction term: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_1X_2 + \\beta_5X_1X_3 + \\beta_6X_2X_3 + \\beta_7X_1X_2X_3.\n\\] The \\(\\beta_7\\) parameter here captures how the interaction between two predictors changes depending on the level of the third predictor.\nAs in the case of higher-order univariate models, we can extend this idea to include both higher-order interaction terms and higher-order univariate terms for each predictor, allowing for very flexible modeling of complex relationships.\nHowever, as before, increasing model complexity in this way raises the risk of overfitting and interpretability challenges, which we will discuss later in this module.\n\n\nContinue\n\n\n\n4.1.3.2 Fitting interaction models in R\nIn R, interaction terms can be included using the * operator in the formula. For example, to fit a second-order interaction model with two predictors:\n\nlm(Y ~ X1*X2*X3, data = mydata)\n\nThis expands to include both main effects and the interaction term. To include only the interaction term without main effects, use the : operator. For example, if we only wanted the interaction between X1 and X2, along with the linear terms for X1, X2, and X3:\n\nlm(Y ~ X1+X2+X3+X1:X2, data = mydata)\n\n\nContinue\n\n\n\n\n4.1.4 Interaction Models with Categorical Predictors\nA categorical variable with \\(k\\) levels is represented by \\(k-1\\) dummy variables: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma_1D_1 + \\dots + \\gamma_{k-1}D_{k-1}.\n\\]\n\nEach \\(\\gamma_j\\) measures the difference between level \\(j\\) and the baseline.\nChanging the baseline changes interpretations but not fitted values.\n\nCategorical predictors may also interact with continuous predictors: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma_1D_1 + \\delta_1(XD_1),\n\\] allowing each group to have its own slope.\n\nNoteWe saw in ?sec-mlr_cat that first order models with categorical predictors are equivalent to the familiar t-tests and ANOVA models from earlier statistics courses. Interaction models with both continuous and categorical predictors are similarly analogous to “ANCOVA” models.\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#assessing-model-fit-and-model-complexity",
    "href": "module03-model-building.html#assessing-model-fit-and-model-complexity",
    "title": "4  Model Building and Selection",
    "section": "4.2 Assessing model fit and model complexity",
    "text": "4.2 Assessing model fit and model complexity\n\n4.2.1 Why not increase model complexity?\nIn the previous section we saw how adding higher-order terms and interactions can help us capture complex relationships in data. However, increasing model complexity is not always beneficial. While more complex models can fit the training data (i.e. the data we use to fit our model) better, they may not generalise well to new data. This is because complex models can start to fit random noise in the training data, rather than the underlying signal. This phenomenon is known as overfitting.\nIn addition, more complex models can be harder to interpret, making it difficult to understand the relationships between predictors and the outcome. They may also be more sensitive to outliers and multicollinearity, leading to unstable parameter estimates.\nA good model is no more complex than necessary to describe the main structure of the data. Therefore, when building regression models, we need to balance the trade-off between model fit and model complexity. In this section, we discuss some common issues that arise with complex models, and metrics for assessing model fit while accounting for complexity.\n\nKey-point: Balancing fit and complexityA good regression model balances: * Adequate fit to the data, * Simplicity and interpretability, * Generalisability to new data.\n\n\n\nContinue\n\n\n\n4.2.2 Fit Metrics\nFit metrics quantify how well a model captures patterns in the data. We have already seen one such metric: the coefficient of determination, \\(R^2\\).\n\nKey-term: Coefficient of Determination (\\(R^2\\))\\[\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n\\] where \\(SS_{res}\\) is the residual sum of squares and \\(SS_{tot}\\) is the total sum of squares. \\(R^2\\) measures the proportion of variance in the outcome explained by the model, ranging from 0 (no explanatory power) to 1 (perfect fit).\n\n\n\\(R^2\\) was introduced in ?sec-slr as a measure of fit for simple linear regression models based soely on the proportion of variance in \\(Y\\) explained by \\(X\\). We can also calculate \\(R^2\\) for multiple regression models, where it measures the proportion of variance in \\(Y\\) explained by all predictors jointly. However, \\(R^2\\) has a key limitation: it always increases (or at least does not decrease) when additional predictors are added to the model, even if those predictors do not meaningfully improve the model’s explanatory power. This can lead to overfitting, where a model appears to fit the training data well but performs poorly on new data - for example, the (n-1)th order polynomial model discussed in ?sec-ho_univariate has A SSres of zero and thus an \\(R^2\\) of 1, but is unlikely to generalise well.\n\n4.2.2.1 Adjusted \\(R^2\\)\nTo address this limitation, we can adjust \\(R^2\\) to penalise the addition of unnecessary predictors. The adjusted \\(R^2\\) introduces a penalty term based on the number of predictors and the sample size, providing a more balanced measure of model fit that accounts for complexity.\n\nKey-term: Adjusted \\(R^2\\)\\[\n\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n - 1}{n - k - 1}\n\\] where \\(n\\) is the sample size and \\(k\\) is the number of predictors. Adjusted \\(R^2\\) can decrease when unnecessary predictors are added, making it a more reliable metric than \\(R^2\\) for model selection. Can be computed in R using summary(lm_model)$adj.r.squared.\n\n\nAdjusted \\(R^2\\) is particulary useful because of it’s standardised scale (0 to 1), and the interpretation of \\(R^2\\) as the proportion of variance explained still holds - now with the added benefit of penalising unnecessary complexity. However, it is important to note that adjusted \\(R^2\\) is just one of many metrics available for assessing model fit while accounting for complexity.\n\n\n4.2.2.2 Information Criteria: AIC, BIC\nAnother common approach to balancing model fit and complexity is to use information criteria, such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These criteria provide a quantitative way to compare models, penalising those with more parameters.\n\nKey-term: Akaike Information Criterion (AIC)\\[\n\\text{AIC} = 2k - 2\\ln(L)\n\\] where \\(k\\) is the number of parameters in the model and \\(L\\) is the likelihood of the model. Lower AIC values indicate a better balance between fit and complexity. Can be computed in R using AIC() function.\n\n\n\nKey-term: Bayesian Information Criterion (BIC)\\[\n\\text{BIC} = \\ln(n)k - 2\\ln(L)\n\\] where \\(n\\) is the sample size, \\(k\\) is the number of parameters, and \\(L\\) is the likelihood of the model. Like AIC, lower BIC values indicate a better balance between fit and complexity. Can be computed in R using BIC() function.\n\n\nBoth AIC and BIC penalise model complexity, but BIC generally imposes a larger penalty for additional parameters, especially in larger samples. The choice between AIC and BIC often depends on the context and goals of the analysis. AIC is generally preferred for predictive accuracy, while BIC is more conservative and favours simpler models.\n\nKey-point: Interpreting AIC and BICUnlike adjusted \\(R^2\\), AIC and BIC do not have a fixed scale, so their absolute values are not interpretable on their own. Instead, they are used to compare models fitted to the same dataset - the model with the lowest AIC or BIC is considered the best among the candidates.\n\n\n\n\n4.2.2.3 Calculating fit metrics with broom::glance()\nIn R, we can calculate adjusted \\(R^2\\), AIC, and BIC using the broom package’s glance() function, which provides a tidy summary of model fit statistics. For example, using our fitted 2nd-order interaction model of the advertising dataset:\n\nlibrary(broom)\nglance(ads_mod_int)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.968         0.967 0.944     1963. 6.68e-146     3  -270.  550.  567.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNote that glance() returns a data frame with the mentioned fit metrics, along with other useful statistics such as the number of observations (nobs), residual standard error (sigma) and the global F-statistic (statistic). Alongside tidy(), glance() is a powerful tool for summarising and comparing regression models in R.\n\n\n4.2.2.4 Comparing model fit\nWhen comparing multiple models fitted to the same dataset, we can use adjusted \\(R^2\\), AIC, and BIC to assess which model provides the best balance between fit and complexity: the key points to remember are:\n\nHigher adjusted \\(R^2\\) values indicate better fit.\nLower AIC and BIC values indicate better fit.\n\n\nExample 6: Comparing first-order and interaction modelsLets continue our investigation of the advertising dataset from ?sec-mlr by comparing our first-order model of Sales (ads_mod) to our second-order interaction model (ads_mod_int).\nLets combine the fit metrics from both models into a single table for easy comparison:\n\nTable of fit metricsCode\n\n\n\n\n# A tibble: 2 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n*     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.897         0.896 1.68       860. 4.83e- 98     2  -386.  780.  794.\n2     0.968         0.967 0.944     1963. 6.68e-146     3  -270.  550.  567.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nads_fit_glance &lt;- rbind(\n  glance(ads_mod) |&gt; `rownames&lt;-`(\"First-order model\"),\n  glance(ads_mod_int) |&gt; `rownames&lt;-`(\"Interaction model\"))\n\nads_fit_glance\n\n\n\n\nSince the interaction model ?eq-ads_int differs from the first-order model ?eq-ads_mod by the addition of only the \\(TV \\times Radio\\) interaction term, we can interpret the change in fit metrics as the change in model fit due to adding this term. From the table above, we can see that the interaction model has a higher adjusted \\(R^2\\) and lower AIC and BIC values compared to the first-order model. This indicates that adding the interaction term improves the model’s fit to the data, even after accounting for the increased complexity.\nMoreover, we can test the hypothesis that the interaction term significantly improves model fit using an ANOVA comparison:\n\nanova(ads_mod, ads_mod_int)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ TV + Radio\nModel 2: Sales ~ TV + Radio + TV:Radio\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    197 556.91                                  \n2    196 174.48  1    382.43 429.59 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows a significant p-value for the comparison, indicating that the interaction model provides a significantly better fit to the data than the first-order model. Therefore, we conclude that including the interaction term is justified in this case.\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#a-model-building-workflow",
    "href": "module03-model-building.html#a-model-building-workflow",
    "title": "4  Model Building and Selection",
    "section": "4.3 A model building workflow",
    "text": "4.3 A model building workflow\nSo far in this chapter we have discussed how to build more complex regression models using higher-order terms and interactions, as well as how to assess model fit while accounting for complexity. These are important tools for building effective regression models, and we now turn to strategies for applying these tools in practice.\n\n4.3.1 Exploratory data analysis and theory-driven model development\nBefore fitting any models, it is crucial to conduct exploratory data analysis (EDA) to understand the relationships between variables and identify potential patterns or anomalies. Visualisation and summary statistics can help identify non-linear relationships, interactions, and outliers that may inform model development. They can also highlight potential issues such as multicollinearity or heteroscedasticity that may need to be addressed (we will cover these potential pitfalls in the next chapter).\n\n4.3.1.1 Pairs plots and correlation matrices\nPairs plots and correlation matrices are useful tools for visualising relationships between multiple variables simultaneously. They can help identify potential predictors for inclusion in the model, as well as potential interactions or non-linear relationships. For example, a pairs plot can show scatterplots of each pair of variables, along with histograms of each variable on the diagonal. A correlation matrix can show the strength and direction of linear relationships between variables. We will use the `ggpairs() function from the GGally package (an extension to ggplot2) to create pairs plots in R\n\nlibrary(GGally)\nggpairs(ads)\n\n\n\n\n4.3.2 Automated model selection methods\n\n\n4.3.3 Stepwise Regression\nStepwise methods provide automated ways to search for simpler models.\n\n4.3.3.1 Forward Selection\nBegin with a minimal model (often the intercept). Add predictors one at a time when they improve AIC or adjusted \\(R^2\\).\n\n\n4.3.3.2 Backward Selection\nBegin with a saturated model containing all candidate predictors. Remove predictors that do not meaningfully contribute.\n\nNoteStepwise procedures should be treated as screening tools, not definitive modelling strategies. Final model decisions should consider diagnostics, interpretability, and subject-matter knowledge.\n\n\n\nExample 7: Samara: comparing seeds across treesMaple samara (seeds) fall like tiny helicopters. A forest scientist measured their fall speed (Velocity) against their ‘disk loading’ (a quantity based on their size and weight; encoded asLoad) on three trees (Tree). The goal: check whether the rate of change in velocity differs between trees.\n\nsamara &lt;- read_csv(\"Data/samara.csv\", show_col_types = FALSE) |&gt;\n  mutate(Tree = factor(Tree))\n\n\n\n\n\n\n\n\n\n\nThe slopes look similar but not identical. Perhaps tree identity matters, and perhaps the slopes differ slightly by tree. We can compare three candidate models:\nCandidate models\n\nsam_mods &lt;- list(\n  same_slope   = lm(Velocity ~ Load, data = samara),\n  add_tree     = lm(Velocity ~ Load + Tree, data = samara),\n  interaction  = lm(Velocity ~ Load * Tree, data = samara)\n)\n\nANOVA and fit metrics\n\nanova(sam_mods$same_slope, sam_mods$add_tree, sam_mods$interaction)\n\nAnalysis of Variance Table\n\nModel 1: Velocity ~ Load\nModel 2: Velocity ~ Load + Tree\nModel 3: Velocity ~ Load * Tree\n  Res.Df     RSS Df Sum of Sq     F  Pr(&gt;F)  \n1     33 0.21476                             \n2     31 0.20344  2  0.011322 0.992 0.38306  \n3     29 0.16549  2  0.037949 3.325 0.05011 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsam_fit &lt;- names(sam_mods) |&gt;\n map_dfr(\\(nm) glance(sam_mods[[nm]]) |&gt;\n           mutate(model = nm, formula = format(formula(sam_mods[[nm]])))) |&gt;\n select(model, formula, adj_r2 = adj.r.squared, AIC, BIC)\n\nsam_fit\n\n# A tibble: 3 × 5\n  model       formula                adj_r2   AIC   BIC\n  &lt;chr&gt;       &lt;chr&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 same_slope  Velocity ~ Load         0.791 -72.9 -68.3\n2 add_tree    Velocity ~ Load + Tree  0.789 -70.8 -63.1\n3 interaction Velocity ~ Load * Tree  0.817 -74.1 -63.2\n\n\nTree alone does not add much (p ≈ 0.38). The interaction is borderline (p ≈ 0.05) and has the lowest AIC/BIC, hinting that slopes might vary slightly by tree.\nForward vs backward stepwise selection\n\nscope &lt;- list(lower = ~1, upper = ~Load * Tree)\nstep_forward  &lt;- step(lm(Velocity ~ 1, samara), scope = scope,\n                      direction = \"forward\", trace = 0)\nstep_backward &lt;- step(lm(Velocity ~ Load * Tree, samara), scope = scope,\n                      direction = \"backward\", trace = 0)\n\nc(forward  = format(formula(step_forward)),\n  backward = format(formula(step_backward)))\n\n                 forward                 backward \n       \"Velocity ~ Load\" \"Velocity ~ Load * Tree\" \n\nAIC(step_forward, step_backward)\n\n              df       AIC\nstep_forward   3 -72.94911\nstep_backward  7 -74.07063\n\n\nForward selection stops at the simple Load-only model; backward selection keeps the interaction. Their AIC values are close, but the interaction edges ahead, so retain it if tree-specific slopes matter for interpretation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html",
    "href": "module04-regression-pitfalls-diagnostics.html",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "",
    "text": "5.1 Residuals as diagnostic tools\nIn the last section we discused cases where regression models can go wrong due to overfitting - fitting to noise rather than signal in the data. However, there are many other pitfalls that can arise in regression analysis. Many of these pitfalls arise from violations of the assumptions underlying the linear model framework (see Section 1.4 and ?sec-multiple_linear_regression for a reminder of how and why we made these assumptions). Therefore, it is important to routinely check the validity of these assumptions when fitting regression models. The most common assumptions to check are: - Linearity: the relationship between predictors and response is linear. - Homoscedasticity: the variance of the errors is constant across all levels of the predictors. - Normality: the errors are normally distributed. - Independence: the errors are independent of each other.\nMoreover, there aare other issues not as directly related to these assumptions that can also affect regression analysis, such as: - Influential points: individual data points that have a disproportionate impact on the model fit. - Multicollinearity: high correlation between predictor variables that can destabilise estimates. - Extrapolation: making predictions outside the range of the observed data. - blind model selection: using automated procedures without considering scientific context. - overfitting: fitting to noise rather than signal in the data. In this section, we will discuss how to diagnose these issues using residual plots and other diagnostic tools, as well as strategies for addressing them when they arise.\nHow can we check whether the assumptions of a linear regression model are met? One of the most common approaches is to examine the residuals of the model. As a reminder, the residuals are the differences between the observed values and the predicted values from the model:\nWe used residuals in ?sec-least_squares_estimation to estimate the parameters of the linear model, but they can also be used to diagnose potential issues with the model fit. Since residuals encode the information about what the model has not captured about the data, examining them can reveal patterns that reveal how our model fails to capture important aspects of the data (e.g. nonlinearities). Moreover, we use residuals as estimates of the unobserved errors in the model, which allows us to assess assumptions about the error distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#residuals-as-diagnostic-tools",
    "href": "module04-regression-pitfalls-diagnostics.html#residuals-as-diagnostic-tools",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "",
    "text": "5.1.1 Residual\nThe residual for observation \\(i\\) in a regression model is defined as \\[e_i = y_i - \\hat{y}_i\\] where \\(e_i\\) is the residual for observation \\(i\\), \\(y_i\\) is the observed value, and \\(\\hat{y}_i\\) is the predicted value from the model.\n\n\n\n5.1.2 Common diagnostic plots\nAnalysing residuals is such a key part of regression analysis that the base R plot() function for linear models automatically produces a set of diagnostic plots. Lets have a look",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#residual-plots-vs-predictors-and-fitted-values",
    "href": "module04-regression-pitfalls-diagnostics.html#residual-plots-vs-predictors-and-fitted-values",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "5.2 Residual plots vs predictors and fitted values",
    "text": "5.2 Residual plots vs predictors and fitted values\n\nPlot residuals against fitted values to check linearity and constant variance; add predictor-specific plots to spot functional-form issues (residual).\n\n\ndiag_mod &lt;- lm(mpg ~ wt + hp + am, data = mtcars)\npar(mfrow = c(1, 2))\nplot(diag_mod, which = 1)      # residuals vs fitted\nplot(mtcars$wt, resid(diag_mod), xlab = \"wt\", ylab = \"Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#diagnosing-nonconstant-variance-and-nonlinearity",
    "href": "module04-regression-pitfalls-diagnostics.html#diagnosing-nonconstant-variance-and-nonlinearity",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "5.3 Diagnosing nonconstant variance and nonlinearity",
    "text": "5.3 Diagnosing nonconstant variance and nonlinearity\n\nFunnel shapes or curved trends in residual plots suggest heteroskedasticity or missing curvature; consider transformations or adding interactions/polynomials.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#assessing-normality-of-residuals",
    "href": "module04-regression-pitfalls-diagnostics.html#assessing-normality-of-residuals",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "5.4 Assessing normality of residuals",
    "text": "5.4 Assessing normality of residuals\n\nUse Q-Q plots and compare \\(t\\)- and \\(p\\)-values to Normal reference.\n\n\nqqnorm(resid(diag_mod)); qqline(resid(diag_mod))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#standardised-residuals-leverage-cooks-distance",
    "href": "module04-regression-pitfalls-diagnostics.html#standardised-residuals-leverage-cooks-distance",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "5.5 Standardised residuals, leverage, Cook’s distance",
    "text": "5.5 Standardised residuals, leverage, Cook’s distance\n\nStandardised (or studentised) residuals scale by estimated variance and flag unusual outcomes (|r| &gt; 2 as a heuristic).\nLeverage (hatvalues()) flags unusual predictor combinations (leverage).\nCook’s distance combines leverage and residual size to assess influence (Cook’s distance).\n\n\ncbind(rstudent(diag_mod),\n      hatvalues(diag_mod),\n      cooks.distance(diag_mod))[1:5, ]\n\n                        [,1]       [,2]        [,3]\nMazda RX4         -1.4433354 0.09320650 0.051538041\nMazda RX4 Wag     -1.1371729 0.12316145 0.044939134\nDatsun 710        -1.3051372 0.08856401 0.040365321\nHornet 4 Drive     0.3121886 0.07518198 0.002046732\nHornet Sportabout  0.4688483 0.07867173 0.004827051",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#transformations-including-boxcox",
    "href": "module04-regression-pitfalls-diagnostics.html#transformations-including-boxcox",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "5.6 Transformations including Box–Cox",
    "text": "5.6 Transformations including Box–Cox\n\nTransformations can stabilise variance or linearise relationships; for strictly positive \\(Y\\), consider Box–Cox transformations to guide power choices.\n\n\nMASS::boxcox(diag_mod, plotit = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#handling-outliers-and-influential-observations",
    "href": "module04-regression-pitfalls-diagnostics.html#handling-outliers-and-influential-observations",
    "title": "5  Regression Pitfalls and Diagnostics",
    "section": "5.7 Handling outliers and influential observations",
    "text": "5.7 Handling outliers and influential observations\n\nInvestigate data quality first (entry errors, unusual units).\nIf influential points are real, fit with and without them to assess robustness; report how conclusions change.\nPrefer model adjustments (functional form, variance stabilisation) over automatic deletion; document any exclusions explicitly.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html",
    "href": "module05-case-naplan.html",
    "title": "6  Regression Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "6.1 Defining the research question",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#defining-the-research-question",
    "href": "module05-case-naplan.html#defining-the-research-question",
    "title": "6  Regression Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "Investigate how student and school-level predictors relate to NAPLAN Reading scores. Clarify outcome, units, and any grouping structure.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#importing-data",
    "href": "module05-case-naplan.html#importing-data",
    "title": "6  Regression Case Study: NAPLAN Reading Scores",
    "section": "6.2 Importing data",
    "text": "6.2 Importing data\n\n# Replace path with the appropriate CSV or RDS from the Resources/naplan reading folder\n# naplan &lt;- read.csv(\"Resources/naplan reading/naplan_reading.csv\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#exploratory-plot",
    "href": "module05-case-naplan.html#exploratory-plot",
    "title": "6  Regression Case Study: NAPLAN Reading Scores",
    "section": "6.3 Exploratory plot",
    "text": "6.3 Exploratory plot\n\nStart with scatterplots and boxplots for key predictors (e.g., study time, SES, gender) against Reading scores.\n\n\n# Example placeholder using mtcars structure; swap to naplan data\nlibrary(ggplot2)\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Reading score\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#fitting-the-least-squares-model",
    "href": "module05-case-naplan.html#fitting-the-least-squares-model",
    "title": "6  Regression Case Study: NAPLAN Reading Scores",
    "section": "6.4 Fitting the least-squares model",
    "text": "6.4 Fitting the least-squares model\n\nBegin with a first-order additive model; consider interactions or polynomials if exploratory plots suggest curvature.\n\n\n# model &lt;- lm(Reading ~ predictor1 + predictor2, data = naplan)\n# summary(model)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#model-refinement-and-interpretation",
    "href": "module05-case-naplan.html#model-refinement-and-interpretation",
    "title": "6  Regression Case Study: NAPLAN Reading Scores",
    "section": "6.5 Model refinement and interpretation",
    "text": "6.5 Model refinement and interpretation\n\nCheck residual diagnostics (see Module 6). Address nonlinearity, heteroskedasticity, or influential points.\nInterpret coefficients, including categorical contrasts and any interaction terms.\n\n\n# plot(model, which = 1:2)\n# coef(summary(model))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#reporting-the-results",
    "href": "module05-case-naplan.html#reporting-the-results",
    "title": "6  Regression Case Study: NAPLAN Reading Scores",
    "section": "6.6 Reporting the results",
    "text": "6.6 Reporting the results\n\nPresent fitted effects with confidence intervals, and provide a prediction interval for a meaningful scenario (e.g., a student profile of interest).\nSummarise key findings in plain language and note any limitations (e.g., omitted variables, sample size).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Simple linear regression\nA linear model with one predictor: \\(E[Y] = \\beta_0 + \\beta_1 X\\) with independent, mean-zero errors.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-mlr",
    "href": "glossary.html#gloss-mlr",
    "title": "Glossary",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nA linear model with two or more predictors; each coefficient is a partial effect holding the others fixed.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-residual",
    "href": "glossary.html#gloss-residual",
    "title": "Glossary",
    "section": "Residual",
    "text": "Residual\nAn observed value minus its fitted value from the model: \\(e_i = y_i - \\hat y_i\\).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-rse",
    "href": "glossary.html#gloss-rse",
    "title": "Glossary",
    "section": "Residual standard error",
    "text": "Residual standard error\nThe estimated standard deviation of the residuals (square root of the residual variance).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-adjusted-r-squared",
    "href": "glossary.html#gloss-adjusted-r-squared",
    "title": "Glossary",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\n\\(R^2\\) penalised for the number of predictors to discourage unnecessary terms.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-aic",
    "href": "glossary.html#gloss-aic",
    "title": "Glossary",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\nModel comparison metric that balances fit and complexity; lower values indicate a preferred model among those compared.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-multicollinearity",
    "href": "glossary.html#gloss-multicollinearity",
    "title": "Glossary",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nStrong correlation among predictors that inflates standard errors and destabilises estimates.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-interaction",
    "href": "glossary.html#gloss-interaction",
    "title": "Glossary",
    "section": "Interaction",
    "text": "Interaction\nA term that allows the effect of one predictor to depend on the level of another (e.g., \\(X_1 X_2\\)).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-dummy",
    "href": "glossary.html#gloss-dummy",
    "title": "Glossary",
    "section": "Dummy variable",
    "text": "Dummy variable\nAn indicator (0/1) used to code categories in regression relative to a baseline.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-leverage",
    "href": "glossary.html#gloss-leverage",
    "title": "Glossary",
    "section": "Leverage",
    "text": "Leverage\nA measure of how far a case’s predictor values are from the predictor means; high leverage can increase influence.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-cooks-distance",
    "href": "glossary.html#gloss-cooks-distance",
    "title": "Glossary",
    "section": "Cook’s distance",
    "text": "Cook’s distance\nInfluence measure combining residual size and leverage to flag points that change fitted values when omitted.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-box-cox",
    "href": "glossary.html#gloss-box-cox",
    "title": "Glossary",
    "section": "Box–Cox transformation",
    "text": "Box–Cox transformation\nA family of power transformations used to stabilise variance or improve linearity for positive responses.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-parsimony",
    "href": "glossary.html#gloss-parsimony",
    "title": "Glossary",
    "section": "Parsimony",
    "text": "Parsimony\nChoosing the simplest adequate model that answers the scientific question.",
    "crumbs": [
      "Glossary"
    ]
  }
]