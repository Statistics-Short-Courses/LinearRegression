[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Regression and Model Selection with R",
    "section": "",
    "text": "Course Overview\nLinear regression is one of the most widely used tools in data analysis. It helps us describe and predict an outcome \\(Y\\) using one or more predictors \\(X\\), while also quantifying how uncertain our conclusions are.\nThis short course aims to introduce you to the key concepts and practical skills needed to use linear regression effectively. The focus is on understanding what regression models do, how to apply them in R, interpret the results, and check that the resulting models are appropriate for your data and research question.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#sec-what-you-will-learn",
    "href": "index.html#sec-what-you-will-learn",
    "title": "Linear Regression and Model Selection with R",
    "section": "What you will learn",
    "text": "What you will learn\n\nAn understanding of linear models: the core idea that underpins regression analysis.\nHow to fit linear models to data using R\nInterpret the output of a regression analysis: coefficients, partial effects, interactions, and categorical predictors.\nQuantify uncertainty in your results using standard errors, confidence intervals, prediction intervals, and hypothesis tests.\nBuild, Compare, and refine models using fit metrics and information criteria (e.g. \\(R^2\\), \\(AIC\\)/\\(BIC\\)).\nDetect common regression pitfalls: nonlinearity, heteroskedasticity, non-normal errors, dependence, multicollinearity, high leverage and influential observations.\nApply practical remedies: transformations, adding appropriate terms, revising variables, and using robust approaches when needed.\nAn end-to-end workflow for regression analysis, from research question formulations and exploratory data analysis, to model fitting, diagnostics, and interpretation.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#sec-course-roadmap",
    "href": "index.html#sec-course-roadmap",
    "title": "Linear Regression and Model Selection with R",
    "section": "Course roadmap",
    "text": "Course roadmap\nThese chapters are designed to build from foundations → modelling → diagnostics → an end-to-end case study.\n\nChapter 00 — Introduction to linear models: what a regression model is, why we need an error term, and what assumptions mean in practice.\nChapter 01 — Simple linear regression (SLR): least squares, fitted values and residuals, inference, and prediction.\nChapter 02 — Multiple linear regression (MLR): partial regression coefficients, adjustment, categorical predictors, and overall model tests.\nChapter 03 — Model building: interactions, polynomial terms, parsimony, and model comparison/selection (with caveats).\nChapter 04 — Regression pitfalls & diagnostics: how to recognise assumption violations and influence, and what to do about them.\nChapter 05 — Case study (NAPLAN): a guided workflow from question → model → diagnostics → interpretation.\nGlossary: short definitions plus formulas and useful R functions.",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#sec-how-to-use-this-book",
    "href": "index.html#sec-how-to-use-this-book",
    "title": "Linear Regression and Model Selection with R",
    "section": "How to use this book",
    "text": "How to use this book\nThis book is part of a larger series of Statistics Short Courses from the UNE School of Science and Technology. Some content from other courses in this series is assumed knowledge here. In particular, a familarity with\n\nBasic R usage (see the UNE Consolidated R Resources for a refresher),\nDescriptive statistics and data visualisation (see the Exploratory Data Analysis/Visualisation Short Course), and\nBasic inferential statistics: estimation and hypothesis testing including t-tests and ANOVA (see the inferential Statistics with R Short Course).\n\nHere are some tips for getting the most out of this book:\n\nRead sequentially from start to finish. Each chapter builds on the previous ones, so it’s best to follow along in order. However, if you are already familiar with some topics, feel free to skip ahead.\nRun the examples as you go. Most examples are written to run in-browser using WebR; you can also run them in R/RStudio.\nUse the Glossary as you go. Important terms are highlighted in the text and defined in the glossary at the end of the book along with relevant formulas and R functions.\nPace yourself — this is a short course, but it covers a lot of material. Each chapter should take around 45-60 minutes to work through, depending on your familiarity with the topics.\n\nFinally, note that this is only a short course so we won’t be able to cover everything in depth. The goal is to give you a solid foundation in linear regression that you can build on with further study and practice. UNE offers several longer form and more advanced courses in statistics and data analysis if you wish to deepen your knowledge further. Also, there are many excellent textbooks and online resources available for further reading on regression analysis. Some recommended texts include: - “Linear Models with R” by Julian J. Faraway - “R for Data Science” by Hadley Wickham and Garrett Grolemund - “An Introduction to Statistical Learning” by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani",
    "crumbs": [
      "Course Overview"
    ]
  },
  {
    "objectID": "ch0-lm.html",
    "href": "ch0-lm.html",
    "title": "Chapter 0: Introduction to linear models",
    "section": "",
    "text": "Statistical models\nThis chapter serves as a conceptual introduction to linear models, the foundation of linear regression. By constructing a simple linear model from basic principles, we aim to understand the assumptions which play a crucial role in linear regression analysis.\nA central aim of statistical modelling is to understand how one variable changes in relation to others. In your own work, these variables will have concrete meaning - perhaps plant growth, reaction time, exam score, or income - but for now we will simply call them \\(x\\) and \\(Y\\).\nIn regression, we choose one variable \\(Y\\) to treat as the outcome we want to explain or predict, and \\(x\\) as one or more predictors. Our goal is to describe how changes in \\(x\\) are associated with changes in \\(Y\\).\nA simple way to express this idea is\n\\[\nY = f(x)\n\\]\nmeaning that the value of \\(Y\\) can be described by some function of \\(x\\). If we knew this function exactly, and if the world behaved perfectly, then knowing \\(x\\) would tell us everything about \\(Y\\). Many physical laws look like this (for example, \\(E = mc^2\\)) but real data rarely follow a perfectly deterministic relationship.\nIn practice, even when \\(x\\) is held constant, repeated observations of \\(Y\\) will vary. People respond differently, instruments fluctuate, biological systems are noisy, and experimental conditions change. To recognise this, statistical models include a random error term:\n\\[\nY = f(x) + \\varepsilon.\n\\]\nHere, \\(\\varepsilon\\) represents natural variability: the part of \\(Y\\) that our model does not or cannot explain.",
    "crumbs": [
      "Chapter 0: Introduction to linear models"
    ]
  },
  {
    "objectID": "ch0-lm.html#sec-linear-prediction",
    "href": "ch0-lm.html#sec-linear-prediction",
    "title": "Chapter 0: Introduction to linear models",
    "section": "Linear prediction",
    "text": "Linear prediction\nTo make our model concrete, we need to choose a form for the function \\(f(x)\\). A natural starting point—because it is simple, interpretable, and surprisingly powerful—is a linear function:\n\\[\nf(x) = \\alpha + \\beta x .\n\\]\nThis allows us to describe the expected value of \\(Y\\) as\n\\[\nE[Y] = \\alpha + \\beta x\n\\]\nThis is the familiar ‘straight-line’ relationship:\n- \\(\\alpha\\) is the intercept, the point where the line meets the vertical axis, and\n- \\(\\beta\\) is the slope, describing how we expect \\(Y\\) to change when \\(x\\) increases by one unit.\n\n\nviewof b0 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (α)\"})\nviewof b1 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β)\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = [-10,10]\nlineData = xRange.map(x =&gt; ({x, y: (b1 * x) + b0}))\n\ntex.block`E[Y] = ${b0} + ${b1}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  x:{domain: [-10,10], label: \"x\", grid: true},\n  y:{domain: [-10,10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(lineData, { x: \"x\", y: \"y\" })]\n})\n\n\n\n\n\n\n\n\n\n\n\nThis decision to model \\(E[Y]\\) as a linear function of \\(x\\) is a key part of the simple linear model. By choosing a linear function (rather than some other form), we are making an important assumption about the relationship between \\(x\\) and \\(Y\\):\n\nAssumption 1: Linearity\\(Y\\) and \\(x\\) have a linear relationship.\n\n\n\nContinue\n\n\nExample 1: Salary growth over timeSuppose you have received a job offer from Company A, and you want to predict your salary after working there for 10 years. You are told that the average starting salary at this company is $50,000, and that salaries increase by $5,000 per year of employment.\nWe can represent this relationship using a simple linear predictor. For an employee with \\(x\\) years at the company, the expected salary is\n\\[\nE[Y] = 50{,}000 + 5{,}000 \\cdot x.\n\\]\n\nPlotCode\n\n\n\n\n\n\n\nExpected salary at Company A as a function of years employed.\n\n\n\n\n\n\n\nggplot() +\n  geom_abline(intercept = 5e4, slope = 5e3, colour = '#2196F3') +\n  lims(x = c(0, 15), y = c(4e4, 13e4)) +\n  labs(x = \"Years Employed\", y = \"Expected Salary ($)\")\n\n\n\n\nAfter 10 years of employment (\\(x = 10\\)), our linear predictor gives\n\\[\nE[Y] = 50{,}000 + 5{,}000 \\times 10 = 100{,}000.\n\\]\n\n\n\n\nExercise 1: A competing offerA second company also offers you a position. Their starting salary is higher—$70,000 on average—but their yearly pay increases are smaller. Employees who have been at the company for 6 years earn, on average, $18,000 more than when they started.\nWe model expected salary after \\(x\\) years as:\n\\[\nE[Y] = \\alpha + \\beta x.\n\\]\n\nAssign these values to the R variables alpha and beta:\n\n\n\n\n\n\n\n\n\n\nThe starting salary gives \\(\\alpha = 70{,}000\\).\nThe 6-year increase gives \\(6\\beta = 18{,}000\\), so \\(\\beta = 3{,}000\\).\n\n\n\nalpha &lt;- 70000\nbeta &lt;- 3000\nalpha &lt;- 70000\nbeta &lt;- 3000\n\n\n\n\n\n\n\n\n\nLinear prediction\nThus the linear predictor for Company B is\n\\[\nE[Y] = 70{,}000 + 3{,}000 \\cdot x.\n\\]\nBelow is a plot comparing salary trends for both companies:\n\nPlotCode\n\n\n\n\n\n\n\nLinear salary trends for two companies.\n\n\n\n\n\n\n\nggplot() +\n  geom_abline(aes(intercept = 7e4, slope = 3e3, colour = \"Company B\")) +\n  geom_abline(aes(intercept = 5e4, slope = 5e3, colour = \"Company A\")) +\n  lims(x = c(0, 15), y = c(4e4, 13e4)) +\n  labs(\n    x = \"Years Employed\",\n    y = \"Expected Salary ($)\",\n    colour = \"Company\"\n  ) +\n  scale_color_manual(values = c(\"Company B\" = \"#4CAF50\", \"Company A\" = \"#2196F3\"))\n\n\n\n\nNow compute the expected salary after 15 years, using the alpha and beta values you just assigned:\n\n\n\n\n\n\n\n\n\n\n\nE_Y &lt;- alpha + (beta * 15)\nE_Y &lt;- alpha + (beta * 15)\n\n\n\n\n\n\n\n\n\nUsing R functions\nEvaluating the expression in R:\n\n\n\n\n\n\n\n\nThis matches the calculation:\n\\[\nE[Y] = 70{,}000 + 3{,}000 \\times 15 = 115{,}000.\n\\]\nNow we can turn this into a reusable function:\n\n\n\n\n\n\n\n\nPredict salaries for these employment durations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsimple_linear_prediction(x)\nsimple_linear_prediction(x)\n\n\n\n\n\n\n\n\nGood work!\nNext we will extend our linear predictor to form a full linear statistical model.",
    "crumbs": [
      "Chapter 0: Introduction to linear models"
    ]
  },
  {
    "objectID": "ch0-lm.html#sec-random-errors",
    "href": "ch0-lm.html#sec-random-errors",
    "title": "Chapter 0: Introduction to linear models",
    "section": "Random Errors",
    "text": "Random Errors\nIn practice, data rarely fall perfectly on a straight line. Even if the underlying relationship between \\(x\\) and \\(Y\\) is approximately linear, individual observations tend to vary around that line. As mentioned in Section Statistical models, to account for this variation we add an error term, \\(\\varepsilon\\), to our model:\n\\[\nY=\\alpha + \\beta x + \\varepsilon\n\\]\nThe error term is the difference between the observed value and the linear predictor: \\[\n\\varepsilon = Y - E[Y] = Y - (\\alpha + \\beta x).\n\\]\n\\(\\varepsilon\\) is a random variable, meaning that it can take different values each time we observe \\(Y\\) for a given \\(x\\). This randomness captures the idea that even when \\(x\\) is held constant, repeated observations of \\(Y\\) will vary.\nWhile we don’t know the exact value of \\(\\varepsilon\\) for any particular observation, we can describe its overall behaviour using probability. We typically assume three properties of \\(\\varepsilon\\):\n\nThe first key property of \\(\\varepsilon\\) is that\n\nOn average, we expect the error terms to balance out.\n\nAssumption 2: Mean of errorsThe mean of the error term is zero (mean-zero errors):\n\\[\nE[\\varepsilon]=\\mu=0\n\\]\n\n\ni.e. The linear predictor gives the correct value of \\(Y\\) on average. This is why the ‘expected value’ of \\(Y\\) is given by the linear function: \\[\nE[Y]=\\alpha + \\beta x + E[\\varepsilon] = \\alpha + \\beta x + 0 = \\alpha + \\beta x\n\\] —\n\nThe second key property of \\(\\varepsilon\\) is that\n\nWhile correct on average, we expect there to be some spread of data around the line (this is why we have the error term). The spread of a random variable like \\(\\varepsilon\\) is captured by its variance, which we denote \\(Var(\\varepsilon)\\). We assume that \\(Var(\\varepsilon)\\) does not depend on \\(x\\). That is, the spread of errors is the same for all values of \\(x\\).\n\nAssumption 3The variance of the error term is constant for all values of x (homoscedasticity): \\[Var(\\varepsilon)=\\sigma^2\\]\n\n\nThis is similar to the \\(\\mu=0\\) assumption above, but where there we specified the specific constant value (zero), here we specify that the variance is constant but leave its value (\\(\\sigma^2\\)) unspecified.\nThe term homoscedasticity comes from the Greek words for ‘same’ (homo) and ‘spread’ (scedasis), meaning ‘same spread’ and is therefore another way of describing the same property. ::: Terminology ### Homoscedasticity Homoscedasticity means constant error variance across all values of \\(x\\). ::: —\n\nThe final assumption we make about \\(\\varepsilon\\) is that it is Normally distributed.\n\nWhile the assumptions of \\(\\mu=0\\) and \\(Var(\\varepsilon)=\\sigma^2\\) describe the center and spread of the errors, they don’t fully specify the shape of their distribution. If we want to draw precise inferences about probability events (e.g. to test hypotheses using p-values), we can make a stronger assumption about the distribution of the errors. In particular, we choose to assume that the errors follow a Normal distribution.\n\nAssumption 4The error is normally distributed (normal errors) with mean \\(\\mu=0\\) and variance \\(\\sigma^2\\). \\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\nThe normal distribution is familiar and convenient to work with, and is often a reasonable approximation for real-world data. As a reminder of its shape, you can use the interactive plot below to explore normal distributions with different means and standard deviations.\n\n\nviewof mu = Inputs.range([-5, 5], {\n  value: 0,\n  step: 0.1,\n  label: `Mean (μ):`\n})\n\nviewof sigma = Inputs.range([0.2, 5], {\n  value: 1,\n  step: 0.1,\n  label: 'Standard deviation (σ):'\n})\n\nSQRT2PI=Math.sqrt(2 * Math.PI)\n\nnormalDensity = (x, mean, sd) =&gt;\n  (1 / (sd * SQRT2PI)) * Math.exp(-0.5 * ((x - mean) / sd) ** 2);\n  \ndensityGrid_1 = d3.range((-4 * sigma)+mu, mu+(4 * sigma), sigma / 50).map(x =&gt; ({\n  x,\n  density: normalDensity(x,mu,sigma)\n}));\n\ntex.block`\\varepsilon \\sim \\text{Normal}(${mu}, ${sigma}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 280,\n  marginLeft: 48,\n  marginBottom: 40,\n  y: { label: \"Density\" },\n  x: {domain: [-10,10], label: \"ε\" },\n  marks: [\n    Plot.areaY(densityGrid_1, {\n      x: \"x\",\n      y: \"density\",\n      fillOpacity: 0.2,\n      stroke: \"#2a5599\",\n      fill: \"#2a5599\"\n    })\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1: Normal distribution with adjustable mean and standard deviation.\n\n\n\n\n\n\n\nThis bell-shaped curve tells us that most values of \\(\\varepsilon\\) are close to zero, with larger deviations becoming increasingly rare. The standard deviation, \\(\\sigma\\), controls how tightly the errors cluster around zero: smaller values of \\(\\sigma\\) lead to a narrower peak, while larger values spread the errors out more widely.\n\nExample 2: Variation in salaryLet’s return to our simple linear model of salary at Company A,\n\\[\nE[Salary] = 50,000 + 5,000\\times Years\n\\]\nThis expresses how salary tends to increase with experience (i.e. on average). But in practice, not every employee with the same number of years earns the same amount — some earn a bit more, some a bit less. Suppose we know that most employees — about two-thirds of them — earn within roughly $4,000 of the average salary for their experience level. In other words, if the average salary after five years is $75,000, then about two-thirds of employees earn between $71,000 and $79,000, and almost everyone (about 95%) earns between $67,000 and $83,000.\nWe can capture this variability with a random error term, \\(\\varepsilon\\), assumed to follow a Normal distribution with mean 0 and standard deviation \\(\\sigma = 4,000\\).\n\\[\nSalary = 50,000 + 5,000\\times Years + \\varepsilon, \\quad{\\varepsilon \\sim \\mathcal{N}(0,4000^2)}\n\\]\nThis means that for a given number of years \\(x\\): - The expected salary is \\(50,000 + 5,000\\cdot x\\) - Actual salaries will vary around that average, typically within about ±$4,000\nFor example, after 5 years (x=5): \\[\nE[Y]=\\$50,000 + \\$5,000 \\times 5 = \\$75,000\n\\]\nThe distribution of salaries for employees with 5 years’ experience is\n\\[\nY\\sim \\mathcal{N}(75,000, 4,000^2)\n\\]\nSince we have a probability distribution over \\(Y\\), we can use R to evaluate the probability of any given salary after x years at the company.\nFor example, we want to know if employed at company A, what is the probability that after working there for 10 years I will have a salary of at least $110,000?\nFirst let’s calculate the average salary after 10 years\n\n50000+(5000*10)\n\n[1] 1e+05\n\n\n$100,000.\nNow\n\npnorm(11e4,1e5, 4e3, lower.tail = FALSE)\n\n[1] 0.006209665\n\n\nThe following diagram tells us roughly the probability of observing a \\(Y\\) value in the given range:\n\n\nmu_salary = 75000\nsigma_salary = 4000\n\nviewof k = Inputs.range([0, 3], {step: 0.1, value: 1, label: \"Interval width (in σ units)\"})\nviewof offset_z = Inputs.range([-5, 5], {step: 0.1, value: 0, label: \"Centre offset (in σ units)\"})\n\ncentre = mu_salary + offset_z * sigma_salary\na = centre - k * sigma_salary\nb = centre + k * sigma_salary\n\n// --- Numerical helpers ---\nerf = x =&gt; {\n  const a1 = 0.254829592, a2 = -0.284496736, a3 = 1.421413741,\n  a4 = -1.453152027, a5 = 1.061405429, p = 0.3275911;\n  const sign = Math.sign(x) || 1;\n  x = Math.abs(x);\n  const t = 1 / (1 + p * x);\n  const y = 1 - (((((a5*t + a4)*t + a3)*t + a2)*t + a1)*t) * Math.exp(-x*x);\n  return sign * y;\n}\nnormalCDF = (x, mean, sd) =&gt; 0.5 * (1 + erf((x - mean) / (sd * Math.SQRT2)))\n\n// Probability mass between a and b\nprob = Math.max(0, Math.min(1, normalCDF(b, mu_salary, sigma_salary) - normalCDF(a, mu_salary, sigma_salary)))\n\n// Density grid and shaded interval\ndensityGrid_salary = d3.range(mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary, sigma_salary/100)\n.map(y =&gt; ({ y, density: normalDensity(y, mu_salary, sigma_salary) }))\n\nshaded = densityGrid_salary.filter(d =&gt; d.y &gt;= a && d.y &lt;= b)\n\n// Display text summary\ntex.block`P(${Math.round(a).toLocaleString()} \\le Y \\le ${Math.round(b).toLocaleString()}) = ${prob.toFixed(3)} \\;\\;(\\approx ${(prob*100).toFixed(1)}\\%)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  height: 300,\n  marginLeft: 56,\n  marginBottom: 40,\n  x: { label: \"Salary ($)\", grid: true, domain: [mu_salary - 4*sigma_salary, mu_salary + 4*sigma_salary] },\n  y: { label: \"Density\",  },\n  marks: [\n    // Full curve (light)\n    Plot.areaY(densityGrid_salary, {x:\"y\", y:\"density\", fill:\"#2a5599\", fillOpacity:0.12, stroke:\"#2a5599\"}),\n    // Shaded probability region\n    Plot.areaY(shaded, {x:\"y\", y:\"density\", fill:\"#FFD54F\", fillOpacity:0.35}),\n    // Vertical rules\n    Plot.ruleX([mu_salary], {stroke: \"black\", strokeDash: [4,4]}),\n    Plot.ruleX([a], {y1:0, y2:normalDensity(a, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    Plot.ruleX([b], {y1:0, y2:normalDensity(b, mu_salary, sigma_salary), stroke:\"#2a5599\"}),\n    // Baseline\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 2: Probability of salary falling within a chosen interval.\n\n\n\n\n\n\n\nHere \\(\\varepsilon\\) represents random deviations from the expected (average) salary for a given number of years x. - Same model as above - given standard deviation - Example Y values - Illustrate error with normal overay - Excersises: - which variance is larger? - (hard) probability of finding E[Y]+2sd observation\n\n\n\nExercise 2: Normally distributed errors\nConsidering our model of salary with \\(\\varepsilon \\sim \\mathcal{N}(0, 4000^2)\\), what is the probability that an employee with 5 years of experience earns more than $83,000?\n\n\n\n\n\n\n\n\n\n\n\npnorm(83000, 75000, 4000, lower.tail = FALSE)\npnorm(83000, 75000, 4000, lower.tail = FALSE)",
    "crumbs": [
      "Chapter 0: Introduction to linear models"
    ]
  },
  {
    "objectID": "ch0-lm.html#sec-simple_linear_model",
    "href": "ch0-lm.html#sec-simple_linear_model",
    "title": "Chapter 0: Introduction to linear models",
    "section": "The Simple Linear Model",
    "text": "The Simple Linear Model\nPutting these pieces together we obtain the full specification of the simple linear model:\n\nAssumption 5: The Simple Linear ModelThe relationship between \\(Y\\) and \\(x\\) is described by a linear function plus normally distributed random error: \\[\nY=\\alpha+\\beta x+\\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\]\n\n\nThis model how the outcome variable \\(Y\\) depends on the predictor variable \\(x\\), with parameters \\(\\alpha\\) (intercept), \\(\\beta\\) (slope), and \\(\\sigma\\) (standard deviation of the errors). These parameters control the location, direction, and spread of the data around the linear trend. We can visualise the probability distribution of our outcome variable \\(Y\\) for different values of \\(x\\) using the interactive plot below:\n\n\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.1, label: \"Slope (β)\"})\nviewof b0_2 = Inputs.range([-10, 10], {step: 1, label: \"Intercept (α)\"})\nviewof sigma_2 = Inputs.range([0.2, 5], {step: 0.1, value: 2.5, label: \"Std. deviation (σ)\"})\n\nviewof n_cs = Inputs.range([10, 50], {step: 1, value: 50, label: \"Number of cross sections visualised\"})\n\ntex.block`Y = ${b0_2} + ${b1_2}x + \\varepsilon, \\quad \\varepsilon \\sim \\text{Normal}(0, ${sigma_2}^2)`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin = -10;\nxMax = 10;\nstep = (xMax - xMin) / (n_cs - 1);\nxSampleValues = d3.range(xMin, xMax + step/2, step);  \n\nySectionValues = d3.range(-10, 10.001, 0.1)\nwidthScale = Math.min(1.8, sigma_2 * 0.9)\n\ndensityCurveData = xSampleValues.flatMap(xVal =&gt; {\n  const mu = b0_2 + b1_2 * xVal;\n  const peakDensity = normalDensity(mu, mu, sigma_2);\n\n  const rightSide = ySectionValues.map(y =&gt; {\n    const density = normalDensity(y, mu, sigma_2);\n    const width = (density / peakDensity) * widthScale;\n    return {x: xVal + width, y, group: xVal};\n  });\n\n  return rightSide\n});\n\ncrossSectionTrendLine = xSampleValues.map(x =&gt; ({\n  x,\n  y: b0_2 + b1_2 * x\n}))\n\nPlot.plot({\n  x: {domain: [-10, 10], label: \"x\", grid: true},\n  y: {domain: [-10, 10], label: \"Y\", grid: true},\n  marks: [\n    Plot.line(densityCurveData, {\n      x: \"x\",\n      y: \"y\",\n      z: \"group\",\n      stroke: \"#2a5599\",\n      strokeWidth: 1.5,\n      curve: \"basis\"\n    }),\n    Plot.line(crossSectionTrendLine, {x: \"x\", y: \"y\", stroke: \"black\", strokeWidth: 2})\n  ]\n});\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i)\n\n\n\n\n\n\nFigure 3: Cross-sections of the simple linear model normal error density.\n\n\n\n\n\n\n\nHere, each vertical ‘slice’ of the plot shows the Normal distribution of \\(Y\\) for a specific value of \\(x\\), with the black line indicating the expected value \\(E[Y] = \\alpha + \\beta x\\). The ‘peak’ of each slice corresponds to the most probable value of \\(Y\\) for that \\(x\\), while the spread reflects the variability introduced by the error term \\(\\varepsilon\\).\nWhen we assume a linear model, we make an assumption about the ‘process’ that generates our data. Specifically, we assume that for each value of \\(x\\), the corresponding \\(Y\\) values are drawn from a Normal distribution whose mean is given by the linear function \\(\\alpha + \\beta x\\) and whose standard deviation is \\(\\sigma\\). While our model is simple, we can use it to generate realistic data that captures both the linear trend and the natural variability around it.\n\nContinue",
    "crumbs": [
      "Chapter 0: Introduction to linear models"
    ]
  },
  {
    "objectID": "ch0-lm.html#sec-a-simple-linear-model-in-r",
    "href": "ch0-lm.html#sec-a-simple-linear-model-in-r",
    "title": "Chapter 0: Introduction to linear models",
    "section": "A simple Linear model in R",
    "text": "A simple Linear model in R\nTo see how the model ‘generates’ data, we will simulate observations using the salary example parameters from above. 1 The \\(x\\) values here are generated just for illustration; in regression we condition on \\(x\\) rather than treat it as random.\n\nTo begin, we start with a collection of \\(x\\) values. We treat these as fixed values, representing years of experience at the company. We can generate some example \\(x\\) values in R like this:\n\n\nn &lt;- 100\nx &lt;- runif(n, min = 0, max = 20)\nhead(x)\n\n[1] 19.32881120  6.55456500  0.08311689 18.45143690 19.21046907  3.43451963\n\n\n\nThis code randomly chooses n = 100 values uniformy at random from the interval \\([0,20]\\). We treat these values as fixed once chosen; the randomness here is just for example data generation.\n\n\nDefine the model\n\nNext, we construct our simple linear model\n\nsimple_linear_model &lt;- function(x, alpha, beta, sigma) {\n  E_Y &lt;- alpha + (beta * x) # Expected value of Y \n  epsilon &lt;- rnorm(length(x), mean = 0, sd = sigma) # random normal errors\n  Y &lt;- E_Y + epsilon # 'Observed' Y values\n  return(Y) \n}\n\nThe simple_linear_model() function takes x values and returns the specified linear function with normally distributed random noise added.\nNow we can simulate observations of \\(Y\\) given our collection of \\(x\\) values and the parameters we have chosen for our salary example:\n\nY &lt;- simple_linear_model(x, alpha=5e4, beta=5e3, sigma=4e3)\nhead(Y)\n\n[1] 147882.51  81396.19  52039.12 142380.59 150521.75  64218.26\n\n\nLet’s look at the joint distribution of x and Y:\n\nPlotCode\n\n\n\n\n\n\n\nData generated from the simple linear model \\(Y=50{,}000+5{,}000\\times x + \\varepsilon\\), with \\(\\varepsilon\\sim N(0,4{,}000^2)\\). Dashed line shows \\(E[Y|x] = \\alpha + \\beta x\\).\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(x = x, Y = Y)\n\nggplot(df, aes(x, Y)) +\n  geom_point(alpha = 0.7) +\n  theme_minimal()\n\n\n\n\nThe scatter plot shows a single simulated ‘sample’ of data from our simple linear model. If we repeated the simulation, we would get a different set of \\(Y\\) values (because of the randomness introduced by the error term), but they would still cluster around the same dashed line representing the expected value \\(E[Y|x] = \\alpha + \\beta x\\).\nHere’s the same code running in webR - try adjusting some of the parameters (e.g. the number of samples), or just running again to plot a different random sample from the same model!",
    "crumbs": [
      "Chapter 0: Introduction to linear models"
    ]
  },
  {
    "objectID": "ch0-lm.html#sec-summary",
    "href": "ch0-lm.html#sec-summary",
    "title": "Chapter 0: Introduction to linear models",
    "section": "Summary",
    "text": "Summary\nIn this section, we have introduced the simple linear model, which describes the relationship between an outcome variable \\(Y\\) and a predictor variable \\(x\\) using a linear function plus normally distributed random error: \\[\nY=\\alpha + \\beta x + \\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\] This model captures both the expected trend of \\(Y\\) as a function of \\(x\\) (given by the linear function \\(\\alpha + \\beta x\\)) and the natural variability around that trend (captured by the error term \\(\\varepsilon\\)).\nWe have also seen how this model can ‘generate’ realistic data that reflects both the linear relationship and the random variation. In the next section, we will explore how to use observed data to estimate the parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) of the simple linear model.",
    "crumbs": [
      "Chapter 0: Introduction to linear models"
    ]
  },
  {
    "objectID": "ch0-lm.html#footnotes",
    "href": "ch0-lm.html#footnotes",
    "title": "Chapter 0: Introduction to linear models",
    "section": "",
    "text": "1↩︎",
    "crumbs": [
      "Chapter 0: Introduction to linear models"
    ]
  },
  {
    "objectID": "ch1-slr.html",
    "href": "ch1-slr.html",
    "title": "1  Simple Linear Regression (SLR)",
    "section": "",
    "text": "1.1 When to use SLR\nLinear regression is a method that uses a [linear model] to describe the relationship between a response variable \\(Y\\) and one or more predictor variables \\(x\\). In this chapter, we focus on the simplest case of linear regression - simple linear regression (SLR) - where there is a single predictor variable \\(x\\).\nSLR is appropriate when you want to model the relationship between a continuous response variable \\(Y\\) and a single continuous predictor variable \\(x\\). There are some additional conditions that should be met for SLR to be valid, but we will cover these in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "ch1-slr.html#sec-when-to-use-slr",
    "href": "ch1-slr.html#sec-when-to-use-slr",
    "title": "1  Simple Linear Regression (SLR)",
    "section": "",
    "text": "Key-pointSLR is used to model the relationship between a continuous response variable (\\(Y\\)) and a single continuous predictor variable (\\(x\\)).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "ch1-slr.html#sec-LinearFit",
    "href": "ch1-slr.html#sec-LinearFit",
    "title": "1  Simple Linear Regression (SLR)",
    "section": "1.2 Fitting models to data",
    "text": "1.2 Fitting models to data\nIn ?sec-introduction-to-linear-models we constructed a simple linear model based on known parameters: \\[\nY=\\alpha + \\beta x + \\varepsilon, \\quad{\\varepsilon \\sim N(0,\\sigma^2)}\n\\] where \\(\\alpha\\) is the intercept, \\(\\beta\\) is the slope, and \\(\\sigma\\) is the standard deviation of the normally distributed error term \\(\\varepsilon\\). In that section, we knew the values of \\(\\alpha\\), \\(\\beta\\) and even \\(\\sigma\\).\nHowever, in most contexts we don’t know the true relationship between \\(x\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) by analysing the data. This analysis is called simple linear regression.\nIndeed, at the end of Section 4 we generated data from a linear model. In preparation for this chapter, we’ve generated some data from a similar such linear model. Here it is:\n\nPlotData\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            x           y\n1   1.6148772 -2.00511170\n2  -4.0312281 -8.22613886\n3   1.0712529 -1.75147101\n4   0.4099033 -1.02595779\n5   3.0773924 -1.65935764\n6   6.1897227  1.38662442\n7  -8.1498706 -8.74264363\n8  -6.8877324 -6.78921632\n9   1.1008535 -4.65515531\n10 -1.1420121 -3.16672528\n11 -3.8521321 -7.22466501\n12 -2.8648770 -2.87521551\n13 -4.2116130 -4.06867811\n14 -3.6559147 -3.52838994\n15 -6.2075168 -4.69189467\n16  3.9109874 -0.07522054\n17 -6.7556840 -5.81420459\n18 -7.8947941 -8.53584145\n19  5.4456119 -1.08631918\n20  6.2075999  0.75156682\n\n\n\n\n\nour task for the remainder of the chapter (and in regression generally) is to try an make a (educated) guess about what linear model gave rise to this data.\n\nContinue\n\n\n1.2.1 Parameter estimates\nWe call the process of trying to guess the parameters of the data generating model (i.e. \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\)), estimation, and our guesses are estimates.\nTo avoid confusion, we will give our estimates different names to the true parameters. In particular, for simple linear regression we will denote an estimate of the intercept,\\(\\alpha\\), by \\(a\\), and estimate of the slope, \\(\\beta\\), by \\(b\\), and an estimate of the error standard deviation,\\(\\sigma\\) by \\(s\\). So, we have\n\nKey-term: Parameter EstimatesThe estimates of the parameters of a simple linear regression model are denoted as: \\[\na: \\text{Estimated intercept}\n\\] \\[\nb: \\text{Estimated slope}\n\\] \\[\ns: \\text{Estimated standard deviation}.\n\\]\n\n\n\nKey-point: Estimated vs true parametersIt is important to distinguish between estimated (sometimes called ‘fitted’, or ‘sample’) parameters and true (sometimes called ‘population’) parameters. The true parameters are assumed to govern the data generating process (and are unknown), while the estimated parameters are our best guesses for the true parameters based on the observed data. In this course we use Greek letters (e.g. \\(\\alpha\\), \\(\\beta\\), \\(\\sigma\\)) to denote true parameters, while Latin letters (e.g. \\(a\\), \\(b\\), \\(s\\)) denote estimates of those parameters.\n\n\n\nContinue\n\n\n\n1.2.2 Predicted values\nJust as the linear model with true parameters gives us the expected value of \\(Y\\): \\[\nE[Y]=\\alpha + \\beta x,\n\\] our estimated linear model gives us an estimate of the value of \\(Y\\). We denote this as \\(\\hat{Y}\\) (pronounced y-hat), the predicted value of \\(Y\\) given our estimated model:\n\nKey-term: Predicted ValueThe predicted value of the outcome variable \\(Y\\) given an esimated simple linear predictor is defined as: \\[\n\\hat{Y}=a+bx\n\\]\n\n\n\n\nContinue\n\n\n\n1.2.3 Fitting a line to data\nLooking at data (i.e. pairs of \\((x, Y)\\) values), we try to find the estimates of our parameters that best ‘fit’ its distribution. Adjust the parameter estimates with the sliders below to find a line that you think fits the data:\n\n\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})\nviewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept (adjust)`})\n\nb0_1 = (-3 - 0.5*b1_1) + b0adj_1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(linearData.x)\n\nlineData_1 = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\nyRange = d3.extent(linearData.y)\nyPad = (yRange[1] - yRange[0]) * 0.1\nyDomain = [yRange[0] - yPad, yRange[1] + yPad]\ntex.block`\\hat{Y} = ${b0_1.toFixed(2)} + ${b1_1.toFixed(2)}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marks: [\n    Plot.line(lineData_1, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: yDomain, label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nWhat is your best estimate for the values of \\(\\alpha\\) and \\(\\beta\\) that best fit the given data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOk great, so we have an estimate for our line-of-best-fit:\n\ntex.block`\\hat{Y} = ${my_a} + ${my_b}\\cdot x`\n\n\n\n\n\n\nBut how good is our estimate really? What do we mean by ‘best’ fit here? And how can we objectively evaluate it and maybe compare it to other estimates?\n\nContinue\n\n\n\n1.2.4 Residuals\nResiduals are the differences between observed values of \\(Y\\), and the value predicted by our estimated linear predictor \\(\\hat{Y}\\). We denote residuals with the letter \\(e\\):\n\nKey-term: ResidualThe difference between an observed value of \\(Y\\) and the value predicted by our estimated model \\(\\hat{Y}\\): \\[\ne=Y-\\hat{Y} = Y - (a + bx).\n\\]\n\n\nResiduals, \\(e\\), are similar to the errors, \\(\\varepsilon\\), that we encountered in Section 3 - but they are distinct. Since we do not know the ‘true’ values of \\(\\alpha\\) and \\(\\beta\\) - we cannot calculate the errors, \\(\\varepsilon= Y- E[Y]= Y-\\alpha + \\beta x\\). However, we do have our estimates, \\(a\\) and \\(b\\), so we can calculate residuals.\nIndeed - calculating residuals gives us a way to assess how well our estimated model fits the data. We can think of the residuals as being the ‘mismatch’ between our estimated linear predictor and each data point. By minimizing the size of residuals (minimising the mismatch of our model to the data), we can get a better fit of our line to data.\n\n\nviewof b0adj_resid = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept`})\nviewof b1_resid = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})\n\nb0_resid = -3 + b0adj_resid\n\nresidualData_resid = transpose(linearData).map(d =&gt; {\n  const yhat = b1_resid * d.x + b0_resid;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange_resid = d3.extent(linearData.x)\n\nlineData_resid = xRange_resid.map(x =&gt; ({x, y: b1_resid * x + b0_resid}))\nyRange_resid = d3.extent(linearData.y)\nyPad_resid = (yRange_resid[1] - yRange_resid[0]) * 0.1\nyDomain_resid = [yRange_resid[0] - yPad_resid, yRange_resid[1] + yPad_resid]\ntex.block`\\hat{Y} = ${b0_resid} + ${b1_resid}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData_resid, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\",\n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes}),\n\n    Plot.line(lineData_resid, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange_resid },\n  y: { domain: yDomain_resid, label: \"y\" },\n  color: { domain: [\"positive\", \"negative\"]}\n})\n\n\n\n\n\n\n\n\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "ch1-slr.html#sec-estimating-coefficients",
    "href": "ch1-slr.html#sec-estimating-coefficients",
    "title": "1  Simple Linear Regression (SLR)",
    "section": "1.3 Estimating coefficients",
    "text": "1.3 Estimating coefficients\n\n1.3.1 Optimising fit by minimising (squared) residuals\nBelow, the plot now also displays squared residuals for each data point as squares. Underneath the model equation is the Sum of squared residuals (SSR), which gives a measure of the absolute difference between our linear predictor and the observed outcomes.\n\nKey-term: Sum of squared residuals (SSR)The sum of squared residuals (SSR) is the sum of the squares of the residuals from our estimated linear predictor: \\[\nSSR=\\sum_{i=1}^{n} e_i^2\n\\]\nwhere \\(e_i = Y_i - \\hat{Y}_i\\).\n\n\nThe SSR gives us a numerical measure of how well the line fits the data - see how small you can get the SSR by adjusting slope and intercept.\n\n\nviewof b0adj_2 = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept`})\nviewof b1_2 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})\n\nb0_2 = -3 + b0adj_2\n\nresidualData = transpose(linearData).map(d =&gt; {\n  const yhat = b1_2 * d.x + b0_2;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n\nsquareData = residualData.map(d =&gt; {\n  const y1 = Math.min(d.y, d.yhat);\n  const y2 = Math.max(d.y, d.yhat);\n  const alignLeft = (d.residual &lt; 0 && b1_2 &gt;= 0) || (d.residual &gt;= 0 && b1_2 &lt; 0);\n  const x1 = alignLeft ? d.x : d.x - d.absRes;\n  const x2 = alignLeft ? d.x + d.absRes : d.x;\n  return {\n    ...d,\n    x1,\n    x2,\n    y1,\n    y2\n  };\n})\n  \nSSRes = d3.sum(residualData, d =&gt; d.residual ** 2)\n\nhtml`&lt;div style=\"background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 6px; padding: 12px; margin-top: 10px;\"&gt;\n  &lt;div style=\"font-weight: 500; margin-bottom: 8px;\"&gt;Sum of Squared Residuals (SSR):&lt;/div&gt;\n  ${tex`\\displaystyle\\sum_{i=1}^{n} e_i^{2} = ${SSRes.toFixed(2)}`}\n&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxMin_2 = d3.min(squareData, d =&gt; Math.min(d.x1, d.x2))\nxMax_2 = d3.max(squareData, d =&gt; Math.max(d.x1, d.x2))\nxPad_2 = (xMax_2 - xMin_2) * 0.05\nxDomain_2 = [xMin_2 - xPad_2, xMax_2 + xPad_2]\n\nyMin_2 = d3.min(squareData, d =&gt; Math.min(d.y1, d.y2))\nyMax_2 = d3.max(squareData, d =&gt; Math.max(d.y1, d.y2))\nyPad_2 = (yMax_2 - yMin_2) * 0.1\nyDomain_2 = [yMin_2 - yPad_2, yMax_2 + yPad_2]\n\nlineData_2 = xDomain_2.map(x =&gt; ({x, y: b1_2 * x + b0_2}))\n\ntex.block`\\hat{Y} = ${b0_2} + ${b1_2}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marks: [\n    Plot.rect(squareData, {\n      x1: \"x1\",\n      x2: \"x2\",\n      y1: \"y1\",\n      y2: \"y2\",\n      fill: \"#4CAF50\",\n      fillOpacity: 0.25,\n      stroke: \"#4CAF50\",\n      strokeOpacity: 0.75\n    }),\n\n    Plot.line(lineData_2, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xDomain_2 },\n  y: { domain: yDomain_2, label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nBased on this visualisation, what do you think is the minimum possible \\(SSR\\) achievable?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want you can update your \\(a\\) and \\(b\\) estimates too (otherwise just proceed):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 The least-squares estimate\nWe call the estimates \\(a\\) and \\(b\\) which minimise the sum of squares the least squares estimates. Some nice mathematics tells us that the least squares esimates for a simple linear model are unique - that is, there is one set of values for \\(a\\) and \\(b\\) which satisfy this property. Moreover, we don’t have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute \\(a\\) and \\(b\\) very efficiently. As a statistical programming language, this is something R does very easily.\n\n1.3.2.1 Fitting a model with the lm() function\nIn R the lm() function computes the least squares estimates \\(a\\) and \\(b\\) for our simple linear model (among other things) in a single command:\n\n\n\n\n\n\n\n\nWe can extract the coefficients from the lm by indexing:\n\n\n\n\n\n\n\n\nor by the coef() function:\n\n\n\n\n\n\n\n\n\nHow do these estimates compare with yours?\n\nLets compare the estimated linear fits graphically:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "ch1-slr.html#sec-estimating-variance",
    "href": "ch1-slr.html#sec-estimating-variance",
    "title": "1  Simple Linear Regression (SLR)",
    "section": "1.4 Estimating variance",
    "text": "1.4 Estimating variance\nOnce the line has been fitted (i.e. \\(\\alpha\\) and \\(\\beta\\) have been estimated as \\(a\\) and \\(b\\)), we also have to estimate the distribution of error terms (remember, as per ?sec-varAssumption, the error distribution has a constant variance defined by \\(\\sigma^2\\)). As you might expect, we again utilise the residuals from our least squares linear predictor to estimate \\(\\sigma\\). The spread of the error terms will be estimated by the spread of the residuals around our line of best fit.\nWe can extract the individual residual values from the fitted lm object by indexing (e.g. my_lm$residuals) or with the residuals() function.\n\n\n\n\n\n\n\n\nThe resulting R object is a vector of the residual for each observation. i.e. residuals(lm_1)[[3]] \\(= e_3\\)\n\nExercise 3: Extracting residuals from an lm object\nExtract the residuals from your least squares fitted model lm_1by indexing and assign them to the variable e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ne &lt;- lm_1$residuals\ne &lt;- lm_1$residuals\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 Residual Standard Error\nThe residuals from our least squares fit can be used to estimate the variance of the error terms in our linear model. The residual sum of squares (RSS) that we calculated in Section 1.3.2 during ‘least squares’ fitting, as a measure of variation of observations around the fitted line can also be used to estimate the variance of the error terms.\n\nKey-term: Residual Sum of Squares (RSS)The residual sum of squares (RSS) is defined as \\[\nRSS = \\sum_{i=1}^{n} e_i^2\n\\] where \\(e_i = Y_i - \\hat{Y}_i\\) are the residuals from our estimated linear predictor.\n\n\n\nExercise 4: Calculate the Residual Sum of Squares\nusing your e object defined in the previous exercise, calculate the residual sum of squares of the least squares model.\n\n\n\n\n\n\n\n\n\n\n\nWe simply square each residual using r^2 and then sum them together using the sum() function:\n\n\n\nrss &lt;- sum(e^2)\nrss &lt;- sum(e^2)\n\n\n\n\n\n\n\n\n\n\n\nHowever, the RSS itself is not a very useful measure of the spread of the residuals, since it depends on the number of observations - more observations will tend to lead to a larger RSS, even if the spread of the residuals is the same. We need a measure of the average size of the residual (per observation). To standardise the RSS, we might consider the average squared residual, which we can obtain by dividing the RSS by the number of observations, \\(n\\). However, this would tend to underestimate the true variance of the error terms, \\(\\sigma^2\\), because the residuals are calculated from the estimated line, which has already ‘used up’ some of the information (see [degrees of freedom]) in the data.\nTo adjust for the fact that we have already estimated two parameters (\\(a\\) and \\(b\\)) from the RSS,we instead divide the RSS by \\(n-k\\) (where \\(n\\) is the number of observations, and \\(k\\) is the number of parameters alreadyu estimated -in this case 2). This gives us the mean squared error (MSE), which is our estimator for \\(\\sigma^2\\):\n\nKey-term: Mean Squared Error (MSE){#sec-residual-standard-error-definition} We define the mean squared error (MSE) of a simple linear regression model as \\[\ns^2 = \\frac{1}{n-2}\\sum_{i=1}^{n} e_i^2= \\frac{SSE}{n-2}\n\\]\n\n\nJust as we often prefer to work with standard deviations rather than variances, we also define the residual standard error (RSE) as the square root of the MSE:\n\nKey-term: Residual Standard Error (RSE)The residual standard error (RSE) of a simple linear regression model is defined as \\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^{n} e_i^2} = \\sqrt{\\frac{SSE}{n-2}}.\n\\]\n\n\n\\(s^2\\) and \\(s\\) are our estimators for \\(\\sigma^2\\) and \\(\\sigma\\), respectively.\n\nExercise 5: Calculate the Residual Standard Error\nUsing your rss object defined in the previous exercise, calculate the residual standard error of the least squares model.\n\n\n\n\n\n\n\n\n\n\n\nThe RSE is \\(s = \\sqrt{\\frac{RSS}{n-2}}\\). You can get \\(n\\) using nrow(linearData).\n\n\n\n\n\nrse &lt;- sqrt(sum(e^2)/(nrow(linearData)-2))\nrse &lt;- sqrt(sum(e^2)/(nrow(linearData)-2))\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n1.4.2 Correlation and \\(R^2\\)\nThe residual standard error \\(s\\) tells us the absolute spread of observations around the fitted line — it is measured in the same units as \\(Y\\). But it does not tell us how much of the original variation in \\(Y\\) our model has explained. For that, we need a relative measure.\nThe total sum of squares (TSS) captures the total variation in \\(Y\\) around its mean:\n::: Key Term #### Total Sum of Squares (TSS) {#sec-total-sum-of-squares-definition}\n\\[\n\\text{TSS} = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2.\n\\] :::\n\nExercise 6: Calculate TSS\nUsing linearData, calculate the total sum of squares (TSS).\n\n\n\n\n\n\n\n\n\n\n\nTSS is sum((linearData$y - mean(linearData$y))^2).\n\n\n\n\nTSS &lt;- sum((linearData$y - mean(linearData$y))^2)\nTSS\nTSS &lt;- sum((linearData$y - mean(linearData$y))^2)\nTSS\n\n\n\n\n\n\n\n\n\n\nBy comparing the residual sum of squares (RSS) to the total sum of squares (TSS), we can quantify the proportion of variance explained by our model. This ratio is called R-squared:\n\nKey-term: R-squared (\\(R^2\\))\nThe \\(R^2\\) statistic is defined as \\[\nR^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}.\n\\]\ni.e. the proportion of the total variance in \\(Y\\) (the total sum of squares, TSS) explained by the model.\n\nWhen \\(R^2 = 1\\), all variation is explained (RSS = 0).\nWhen \\(R^2 = 0\\), the model explains none of the variation (RSS = TSS).\n\n\n\nAs a visual demonstration of how \\(R^2\\) changes with increasing error variance, consider the following interactive plot:\n\n\ndata_r2 = transpose(linearData)\n\n// True model parameters that generated the data\nalpha_true_r2 = -3\nbeta_true_r2 = 0.5\n\n// Compute true fitted values and errors (difference from true model)\ntrueModelData_r2 = data_r2.map(d =&gt; ({\n  ...d,\n  yTrue: alpha_true_r2 + beta_true_r2 * d.x,\n  error: d.y - (alpha_true_r2 + beta_true_r2 * d.x)\n}))\n\n// Observed error SD (used to create unit errors)\nerrorSD_observed = Math.sqrt(d3.sum(trueModelData_r2, d =&gt; d.error ** 2) / data_r2.length)\n\n// Unit errors: error / errorSD (so they have approx unit variance)\nunitError_r2 = trueModelData_r2.map(d =&gt; d.error / errorSD_observed)\n\nviewof varScale_r2 = Inputs.range([0, 4], {\n  value: 1,\n  step: 0.05,\n  label: \"Variance scale\"\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsigma_current = varScale_r2 * errorSD_observed\n\n// Compute displayed y = true model + scaled error\npointData_r2 = trueModelData_r2.map((d, i) =&gt; ({\n  x: d.x,\n  y: d.yTrue + sigma_current * unitError_r2[i]\n}))\n\n// Compute LS fit for the current points\nxMean_r2 = d3.mean(pointData_r2, d =&gt; d.x)\nyMean_r2 = d3.mean(pointData_r2, d =&gt; d.y)\nsxy_r2 = d3.sum(pointData_r2, d =&gt; (d.x - xMean_r2) * (d.y - yMean_r2))\nsxx_r2 = d3.sum(pointData_r2, d =&gt; (d.x - xMean_r2) ** 2)\nbeta_ls_r2 = sxy_r2 / sxx_r2\nalpha_ls_r2 = yMean_r2 - beta_ls_r2 * xMean_r2\n\n// Add LS fitted values to points\npointDataWithFit_r2 = pointData_r2.map(d =&gt; ({\n  ...d,\n  yFitted: alpha_ls_r2 + beta_ls_r2 * d.x\n}))\n\n// Compute RSS, TSS, R² using LS fit\nRSS_r2 = d3.sum(pointDataWithFit_r2, d =&gt; (d.y - d.yFitted) ** 2)\nTSS_r2 = d3.sum(pointDataWithFit_r2, d =&gt; (d.y - yMean_r2) ** 2)\nR2_demo = TSS_r2 &gt; 0 ? Math.max(0, 1 - RSS_r2 / TSS_r2) : 1\n\ntex.block`R^2 = ${R2_demo.toFixed(3)}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxDomain_r2 = d3.extent(data_r2, d =&gt; d.x)\nxPad_r2 = (xDomain_r2[1] - xDomain_r2[0]) * 0.1\n\n// Fixed y domain: use range that accommodates max variance scale (3)\nmaxSigma_r2 = 3 * errorSD_observed\nyMin_r2_fixed = d3.min(trueModelData_r2, (d, i) =&gt; d.yTrue + maxSigma_r2 * unitError_r2[i])\nyMax_r2_fixed = d3.max(trueModelData_r2, (d, i) =&gt; d.yTrue + maxSigma_r2 * unitError_r2[i])\nyPad_r2 = (yMax_r2_fixed - yMin_r2_fixed) * 0.1\n\nPlot.plot({\n  height: 300,\n  x: { domain: [xDomain_r2[0] - xPad_r2, xDomain_r2[1] + xPad_r2], label: \"x\", grid: true },\n  y: { domain: [yMin_r2_fixed - yPad_r2, yMax_r2_fixed + yPad_r2], label: \"Y\", grid: true },\n  marks: [\n    Plot.line(pointDataWithFit_r2, {x: \"x\", y: \"yFitted\", stroke: \"steelblue\", strokeWidth: 2}),\n    Plot.dot(pointData_r2, {x: \"x\", y: \"y\", r: 4, fillOpacity: 0.7})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that as \\(\\sigma\\) increases, \\(R^2\\) decreases, indicating that the model explains less of the variance in \\(Y\\). Fitting a line to data with high noise results in a poor fit, as reflected by a low \\(R^2\\) value.\n\nExercise 7: Correlation and \\(R^2\\) in our example\ncompute \\(R^2\\) using the residuals from lm_1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr &lt;- cor(linearData$x, linearData$y)\nr_squared &lt;- 1 - sum(e^2) / sum((linearData$y - mean(linearData$y))^2)\nr &lt;- cor(linearData$x, linearData$y)\nr_squared &lt;- 1 - sum(e^2) / sum((linearData$y - mean(linearData$y))^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "ch1-slr.html#sec-inference-for-slr",
    "href": "ch1-slr.html#sec-inference-for-slr",
    "title": "1  Simple Linear Regression (SLR)",
    "section": "1.5 Inference for SLR",
    "text": "1.5 Inference for SLR\nUp to this point, we have fitted a straight-line model to a sample of data, obtaining estimates \\(a\\) and \\(b\\) for the intercept and slope. These estimates describe the pattern visible in the sample, but they do not tell us how closely they reflect the true population parameters \\(\\alpha\\) and \\(\\beta\\). Because sampling introduces randomness, different samples would produce different fitted lines.\nThis raises the central question:\nHow much confidence can we place in the slope and intercept we estimated, and what do they tell us about the true relationship in the population?\nTo answer this, we rely on statistical inference, which quantifies the uncertainty in the estimates and evaluates whether the predictor genuinely influences the response.\nImportantly, the inferential procedures we use rest on the assumptions of the linear regression model:\n\nThe errors \\(\\varepsilon\\) have mean \\(0\\).\nThey have constant variance \\(\\sigma^2\\) (homoscedasticity).\nThey are independent.\nThey are Normally distributed.\n\nThe Normality assumption is what allows us to derive the sampling distributions of \\(a\\) and \\(b\\), leading directly to the t-tests and confidence intervals used for inference. Without these assumptions—especially Normality—the exact forms of these inferential tools would not hold.\n\nNoteFor a broader introduction to statistical inference, see the Inferential Statistics with R short course. If concepts such as the Normal distribution or t-tests feel unfamiliar, please review that material before continuing.\n\n\n\n\n1.5.1 Inference About the Slope, \\(\\beta\\)\nIn simple linear regression, the population relationship is modelled as\n\\[\nY = \\alpha + \\beta x + \\varepsilon.\n\\]\nTo determine whether \\(x\\) is genuinely associated with \\(Y\\), we test:\n\\[\nH_0: \\beta = 0\n\\qquad \\text{vs.} \\qquad\nH_a: \\beta \\ne 0.\n\\]\n\nUnder \\(H_0\\), changes in \\(x\\) do not affect the mean of \\(Y\\) (a change in \\(x\\) will lead to \\(\\beta\\cdot x = 0\\cdot x = 0\\) change in \\(Y\\)).\nUnder \\(H_a\\), there is evidence of a real linear effect (i.e. a change in \\(x\\) will lead to a non-zero change in \\(Y\\)).\n\nBecause the Normality assumption implies that the estimator \\(b\\) has a Normal sampling distribution (and hence a \\(t\\) distribution once \\(\\sigma\\) is estimated), we are able to quantify how unusual our observed slope would be if \\(H_0\\) were correct.\n\nExercise 8: Hypothesis testing revision (p-values)\n\n\n\n1.5.1.1 The t-Test for the Slope\nThe hypothesis test is carried out using the statistic\n\\[\nt = \\frac{b}{\\text{SE}(b)},\n\\]\nwhich follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom.\nInterpretation:\n\nA large value of \\(|t|\\) (small p-value) indicates evidence that \\(\\beta \\ne 0\\).\nA small value of \\(|t|\\) suggests the data are consistent with no linear effect.\n\nThe validity of this test relies on the Normality of the errors, which guarantees that this \\(t\\) statistic follows the appropriate reference distribution.\n\nNoteWhile the assumption of Normality is what theoretically grounds the use of the t-test for parameter estimates, SLR is quite robust to violations of this assumption and inferences about our slope parameters may be of interest even with non-normal residual distributions.\n\n\n\np-value vs SE(b)CI width vs SE(b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1: t-test for slope\n\n\n\nExercise 9: t-test for slope\n\n\n\n\n1.5.1.2 Confidence Interval for the Slope\nA \\((1-\\alpha)100%\\) confidence interval for \\(\\beta\\) is\n\\[\nb ;\\pm; t_{\\alpha/2,,n-2},\\text{SE}(b).\n\\]\nInterpretation:\n\nAn interval excluding zero indicates a likely genuine relationship.\nAn interval including zero suggests weaker evidence.\n\nConfidence intervals complement hypothesis tests by communicating both the direction and plausible magnitude of the effect.\nIn R, you can compute the confidence interval for the slope directly:\n\n\n\n\n\n\n\n\n\n\n\n1.5.2 Inference About the Response, \\(Y\\)\nOnce we have fitted a regression model, we often want to make statements about the value of the response at a given predictor value \\(x_0\\). There are two distinct quantities of interest:\n\nThe mean (average) response at \\(x_0\\): \\[\n\\mu_Y(x_0) = \\alpha + \\beta x_0.\n\\]\nA new individual response at \\(x_0\\): \\[\nY_{\\text{new}} = \\alpha + \\beta x_0 + \\varepsilon.\n\\]\n\nThese involve different uncertainties, and therefore require different intervals.\nConfidence intervals for the mean response reflect uncertainty in \\(a\\) and \\(b\\). Prediction intervals include that uncertainty plus the additional variability from the random error term \\(\\varepsilon\\).\n\n1.5.2.1 Confidence interval for the Mean Response\nLet \\(\\hat{y}_0 = a + b x_0\\) be the fitted value at \\(x_0\\). A confidence interval for the mean response is:\n\\[\n\\hat{y}*0\n;\\pm;\nt*{\\alpha/2,,n-2} ,\ns\\sqrt{\n\\frac{1}{n} +\n\\frac{(x_0 - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n}.\n\\]\nThis interval quantifies uncertainty in the average value of \\(Y\\) for units with predictor value \\(x_0\\).\npredict(fit, newdata = new_point, interval = \"confidence\")\n\n\n1.5.2.2 Prediction interval for a New Observation\nTo predict an individual outcome at \\(x_0\\), we must include the additional uncertainty from the random error \\(\\varepsilon\\):\n\\[\n\\hat{y}*0\n;\\pm;\nt*{\\alpha/2, , n-2} ,\ns\\sqrt{\n1 +\n\\frac{1}{n} +\n\\frac{(x_0 - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n}.\n\\]\nBecause of the extra “1” term, prediction intervals are always wider than confidence intervals.\npredict(fit, newdata = new_point, interval = \"prediction\")\n\nR code: confidence intervalR code: prediction interval\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.2.3 Summary\n\nConfidence interval → uncertainty in the expected value at \\(x_0\\)\nPrediction interval → uncertainty in a new outcome at \\(x_0\\)\n\n\n\nContinue\n\nWe now have the full SLR toolkit: estimation, fit summaries, and inference for slopes and responses. Next, we extend these ideas to multiple predictors in Chapter 2, and later return to residual diagnostics and assumption checking in Chapter 4.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "ch2-mlr.html",
    "href": "ch2-mlr.html",
    "title": "2  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "2.1 When to extend SLR\nMost practical applications of linear regression require models more complex than the simple linear regression(SLR) model introduced in Section 4. Multiple linear regression (MLR) extends simple linear regression by allowing \\(Y\\) to depend on more than one predictor variable. This enables richer models and allows us to estimate the partial contribution of each predictor while accounting for others.\nSLR is limited to one predictor. MLR becomes appropriate when:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "ch2-mlr.html#sec-when-to-extend-slr",
    "href": "ch2-mlr.html#sec-when-to-extend-slr",
    "title": "2  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Multiple factors plausibly influence the response.\nExcluding predictors may bias results.\nYou wish to measure the unique contribution of each predictor while controlling for others.\n\n\nExample 1: Advertising salesLets consider a simple example using the Advertising dataset\n\n\n\n\n\n\n\n\nSuppose that we are statistical consultants hired by a client to investigate the association between advertising and sales of a particular product. The Advertising dataset (which we have imported as ads) consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for two different media: TV, and radio. 1\n\n\n\n\n\n\n\n\nWe might look at each bivariate (i.e. two variables) relationship between sales and advertising expendature separately:\n\n\\(\\text{Sales} \\sim \\text{TV}\\)\\(\\text{Sales}\\sim \\text{Radio}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever we might also look at the multivariate relationship:\n\nColoured Scatter3-D Scatter\n\n\nHere we encode the values of the second predictor (Radio) with colour\n\n\n\n\n\n\n\n\n\n\nIn the case of 3 variables we can also extend our visualisation to 3 dimensions\n\n\n\n\n\n\n\n\n\n\n\nUsing multiple predictors simultaneously gives us more information than using each variable separately - this is a great case for applying multiple regression!\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "ch2-mlr.html#sec-multiple_linear_regression",
    "href": "ch2-mlr.html#sec-multiple_linear_regression",
    "title": "2  Multiple Linear Regression (MLR)",
    "section": "2.2 The multiple linear regression model",
    "text": "2.2 The multiple linear regression model\nThe multiple linear regression model extends the simple linear model from Section 4 straightforwardly by adding the extra variables to the deterministic linear predictor, each with its own parameter:\n\nKey-point: The MLR model\\[\nY = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\varepsilon, \\quad \\varepsilon \\sim N(0,\\sigma^2)\n\\]\n\n\nNow, instead of a single predictor \\(x\\), we may have several (\\(k\\)) predictors \\(x_1, x_2, \\ldots, x_k\\), each corresponding to a column in our dataset. We use an index to distinguish these predictors, and each predictor \\(x_i\\) has a corresponding parameter \\(\\beta_i\\) governing its effect on the response. Note that we have updated the intercept parameter \\(\\alpha\\) from the simple linear regression section to \\(\\beta_0\\), because it is simply another parameter in the model!\nThe random error term, \\(\\varepsilon\\), stays the same, so the assumptions of MLR are very similar to those of SLR:\n\nAssumption: Assumptions of the MLR model\n\nA linear predictor, e.g. \\(E[Y]=\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k\\).\nIndependent and identically distributed random error terms that\n\nHave a mean \\(\\mu=0\\) (mean-zero errors), and a constant variance \\(Var(\\varepsilon)=\\sigma^2\\) (homoscedasticity)\nFollow a normal distribution (normal errors): \\(N(0,\\sigma^2)\\)\n\n\n\n\n\nNote: Multivariate LinearityAlthough this is still a linear model, the term “linear” no longer refers to a literal straight line in two dimensions as it did in simple linear regression. Instead, “linear” means that the model is linear in its parameters: each \\(\\beta_i\\) multiplies a predictor and enters additively. This linear model may live in a high-dimensional predictor space and cannot be visualised as a single line. For example, for the three dimensional data presented above (one outcome: Sales + two predictors: TV, Radio), a linear model may instead look like a plane - the three dimensional equivalent of a line in two dimensions.\n\n\n\n\n\n\n\n\nA further discussion of linearity will take place in Section 3.1.2, when we discuss higher order multiple regression models.\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "ch2-mlr.html#sec-partial-regression-coefficients-betai",
    "href": "ch2-mlr.html#sec-partial-regression-coefficients-betai",
    "title": "2  Multiple Linear Regression (MLR)",
    "section": "2.3 Partial regression coefficients, \\(\\beta_i\\)",
    "text": "2.3 Partial regression coefficients, \\(\\beta_i\\)\nEven though our model is no longer just a straight line, the slope interpretation from SLR is still relevant in MLR. Now however, \\(\\beta_i\\) describes the expected change in the mean response for a one-unit increase in \\(x_i\\), holding all other predictors constant.\n\nExample 2: Interpreting a partial coefficient, \\(\\beta_i\\)\nWe may propose a model for Sales whereby the baseline sales (when no money is invested in advertising) is \\(\\beta_0=3\\), \\(\\beta_{TV}= 0.1\\) and \\(\\beta_{Radio}= 0.2\\), i.e.  \\[\nE[Sales]= 3 + 0.1\\cdot\\text{TV} + 0.2\\cdot \\text{Radio}\n\\] In this model: * A $1k increase in TV advertising expenditure is associated with an expected 100 unit (0.11000units - the scale of the outcome) increase in sales, holding Radio expenditure constant.  Converely, at a fixed level of TV advertising expenditure, a $1k increase in Radio advertisin expenditure corresponds to an expected 200 unit increase in sales.\n\nContinue\n\nFor example, lets fix one predictor. Say, we have already have invested $5k in Radio, then our linear predictor becomes\n\\[\nE[Sales]= 3 + 0.1\\cdot\\text{TV} + 0.2\\cdot 5= 4 + 0.1\\cdot\\text{TV}\n\\]\ni.e. only one variable (TV) remains (Radio no longer varies but is held constant - contributing \\(0.2\\cdot 5=1\\)k units to \\(E[Y]\\)), so the parameter \\(\\beta_1\\) can be interpreted as the slope of the SLR line:\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Calculating the expected outcome of a multiple regression model\nGiven this model of sales by TV and Radio advertising, \\[\nE[Sales]= 3 + 0.1\\cdot\\text{TV} + 0.2\\cdot \\text{Radio}\n\\]\n$12k has been spent on TV advertising due to a persisting contract. How much should the company invest in Radio advertising so that the expected sales matches their inventory of 8k units?\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.1 Fitting MLR Models in R\nFitting a multiple linear regression uses the exact same conceptual process from Section 1.3.2. Residuals are still defined \\(e = Y- \\hat{Y}\\), and minimising the sum of squared residuals provides optimal and unique least squares estimates for the model parameters \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_k\\).\nIn R we use the same lm() function as SLR (these are both ‘linear models’, after all). Now however, the model formula includes both predictors on the RHS, separated by a +:\n\n\n\n\n\n\n\n\n\nNote: R formulas(the first argument, identified by the ~ separating the LHS and RHS)\n\n\nLets visualise our fitted model:\n\n\n\n\n\n\n\n\nWe can extract our fitted model coefficients in the same way:\n\nUsing indexingThe coef() function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: extracting coefficients from an lm model\nUse the the least-squares coefficients from abs mod to complete these partial regression plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 broom::tidy()\nWe can also use the tidy() function from the broom package for a nice summary of the relevant inferential statistics for each parameter:\n\n\n\n\n\n\n\n\nThe hypotheses being tested for each coefficient is analagous to that in the SLR case (Section 1.5.1). \\[\nH_0 : \\beta_i = 0\n\\qquad\\text{vs}\\qquad\nH_a : \\beta_i \\ne 0.\n\\] using t-statistic which takes into account the size of the estimate along with its standard error: \\[\nt_i = \\frac{\\hat\\beta_i}{\\operatorname{SE}(\\hat\\beta_i)}.\n\\]\nThe main difference in calculation here is that this test statistic has degrees of freedom that depend on the number of predictors in the model. Specifically, \\(t_i\\) has \\(n - (k+1)\\) degrees of freedom - where \\(n\\) is the number of observations and \\(k\\) is the number of predictor variables. Note that this is consistent with the df used for SLR.\nThe interpretation of these hypotheses is slightly more nuanced. As pointed out above, the value of \\(\\beta_i\\) here does not represent the relationship between \\(Y\\) and \\(X_i\\) without qualification - but only as the other variables are held constant. Analogously, the hypothesis being tested by \\(t_i\\) is whether \\(X_i\\) is related to to \\(Y\\) - once we account for the other predictors in the model.\n\nKey-point: Interpreting the t-test of \\(\\beta_i\\)\\(t_i\\) is a \\(n-k+1\\) statistic which tests the null hypothesis that \\(\\beta_i=0\\) in the multiple regression model. A significant p-value for this test entails that given the other predictors in the model.\n\n\nBecause of this:\n\nThe significance of a predictor can change when variables are added or removed.\nA small p-value for \\(\\beta_i\\) suggests a meaningful partial effect, not necessarily a marginal (unadjusted) one.\n\n\n\n2.3.3 [Multicollinearity]\n\nleast squares is an amazing method for fitting but it has a weakness\ncorrelated predictors can cause unstable parameter estimates\nCheck for correlation using a correlation matrix or using vif\nthis will be adressed more comprehensively in chapter 6: diagnostics an transformations.\n\n\n\nNote: Multiple t-tests and type I error inflationWhile it is easy to compute t-tests for each model coefficient individually, this is generally not a good way to determine whether the model as a whole is contributing useful information for the prediction of \\(Y\\). While the model above only has two predictors, larger datasets have the potential for dozens, hundreds, or thousands of predictors! If we were to rely on this series of \\(t\\)-tests alone to determine whether we have any useful predictors of the outcome, we would be compounding our chance of making a ‘type I error’ (i.e. concluding there is a effect where there is none) with each test. Instead of performing several small tests, we have reason to want a single global test - one that encompasses all the \\(\\beta\\) parameters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "ch2-mlr.html#sec-the-f-test-for-model-model-usefulness-and-measures-of-model-fit",
    "href": "ch2-mlr.html#sec-the-f-test-for-model-model-usefulness-and-measures-of-model-fit",
    "title": "2  Multiple Linear Regression (MLR)",
    "section": "2.4 The F-test for model model usefulness. : and measures of model fit",
    "text": "2.4 The F-test for model model usefulness. : and measures of model fit\nHaving multiple predictor variables expands the scope of the kind of questions we can ask about our linear model. Rather than looking at each coefficient separately, we can ask whether our model as a whole is effective in explaining the outcome \\(Y\\) In other words, is the combined contribution of the predictors enough to conclude that a linear relationship exists?\nFormally, the global hypothesis test is:\n\n\\(H_0\\): all slope coefficients are zero (no linear relationship between \\(Y\\) and the predictors)\n\n\\(H_a\\): at least one slope coefficient is non-zero (the model is useful)\n\nThis shift parallels the move from multiple t-tests to an ANOVA - instead of testing individual “effects,” we examine the overall variance explained by a model.\nThe F-statistic compares:\n\nvariation explained by the model (mean regression sum of squares, MSR),\n\nresidual variation (mean squared error, MSE).\n\nBecause each of these quantities is constructed from squared normal deviations, the ratio\n\\[\nF = \\frac{\\text{MSR}}{\\text{MSE}}\n\\]\nfollows an F-distribution under \\(H_0\\).\nIf the model truly explains some structure in the data, MSR will be noticeably larger than MSE.\n[#^1] The links between ANOVA and linear regression will be futher explored here in Section 2.5.\n\nExample 3In the life expectancy example, the output might report\nF-statistic: 28.2 on 4 and 44 DF,  p-value: 1.19e-11\nThe very small p-value indicates strong evidence against \\(H_0\\). We conclude that at least one of the predictors (Population, Health, Internet, BirthRate) is useful for predicting life expectancy, and that the model is useful overall.\n\n\n\nExercise 3Given an F-statistic of 12.3 with p = 0.0004, state the null and alternative hypotheses and conclude whether the model is useful at the 5% level.\n\n\n\n2.4.0.1 global statistics with anova()\nWe can obtain the model F-statistic using the anova() function, which reinforces the connection between regression and the ANOVA framework introduced earlier.\n\n\n\n\n\n\n\n\nThe final row of the ANOVA table reports the global F-test for the model.\n\n\n2.4.0.2 global statistics with glance()\nThe glance() function from the broom package also reports the model F-statistic while providing several additional summary measures in one place.\n\n\n\n\n\n\n\n\nThis output includes several global model features besides the global test statistic and p-value.\n\n\n2.4.1 Adjusted \\(R^2\\)\nOnce we have established that the model is useful overall, we can quantify how much of the variation in \\(Y\\) it explains. The measure \\(R^2\\) was introduced in SLR and extends naturally to MLR In multiple regression, \\(R^2\\) plays the same descriptive role, but its interpretation changes subtly because the model now includes several predictors.\n\n\n2.4.2 \\(R^2\\) in Multiple Regression\nWe define \\[\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}},\n\\] exactly as before. The difference lies in what \\(R^2\\) represents:\n\nIn SLR, \\(R^2\\) reflects how well a single predictor explains variation in \\(Y\\).\nIn MLR, \\(R^2\\) reflects the combined explanatory power of all predictors working together.\n\nBecause adding a new predictor can never increase SSE, it follows that \\(R^2\\) can never decrease when more predictors are added, even if the new predictor has little or no real relationship with the response. For this reason, \\(R^2\\) is not reliable for comparing models with different numbers of predictors.\n\n\n2.4.3 Adjusted \\(R^2\\)\nTo address the fact that \\(R^2\\) is overly optimistic in larger models, we use the adjusted coefficient of determination: \\[\nR^2_{\\text{adj}} = 1 -\\frac{\\text{SSE}/(n - k - 1)}{\\text{SST}/(n - 1)}.\n\\]\nAdjusted R-squared:\n\npenalises the inclusion of additional predictors,\nincreases only when a predictor provides meaningful explanatory value,\nand may decrease when a predictor contributes little or nothing.\n\nThus,\n\n\\(R^2\\) is appropriate as a descriptive measure of how much variation the fitted model explains,\nadjusted \\(R^2\\) is more appropriate for comparing different models, especially those with differing numbers of predictors.\n\nThis completes our introduction to multiple linear regression: we now have a model with several predictors, an interpretation for its coefficients, and tools to judge whether the model is useful and how well it fits the data.\n\n\n2.4.4 Other model fit statistics: logLikelihood, AIC, and BIC\nThe glance() output also includes two additional quantities: AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\nAlthough they appear alongside the F-statistic and \\(R^2\\), they serve a different purpose.\nAIC and BIC are not measures of how well a single model fits the data in an absolute sense.\nInstead, they are designed for comparing multiple competing models, balancing goodness of fit against model complexity. Lower values indicate a preferable trade-off, but only relative comparisons are meaningful.\nBecause AIC and BIC are tools for model selection rather than model assessment, we do not interpret them here. They will be discussed in detail in Chapter 5 (Principles of Model Building), where they are used to guide decisions about which predictors to include in a regression model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "ch2-mlr.html#sec-mlr_cat",
    "href": "ch2-mlr.html#sec-mlr_cat",
    "title": "2  Multiple Linear Regression (MLR)",
    "section": "2.5 Categorical predictors in MLR",
    "text": "2.5 Categorical predictors in MLR\nSo far, we have only considered numerical predictors in our multiple linear regression models. However, categorical predictors (also called factors in R) can also be included seamlessly.\n\n2.5.1 Factors and indicator variables\nTo include a categorical predictor in a linear model, we represent each category using dummy variables. For a binary factor with levels A (reference) and B, we create a single indicator \\[\nD = I(\\text{Group} = B),\n\\] and fit \\[\nY = \\beta_0 + \\beta_1 D + \\varepsilon.\n\\] Here, \\(\\beta_0\\) is the mean of the reference level (A), and \\(\\beta_1\\) is the difference in means between B and A. This is exactly the same comparison made by a two-sample t-test, and the slope test of \\(\\beta_1\\) uses the same logic as Section 1.5.1.\n\nExample 4: Transmission type and fuel economyIn mtcars, transmission (am) is coded as 0 = automatic and 1 = manual. We can treat this as a factor and model mean fuel economy by transmission type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe intercept is the mean mpg for the reference group (automatic), and the am_fmanual coefficient is the mean difference (manual minus automatic).\n\n\n\n\n2.5.2 Multi-level factors and one-way ANOVA\nFor a factor with \\(k\\) levels, R creates \\(k-1\\) dummy variables by default (a contrast coding scheme). The overall F-test for the factor is the same as the one-way ANOVA you have already seen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ANOVA table tests whether any cylinder group differs from the reference group, just like a standard one-way ANOVA. This mirrors the overall model test described in Section 2.4.\n\n\n2.5.3 Adjusted comparisons (ANCOVA)\nWe can combine categorical and numerical predictors. This lets us compare group means while controlling for other variables (an ANCOVA-style interpretation).\n\n\n\n\n\n\n\n\nHere, the am_fmanual coefficient estimates the difference in mpg between manual and automatic cars holding weight constant.\n\n\n2.5.4 Reference levels and releveling\nThe reference level is the baseline against which other levels are compared. You can change it to make interpretation easier:\n\n\n\n\n\n\n\n\nNotice how the sign of the coefficient flips when the reference group changes.\n\nExercise 4: Interpreting a binary factor\nFit mpg ~ am_f on mtcars_cat.\n\nWhat does the intercept represent?\nWhat does the am_fmanual coefficient represent?\nHow does this relate to a two-sample t-test?\n\n\n\n\nExercise 5: Factor with three levels\nFit mpg ~ cyl_f and identify the reference level.\n\nWhich cylinder group is the baseline?\nHow would you change the baseline to 8 cylinders?\nWhich test in the ANOVA table tells you whether any cylinder group differs?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "ch2-mlr.html#footnotes",
    "href": "ch2-mlr.html#footnotes",
    "title": "2  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "This exmple is adapted from “Introduction to Statistical Learning (2)” - an excellent (and freely available) text on statistical modelling.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "ch3-model-building.html",
    "href": "ch3-model-building.html",
    "title": "3  Model Building and Variable Selection",
    "section": "",
    "text": "3.1 Increasing model complexity to model complex relationships\nIn this chapter, we explore strategies for building effective regression models. We cover: - Increasing model complexity to capture non-linear relationships and interactions, - Assessing model fit while balancing complexity, - Techniques for selecting parsimonious models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building and Variable Selection</span>"
    ]
  },
  {
    "objectID": "ch3-model-building.html#sec-increasing-model-complexity-to-model-complex-relationships",
    "href": "ch3-model-building.html#sec-increasing-model-complexity-to-model-complex-relationships",
    "title": "3  Model Building and Variable Selection",
    "section": "",
    "text": "3.1.1 Why increase model complexity?\nWe began this course with the simple linear regression model: a single continuous predictor with a straight-line effect. We then expanded to multiple predictors, where each predictor still had a straight-line effect. For reasons that will become clear, these are examples of so-called first-order models:\n\nKey-term: First-order modelA linear regression where each predictor appears only once and to the first power (no squared terms or interactions). For one predictor: \\[\nE[Y] = \\beta_0 + \\beta_1 X.\n\\] With multiple predictors, \\(X_1, X_2, \\dots, X_i\\), this extends to \\[E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_i X_i\\]\n\n\nThis first-order structure is the simplest linear model we can make with \\(X\\). It is often a reasonable starting point, and can go a long way to modeling real-world phenomena (especially in the multiple regression case), but real data are often more complex. Real relationships between predictors and outcomes are often non-linear, or involve interactions between predictors. To capture these more complex relationships, we can increase model complexity by adding ‘higher-order’ and ‘interaction’ terms.\n\nExample 1: Non-linear relationships in data\nFor example, fitting a straight line to the following data is not ideal, as the relationship between \\(X\\) and \\(Y\\) is curved (U-shaped), so there is always part of the relationship the line cannot capture:\n\n\nquadPoints = transpose(quadData)\nquadXDomain = d3.extent(quadData.x)\n\nxRange = d3.extent(quadData.x)\nyRange = d3.extent(quadData.y)\n\nviewof b0adj_1 = Inputs.range([-1,16], {step: 1, label: html`${tex`\\beta_0`}: Intercept `})\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\beta_1`}: Slope`})\n\nb0_1 = b0adj_1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\hat{Y} = ${b0_1.toFixed(0)} + ${b1_1.toFixed(2)}x`\n\n\n\n\n\n\n\nlineData = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\nlineResiduals = quadPoints.map((d) =&gt; {\n  const fit = b0_1 + b1_1 * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\nlineMaxAbsResid = d3.max(lineResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\nPlot.plot({\n  marks: [\n    Plot.link(lineResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / lineMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(quadPoints, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: yRange, label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n3.1.2 Square, Cubic, and Higher-Order Univariate Models\nWe enrich the basic linear model by adding powers of a predictor, allowing the fitted relationship to bend. In the above case, the “U” shape suggests a quadratic (squared) term may be appropriate.\n\nKey-term: Second-order (Quadratic) univariate model\\[E[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2\\]\n\n\n\nExample 2: Seeing how \\(\\beta_2\\) bends the curve\nAdjust the slider for the squared term to see how changing \\(\\beta_2\\) adds curvature to the fitted relationship . Try to find a good fit - can you guess what model generated this data?.\n\n\nviewof b0_quad = Inputs.range([-4, 4], {step: 1, value: 0, label: html`${tex`\\beta_0`}: Intercept`})\nviewof b1_quad = Inputs.range([-3, 3], {step: 1, value: 0, label: html`${tex`\\beta_1`}: Linear term`})\nviewof b2_quad = Inputs.range([-1, 3], {step: 0.05, value: 0, label: html`${tex`\\beta_2`}: Squared term`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\hat{Y} = ${b0_quad.toFixed(0)} + ${b1_quad.toFixed(0)}x + ${b2_quad.toFixed(2)}x^2`\n\n\n\n\n\n\n\nquadCurve = d3.range(quadXDomain[0], quadXDomain[1] + 0.05, 0.05).map((x) =&gt; ({\n  x,\n  y: b0_quad + b1_quad * x + b2_quad * x * x\n}))\n\nquadResiduals = quadPoints.map((d) =&gt; {\n  const fit = b0_quad + b1_quad * d.x + b2_quad * d.x * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\nquadMaxAbsResid = d3.max(quadResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\n\nPlot.plot({\n  height: 320,\n  marginLeft: 50,\n  x: {label: \"Predictor (x)\", domain: xRange},\n  y: {label: \"Outcome (y)\", domain: yRange},\n  marks: [\n    Plot.link(quadResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / quadMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(quadPoints, {\n      x: \"x\",\n      y: \"y\",\n      r: 3,\n      title: (d) =&gt; `x = ${d.x.toFixed(2)}\\ny = ${d.y.toFixed(2)}`,\n    }),\n    Plot.line(quadCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.2.1 Third-order (cubic) univariate model\n\\[\nE[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3\n\\]\nallows more complex curvature with two changes in direction (an “S”-shape) that a quadratic term cannot capture.\n\nExample 3: Seeing how \\(\\beta_3\\) twists the curve\nAdjust the slider for the cubic term to see how changing \\(\\beta_3\\) adds an inflection point to the fitted relationship. Can you tune the parameters to recover the curve that generated this data?\n\n\ncubicPoints = transpose(cubicData)\ncubicXDomain = d3.extent(cubicData.x)\n\nviewof b0_cub = Inputs.range([0, 4], {step: 1, value: 2, label: html`${tex`\\beta_0`}: Intercept`})\nviewof b1_cub = Inputs.range([-2, 2], {step: 1, value: -1, label:  html`${tex`\\beta_1`}: Linear term`})\nviewof b2_cub = Inputs.range([-1, 1], {step: 0.1, value: 0.5, label: html`${tex`\\beta_2`}: Squared term`})\nviewof b3_cub = Inputs.range([-0.6, 0.6], {step: 0.1, value: 0.0, label: html`${tex`\\beta_3`}: Cubic term`})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\hat{Y} = ${b0_cub.toFixed(1)} + ${b1_cub.toFixed(2)}x + ${b2_cub.toFixed(2)}x^2 + ${b3_cub.toFixed(2)}x^3`\n\n\n\n\n\n\n\ncubicCurve = d3.range(cubicXDomain[0], cubicXDomain[1] + 0.05, 0.05).map((x) =&gt; ({\n  x,\n  y: b0_cub + b1_cub * x + b2_cub * x * x + b3_cub * x * x * x\n}))\n\ncubicResiduals = cubicPoints.map((d) =&gt; {\n  const fit = b0_cub + b1_cub * d.x + b2_cub * d.x * d.x + b3_cub * d.x * d.x * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\ncubicMaxAbsResid = d3.max(cubicResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\ncubicYDomain = d3.extent(cubicData.y)\n\nPlot.plot({\n  height: 320,\n  marginLeft: 50,\n  x: {label: \"Predictor (x)\", domain: cubicXDomain},\n  y: {label: \"Outcome (y)\", domain: cubicYDomain},\n  marks: [\n    Plot.link(cubicResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / cubicMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(cubicPoints, {\n      x: \"x\",\n      y: \"y\",\n      r: 3,\n      title: (d) =&gt; `x = ${d.x.toFixed(2)}\\ny = ${d.y.toFixed(2)}`,\n    }),\n    Plot.line(cubicCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.2.2 N-th order univariate model\nWe can extend this process of deriving new terms by adding further powers of \\(X\\) - allowing ius to to fit arbitrarily complex curves: \\[\nE[Y] = \\beta_0 + \\beta_1X +  \\dots + \\beta_nX^n.\n\\]\nNote that each term gets its own parameter (\\(\\beta_i\\)). If we have \\(n\\) data points, a (n-1)^th-order model will have a parameter for each point, meaning it will fit the data perfectly!\n\nExample 4: Fitting higher-order [polynomial terms]\n\nreg= require(\"d3-regression\")\npolyData=transpose(nonlinearData)\nviewof degree = Inputs.range([1, 9], {step: 1, label: \"Model Order\", value: 1})\n\npolyRegression = reg.regressionPoly()\n  .x((d) =&gt; d.x)\n  .y((d) =&gt; d.y)\n  .order(degree)\n  .domain(d3.extent(polyData, (d) =&gt; d.x));\n\npolyCurveRaw = polyRegression(polyData)\npolyCurve = polyCurveRaw.map(([x, y]) =&gt; ({ x, y }))\npolyPredict = (x) =&gt; {\n  if (typeof polyRegression.predict === \"function\") return polyRegression.predict(x);\n  if (polyCurveRaw.coefficients) {\n    return polyCurveRaw.coefficients.reduce((acc, coeff, i) =&gt; acc + coeff * x ** i, 0);\n  }\n  return NaN;\n}\npolyResiduals = polyData.map((d) =&gt; {\n  const fit = polyPredict(d.x);\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\npolyMaxAbsResid = d3.max(polyResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolyTerms = Array.from({length: degree + 1}, (_, k) =&gt; {\n  if (k === 0) return `\\\\beta_{0}`;\n  if (k === 1) return `\\\\beta_{1} x`;\n  return `\\\\beta_{${k}} x^{${k}}`;\n}).join(\" + \");\n\nhtml`&lt;div style=\"text-align:center; margin: 0.5rem 0;\"&gt;${tex`E[Y] = ${polyTerms}`}&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolyYDomain = d3.extent([\n  ...polyData.map((d) =&gt; d.y),\n  ...polyCurve.map((d) =&gt; d.y),\n  ...polyResiduals.map((d) =&gt; d.fit)\n])\n\nPlot.plot({\n  marginLeft: 50,\n  height: 320,\n  x: {label: \"Predictor (x)\"},\n  y: {label: \"Outcome (y)\", domain: polyYDomain},\n  marks: [\n    Plot.link(polyResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / polyMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(polyData, {\n      x: \"x\",\n      y: \"y\",\n      r: 4,\n    }), \n    Plot.line(polyCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2,\n    }),\n  ],\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher-order terms increase the model’s flexibility, but also increase the risk of fitting random noise rather than meaningful structure. We will return to this important issue later in the chapter.\n\nNote: The meaning of “linear” in linear modelsEven though higher-order models can model nonlinear relationships between the outcome and predictors, they are still linear models because each parameter enters additively. “Linearity” here refers to the parameters, not the shape of the fitted curve.\n\n\n\n\n3.1.2.3 Fitting higher-order univariate models in R\nIn R, higher-order polynomial terms can be included using the I() function to indicate ‘as is’ operations. For example, to fit a quadratic model:\n\nlm(y ~ x + I(x^2), data = nonlinearData)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = nonlinearData)\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n     2.6185      -3.3760       0.8218  \n\n\nor using the poly() function:\n\nlm(y ~ poly(x, 2), data = nonlinearData)\n\n\nCall:\nlm(formula = y ~ poly(x, 2), data = nonlinearData)\n\nCoefficients:\n(Intercept)  poly(x, 2)1  poly(x, 2)2  \n     0.3493       0.3941       3.0208  \n\n\n\nContinue\n\n\n\n\n3.1.3 Interaction Models with Continuous Predictors\nIn Chapter 1 we saw a multiple regression model with two continuous predictors:\n\\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2.\n\\]\nThis again is a first-order model - each predictor appears only once, ‘as is’. Of course, we can add higher-order terms for each predictor separately (e.g., \\(X_1^2\\), \\(X_2^3\\)) to capture curvature in their individual effects as we did above. e.g. A second-order multivariate model with two predictors might look like: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1^2 + \\beta_4X_2^2.\n\\]\nNow however, we can also consider a different form of higher order term: what happens when we multiply predictors together? This gives us an interaction term - a term that combines two (or more) predictors.\n\nKey-term: Interaction termAn interaction is a product of predictors that allows the effect of one predictor to depend on the level of another. For two continuous predictors, the second-order interaction model is: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2.\n\\] If \\(\\beta_3 = 0\\) (i.e. as in the first-order model), predictors act independently; if \\(\\beta_3 \\neq 0\\), the slope for \\(X_1\\) varies with \\(X_2\\) and vice versa.\n\n\n\nExample 5: Fitting an interaction model to the advertising dataset.If we recall our plot of the advertising dataset from Section 2.2, our fist-order linear model did not fit the data in particular ways:\nThe first-order model is \\[\nE[Sales] = \\beta_0 + \\beta_1TV + \\beta_2Radio.\n\\tag{3.1}\\]\n\nads &lt;- read.csv(\"Data/Advertising.csv\")\nads_mod &lt;- lm(Sales ~ TV + Radio, data = ads)\n\n\n\n\n\n\n\nThe plane does not fit the data at the edges - it overestimates sales when either Radio or TV advertising is low (separately), but underestimates sales when both are high. This suggests that the effect of increasing one type of advertising depends on the level of the other type - an interaction effect.\nWe can fit a second-order interaction model to capture this: \\[\nE[Sales] = \\beta_0 + \\beta_1TV + \\beta_2Radio + \\beta_3(TV \\times Radio).\n\\tag{3.2}\\]\nThe fitted interaction model has a charateristic ‘saddle’ shape, which can better capture the data structure:\n\n\n\n\n\n\nThe interaction effect can also be visualised by fixing one predictor and plotting the relationship between the other predictor and the outcome. In this case, we can plot lines representing the relationship between TV advertising and Sales when Radio advertising is low ($0 spent), average (i.e. mean(ads$Radio)= 23.264 thousand dollars ), and high (i.e. max(ads$Radio)= $49.6 thousand dollars):\n\n\n\n\n\n\n\n\n\nWe can see that the relationship between TV advertising and Sales (represented by the slope of the regression lines) is different at different levels of Radio advertising. In particular, as radio advertising increases, the slope of the line increases, indicating that TV advertising has a larger effect on Sales when Radio advertising is also high.\n\n\n\nContinue\n\nWhen our first order model has more than two predictors, we can include interaction terms between any pair (or more) of predictors. For example, with three predictors \\(X_1\\), \\(X_2\\), and \\(X_3\\), a second-order interaction model would include interactions between each pair: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_1X_2 + \\beta_5X_1X_3 + \\beta_6X_2X_3.\n\\]\n\n3.1.3.1 Higher-order interaction models\nWe can also consider higher-order interactions - those that include combinations of three or more predictors. For example, a third-order interaction model with three predictors would include the ‘three-way’ interaction term: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_1X_2 + \\beta_5X_1X_3 + \\beta_6X_2X_3 + \\beta_7X_1X_2X_3.\n\\] The \\(\\beta_7\\) parameter here captures how the interaction between two predictors changes depending on the level of the third predictor.\nAs in the case of higher-order univariate models, we can extend this idea to include both higher-order interaction terms and higher-order univariate terms for each predictor, allowing for very flexible modeling of complex relationships.\nHowever, as before, increasing model complexity in this way raises the risk of overfitting and interpretability challenges, which we will discuss later in this chapter.\n\n\nContinue\n\n\n\n3.1.3.2 Fitting interaction models in R\nIn R, interaction terms can be included using the * operator in the formula. For example, to fit a second-order interaction model with two predictors:\n\nlm(Y ~ X1*X2*X3, data = mydata)\n\nThis expands to include both main effects and the interaction term. To include only the interaction term without main effects, use the : operator. For example, if we only wanted the interaction between X1 and X2, along with the linear terms for X1, X2, and X3:\n\nlm(Y ~ X1+X2+X3+X1:X2, data = mydata)\n\n\nContinue\n\n\n\n\n3.1.4 Interaction Models with Categorical Predictors\nAs in Section 2.5, a categorical predictor with \\(k\\) levels is represented by \\(k-1\\) dummy variables: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma_1D_1 + \\dots + \\gamma_{k-1}D_{k-1}.\n\\]\n\nEach \\(\\gamma_j\\) measures the difference between level \\(j\\) and the baseline.\nChanging the baseline changes interpretations but not fitted values.\n\n\n\n\n\n\n\n\n\n\n3.1.4.1 Continuous \\(\\times\\) categorical interactions\nWhen a continuous predictor interacts with a factor, each group can have its own slope: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma D + \\delta (XD),\n\\] where \\(D\\) indicates the non-reference group. The baseline group has slope \\(\\beta_1\\), and the other group has slope \\(\\beta_1 + \\delta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe interaction term tells us whether the relationship between weight and fuel economy differs between automatic and manual cars (an ANCOVA-style comparison).\n\n\n3.1.4.2 Categorical \\(\\times\\) categorical interactions\nWith two factors, an interaction means the difference between levels of one factor depends on the level of the other. This is the two-way ANOVA setting with an interaction term.\n\n\n\n\n\n\n\n\nIf the interaction term is significant, the transmission effect is not consistent across cylinder groups.\n\nNoteWe saw in Section 2.5 that first order models with categorical predictors are equivalent to the familiar t-tests and ANOVA models from earlier statistics courses. Interaction models with both continuous and categorical predictors are similarly analogous to “ANCOVA” models.\n\n\n\nExercise 1: Interpreting a continuous-by-categorical interaction\nFit mpg ~ wt * am_f on mtcars_cat.\n\nWhich group has the steeper slope for weight?\nWhat does the interaction term represent in words?\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building and Variable Selection</span>"
    ]
  },
  {
    "objectID": "ch3-model-building.html#sec-assessing-model-fit-and-model-complexity",
    "href": "ch3-model-building.html#sec-assessing-model-fit-and-model-complexity",
    "title": "3  Model Building and Variable Selection",
    "section": "3.2 Assessing model fit and model complexity",
    "text": "3.2 Assessing model fit and model complexity\n\n3.2.1 Why not increase model complexity?\nIn the previous section we saw how adding higher-order terms and interactions can help us capture complex relationships in data. However, increasing model complexity is not always beneficial. While more complex models can fit the training data (i.e. the data we use to fit our model) better, they may not generalise well to new data. This is because complex models can start to fit random noise in the training data, rather than the underlying signal. This phenomenon is known as overfitting.\nIn addition, more complex models can be harder to interpret, making it difficult to understand the relationships between predictors and the outcome. They may also be more sensitive to outliers and multicollinearity, leading to unstable parameter estimates.\nA good model is no more complex than necessary to describe the main structure of the data. Therefore, when building regression models, we need to balance the trade-off between model fit and model complexity. In this section, we discuss some common issues that arise with complex models, and metrics for assessing model fit while accounting for complexity.\n\nKey-point: Balancing fit and complexityA good regression model balances: * Adequate fit to the data, * Simplicity and interpretability, * Generalisability to new data.\n\n\n\nContinue\n\n\n\n3.2.2 Fit Metrics\nFit metrics quantify how well a model captures patterns in the data. We have already seen one such metric: the coefficient of determination, \\(R^2\\).\n\nKey-term: Coefficient of Determination (\\(R^2\\))\\[\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n\\] where \\(SS_{res}\\) is the residual sum of squares and \\(SS_{tot}\\) is the total sum of squares. \\(R^2\\) measures the proportion of variance in the outcome explained by the model, ranging from 0 (no explanatory power) to 1 (perfect fit).\n\n\n\\(R^2\\) was introduced in Chapter 1 as a measure of fit for simple linear regression models based soely on the proportion of variance in \\(Y\\) explained by \\(X\\). We can also calculate \\(R^2\\) for multiple regression models, where it measures the proportion of variance in \\(Y\\) explained by all predictors jointly. However, \\(R^2\\) has a key limitation: it always increases (or at least does not decrease) when additional predictors are added to the model, even if those predictors do not meaningfully improve the model’s explanatory power. This can lead to overfitting, where a model appears to fit the training data well but performs poorly on new data - for example, the (n-1)th order polynomial model discussed in Section 3.1.2.2 has A SSres of zero and thus an \\(R^2\\) of 1, but is unlikely to generalise well.\n\n3.2.2.1 [Adjusted \\(R^2\\)]\nTo address this limitation, we can adjust \\(R^2\\) to penalise the addition of unnecessary predictors. The adjusted \\(R^2\\) introduces a penalty term based on the number of predictors and the sample size, providing a more balanced measure of model fit that accounts for complexity.\n\nKey-term: [Adjusted \\(R^2\\)]\\[\n\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n - 1}{n - k - 1}\n\\] where \\(n\\) is the sample size and \\(k\\) is the number of predictors. Adjusted \\(R^2\\) can decrease when unnecessary predictors are added, making it a more reliable metric than \\(R^2\\) for model selection. Can be computed in R using summary(lm_model)$adj.r.squared.\n\n\nAdjusted \\(R^2\\) is particulary useful because of it’s standardised scale (0 to 1), and the interpretation of \\(R^2\\) as the proportion of variance explained still holds - now with the added benefit of penalising unnecessary complexity. However, it is important to note that adjusted \\(R^2\\) is just one of many metrics available for assessing model fit while accounting for complexity.\n\n\n3.2.2.2 Information Criteria: AIC, BIC\nAnother common approach to balancing model fit and complexity is to use information criteria, such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These criteria provide a quantitative way to compare models, penalising those with more parameters.\n\nKey-term: [Akaike Information Criterion (AIC)]\\[\n\\text{AIC} = 2k - 2\\ln(L)\n\\] where \\(k\\) is the number of parameters in the model and \\(L\\) is the likelihood of the model. Lower AIC values indicate a better balance between fit and complexity. Can be computed in R using AIC() function.\n\n\n\nKey-term: [Bayesian Information Criterion (BIC)]\\[\n\\text{BIC} = \\ln(n)k - 2\\ln(L)\n\\] where \\(n\\) is the sample size, \\(k\\) is the number of parameters, and \\(L\\) is the likelihood of the model. Like AIC, lower BIC values indicate a better balance between fit and complexity. Can be computed in R using BIC() function.\n\n\nBoth AIC and BIC penalise model complexity, but BIC generally imposes a larger penalty for additional parameters, especially in larger samples. The choice between AIC and BIC often depends on the context and goals of the analysis. AIC is generally preferred for predictive accuracy, while BIC is more conservative and favours simpler models.\n\nKey-point: Interpreting AIC and BICUnlike adjusted \\(R^2\\), AIC and BIC do not have a fixed scale, so their absolute values are not interpretable on their own. Instead, they are used to compare models fitted to the same dataset - the model with the lowest AIC or BIC is considered the best among the candidates.\n\n\n\n\n3.2.2.3 Calculating fit metrics with broom::glance()\nIn R, we can calculate adjusted \\(R^2\\), AIC, and BIC using the broom package’s glance() function, which provides a tidy summary of model fit statistics. For example, using our fitted 2nd-order interaction model of the advertising dataset:\n\nlibrary(broom)\nglance(ads_mod_int)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.968         0.967 0.944     1963. 6.68e-146     3  -270.  550.  567.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nNote that glance() returns a data frame with the mentioned fit metrics, along with other useful statistics such as the number of observations (nobs), residual standard error (sigma) and the global F-statistic (statistic). Alongside tidy(), glance() is a powerful tool for summarising and comparing regression models in R.\n\n\n3.2.2.4 Comparing model fit\nWhen comparing multiple models fitted to the same dataset, we can use adjusted \\(R^2\\), AIC, and BIC to assess which model provides the best balance between fit and complexity: the key points to remember are:\n\nHigher adjusted \\(R^2\\) values indicate better fit.\nLower AIC and BIC values indicate better fit.\n\n\nExample 6: Comparing first-order and interaction modelsLets continue our investigation of the advertising dataset from Section 2.2 by comparing our first-order model of Sales (ads_mod) to our second-order interaction model (ads_mod_int).\nLets combine the fit metrics from both models into a single table for easy comparison:\n\nTable of fit metricsCode\n\n\n\n\n# A tibble: 2 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n*     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.897         0.896 1.68       860. 4.83e- 98     2  -386.  780.  794.\n2     0.968         0.967 0.944     1963. 6.68e-146     3  -270.  550.  567.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nads_fit_glance &lt;- rbind(\n  glance(ads_mod) |&gt; `rownames&lt;-`(\"First-order model\"),\n  glance(ads_mod_int) |&gt; `rownames&lt;-`(\"Interaction model\"))\n\nads_fit_glance\n\n\n\n\nSince the interaction model Equation 3.2 differs from the first-order model Equation 3.1 by the addition of only the \\(TV \\times Radio\\) interaction term, we can interpret the change in fit metrics as the change in model fit due to adding this term. From the table above, we can see that the interaction model has a higher adjusted \\(R^2\\) and lower AIC and BIC values compared to the first-order model. This indicates that adding the interaction term improves the model’s fit to the data, even after accounting for the increased complexity.\nMoreover, we can test the hypothesis that the interaction term significantly improves model fit using an ANOVA comparison:\n\nanova(ads_mod, ads_mod_int)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ TV + Radio\nModel 2: Sales ~ TV + Radio + TV:Radio\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    197 556.91                                  \n2    196 174.48  1    382.43 429.59 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows a significant p-value for the comparison, indicating that the interaction model provides a significantly better fit to the data than the first-order model. Therefore, we conclude that including the interaction term is justified in this case.\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building and Variable Selection</span>"
    ]
  },
  {
    "objectID": "ch3-model-building.html#sec-a-model-building-workflow",
    "href": "ch3-model-building.html#sec-a-model-building-workflow",
    "title": "3  Model Building and Variable Selection",
    "section": "3.3 A model building workflow",
    "text": "3.3 A model building workflow\nSo far in this chapter we have discussed how to build more complex regression models using higher-order terms and interactions, as well as how to assess model fit while accounting for complexity. These are important tools for building effective regression models, and we now turn to strategies for applying these tools in practice.\n\n3.3.1 Exploratory data analysis and theory-driven model development\nBefore fitting any models, it is crucial to conduct exploratory data analysis (EDA) to understand the relationships between variables and identify potential patterns or anomalies. Visualisation and summary statistics can help identify non-linear relationships, interactions, and outliers that may inform model development. They can also highlight potential issues such as multicollinearity or heteroscedasticity that may need to be addressed (we will cover these potential pitfalls in the next chapter).\n\n3.3.1.1 Pairs plots and correlation matrices\nPairs plots and correlation matrices are useful tools for visualising relationships between multiple variables simultaneously. They can help identify potential predictors for inclusion in the model, as well as potential interactions or non-linear relationships. For example, a pairs plot can show scatterplots of each pair of variables, along with histograms of each variable on the diagonal. A correlation matrix can show the strength and direction of linear relationships between variables. We will use the `ggpairs() function from the GGally package (an extension to ggplot2) to create pairs plots in R\n\nlibrary(GGally)\nggpairs(ads)\n\n\n\n\n3.3.2 Automated model selection methods\n\n\n3.3.3 [Stepwise regression]\nStepwise methods provide automated ways to search for simpler models.\n\n3.3.3.1 Forward Selection\nBegin with a minimal model (often the intercept). Add predictors one at a time when they improve AIC or adjusted \\(R^2\\).\n\n\n3.3.3.2 Backward Selection\nBegin with a saturated model containing all candidate predictors. Remove predictors that do not meaningfully contribute.\n\nNoteStepwise procedures should be treated as screening tools, not definitive modelling strategies. Final model decisions should consider diagnostics, interpretability, and subject-matter knowledge.\n\n\n\nExample 7: Samara: comparing seeds across treesMaple samara (seeds) fall like tiny helicopters. A forest scientist measured their fall speed (Velocity) against their ‘disk loading’ (a quantity based on their size and weight; encoded asLoad) on three trees (Tree). The goal: check whether the rate of change in velocity differs between trees.\n\nsamara &lt;- read_csv(\"Data/samara.csv\", show_col_types = FALSE) |&gt;\n  mutate(Tree = factor(Tree))\n\n\n\n\n\n\n\n\n\n\nThe slopes look similar but not identical. Perhaps tree identity matters, and perhaps the slopes differ slightly by tree. We can compare three candidate models:\nCandidate models\n\nsam_mods &lt;- list(\n  same_slope   = lm(Velocity ~ Load, data = samara),\n  add_tree     = lm(Velocity ~ Load + Tree, data = samara),\n  interaction  = lm(Velocity ~ Load * Tree, data = samara)\n)\n\nANOVA and fit metrics\n\nanova(sam_mods$same_slope, sam_mods$add_tree, sam_mods$interaction)\n\nAnalysis of Variance Table\n\nModel 1: Velocity ~ Load\nModel 2: Velocity ~ Load + Tree\nModel 3: Velocity ~ Load * Tree\n  Res.Df     RSS Df Sum of Sq     F  Pr(&gt;F)  \n1     33 0.21476                             \n2     31 0.20344  2  0.011322 0.992 0.38306  \n3     29 0.16549  2  0.037949 3.325 0.05011 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsam_fit &lt;- names(sam_mods) |&gt;\n map_dfr(\\(nm) glance(sam_mods[[nm]]) |&gt;\n           mutate(model = nm, formula = format(formula(sam_mods[[nm]])))) |&gt;\n select(model, formula, adj_r2 = adj.r.squared, AIC, BIC)\n\nsam_fit\n\n# A tibble: 3 × 5\n  model       formula                adj_r2   AIC   BIC\n  &lt;chr&gt;       &lt;chr&gt;                   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 same_slope  Velocity ~ Load         0.791 -72.9 -68.3\n2 add_tree    Velocity ~ Load + Tree  0.789 -70.8 -63.1\n3 interaction Velocity ~ Load * Tree  0.817 -74.1 -63.2\n\n\nTree alone does not add much (p ≈ 0.38). The interaction is borderline (p ≈ 0.05) and has the lowest AIC/BIC, hinting that slopes might vary slightly by tree.\nForward vs backward stepwise selection\n\nscope &lt;- list(lower = ~1, upper = ~Load * Tree)\nstep_forward  &lt;- step(lm(Velocity ~ 1, samara), scope = scope,\n                      direction = \"forward\", trace = 0)\nstep_backward &lt;- step(lm(Velocity ~ Load * Tree, samara), scope = scope,\n                      direction = \"backward\", trace = 0)\n\nc(forward  = format(formula(step_forward)),\n  backward = format(formula(step_backward)))\n\n                 forward                 backward \n       \"Velocity ~ Load\" \"Velocity ~ Load * Tree\" \n\nAIC(step_forward, step_backward)\n\n              df       AIC\nstep_forward   3 -72.94911\nstep_backward  7 -74.07063\n\n\nForward selection stops at the simple Load-only model; backward selection keeps the interaction. Their AIC values are close, but the interaction edges ahead, so retain it if tree-specific slopes matter for interpretation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Model Building and Variable Selection</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html",
    "href": "ch4-pitfalls-diagnostics-remedies.html",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "",
    "text": "4.1 Common assumptions and pitfalls\nIn Chapter 3 we saw that regression models can go wrong due to overfitting—fitting to noise rather than signal in the data. But even a simple model can be misleading if its assumptions are violated, or if a small number of observations have an outsized effect on the fit.\nIn this chapter we focus on the ways in which linear regression can go wrong, how to diagnose problems using residuals, and what to do when issues arise.\nIn ?sec-introduction-to-linear-models, we introduced the basic assumptions for constructing a simple linear model model of the relationship between two variables. In Chapter 2 we extended these these assumptions to the multiple predictor setting. As a reminder, a linear regression analysis assumes that the data were generated according to something of following form:\nBy assuming this form we can proceed to estimate the parameters \\(b_0, b_1, \\ldots, b_p\\) and \\(s^2\\) using the method of least squares, and then use these estimates to perform inference and prediction. However, if it is not the case that the data were generated according to this form, then our estimates, inferences, and predictions may be misleading.\nIt is often the case, some aspects of 1 may be violated while others are still reasonably satisfied. For example, a model may have nonconstant variance but still have linear mean structure and approximately normal errors. For this reason, we often break the general assumption above into more specific parts when performing model diagnostics. A common, equivalent way to state present the assumptions of a linear model is via the following four assumptions about the error terms \\(\\epsilon_i\\):\nWhen\nOther common pitfalls that can undermine interpretation include:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html#sec-common-assumptions-and-pitfalls",
    "href": "ch4-pitfalls-diagnostics-remedies.html#sec-common-assumptions-and-pitfalls",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "",
    "text": "Assumption 2: LinearityThe relationship between the predictors and the response is linear (in the parameters) \\[\nY_i ~ \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\cdots + \\beta_p X_{ip}\n\\] Where\n\n\n\nAssumption 3: HomoscedasticityThe variance of the errors is constant across the range of predictors: \\[\n\\text{Var}(\\epsilon_i) = \\sigma^2\n\\]\n\n\n\nAssumption 4: NormalityThe errors are approximately normally distributed: \\[\n\\epsilon_i \\sim \\text{Normal}(0, \\sigma^2)\n\\]\n\n\n\nAssumption 5: IndependenceThe errors are independent: \\(\\epsilon_i\\) is independent of \\(\\epsilon_j\\) for \\(i \\neq j\\).\n\n\n\n\n\nInfluential points: observations that strongly affect estimates.\nMulticollinearity: predictors that are highly correlated, inflating uncertainty.\nExtrapolation: making predictions outside the range of observed data.\nBlind model selection: using automated procedures without scientific context or diagnostic checks.\n\n\nKey-pointDiagnostics are not an optional add-on—they are part of the modelling workflow. A good model should have residuals that look like structureless noise.\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html#sec-the-importance-of-checking-residuals-anscombe-s-quartet",
    "href": "ch4-pitfalls-diagnostics-remedies.html#sec-the-importance-of-checking-residuals-anscombe-s-quartet",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "4.2 Why check residuals? Anscombe’s quartet",
    "text": "4.2 Why check residuals? Anscombe’s quartet\n\nExample 1Anscombe’s quartet is a classic reminder that identical regression summaries can mask radically different data patterns. Each dataset below has the same \\(n\\), mean \\(x\\), mean \\(y\\), correlation, and the same fitted regression line, yet the residual plots tell very different stories.\n\n\n\n\n\n\n\n\n\nScatter plots (with fitted line)Residual plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResidual plots often reveal problems that are easy to miss from summaries and even from the original scatter plot (especially when you have many predictors).\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html#sec-residuals-as-diagnostic-tools",
    "href": "ch4-pitfalls-diagnostics-remedies.html#sec-residuals-as-diagnostic-tools",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "4.3 Residuals as diagnostic tools",
    "text": "4.3 Residuals as diagnostic tools\nHow can we check whether the assumptions of a linear regression model are met? One of the most common approaches is to examine the residuals of the model. Residuals are the part of the data that your model did not explain.\n\nKey-term: ResidualThe residual for observation \\(i\\) in a regression model is defined as \\[e_i = y_i - \\hat{y}_i\\] where \\(e_i\\) is the residual for observation \\(i\\), \\(y_i\\) is the observed value, and \\(\\hat{y}_i\\) is the predicted value from the model.\n\n\nWe used residuals in Section 1.3.2 to estimate the parameters of the linear model, but they can also be used to diagnose model problems. Since residuals encode the information about what the model has not captured, plotting residuals can reveal systematic patterns such as:\n\ncurvature (missing nonlinearity),\nchanging spread (nonconstant variance),\nextreme points (outliers/influence),\ndepartures from normality.\n\n\nExample 2: Residuals in the Olympic cholesterol dataThe OLYMPIC dataset records fat intake (mg) and cholesterol (mg/L) for 20 athletes. A simple linear model leaves a curved pattern in the residuals.\n\n\n\n\n\n\n\n\n\nScatter plot (with fitted line)Residuals vs fittedResiduals vs FAT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe curved pattern indicates that a straight-line model is missing structure in the data (a sign of nonlinearity).\n\n\n\n4.3.1 Extracting diagnostics with broom::augment()\nWe saw in Section 1.2.4 how to extract residuals using resid(). A convenient alternative is broom::augment(), which returns a data frame containing the original data plus fitted values, residuals, and several diagnostic quantities.\n\nExample 3: Olympic residuals with broom::augment()\n\n\n\n\n\n\n\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html#sec-common-diagnostic-plots",
    "href": "ch4-pitfalls-diagnostics-remedies.html#sec-common-diagnostic-plots",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "4.4 A small set of diagnostic plots",
    "text": "4.4 A small set of diagnostic plots\nIn practice you can learn a lot from a small, repeatable set of plots. The most common ones correspond closely to the default plot(lm_object) panel in base R:\n\nResiduals vs fitted: check linearity and constant variance.\nNormal Q–Q: assess whether residuals are approximately normal.\nScale–location: another view of nonconstant variance.\nResiduals vs leverage (Cook’s distance): identify influential observations.\n\n\nNoteDiagnostics are graphical because many important failures are about patterns. A model summary is a single snapshot; a diagnostic plot is a pattern detector.\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs fittedNormal Q–QScale–locationResiduals vs leverage\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey-pointIf residuals show a clear pattern, that pattern is telling you what your model is missing.\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html#sec-diagnosing-issues-and-choosing-remedies",
    "href": "ch4-pitfalls-diagnostics-remedies.html#sec-diagnosing-issues-and-choosing-remedies",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "4.5 Diagnosing issues and choosing remedies",
    "text": "4.5 Diagnosing issues and choosing remedies\nThis section focuses on the most common “what now?” moves: what to look for, and what you might do when you see it.\n\n4.5.1 Nonlinearity (missing curvature)\nSymptoms\n\nA curved trend in residuals vs fitted (or residuals vs a predictor).\n\nCommon remedies\n\nAdd polynomial terms (Chapter 03), transformations, or interactions.\nConsider omitted predictors that explain structure in the residuals.\n\n\nExample 4: A residual curve suggests missing nonlinearity\n\n\n\n\n\n\n\n\n\nLinear model residualsQuadratic model residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 Nonconstant variance (heteroskedasticity)\nSymptoms\n\nA “funnel” shape in residuals vs fitted (spread increases or decreases with the fitted value).\nA trend in the scale–location plot.\n\nCommon remedies\n\nTransform the response (e.g., log or square-root) to stabilise variance.\nUse weighted least squares if you can model how variance changes.\nUse robust standard errors when inference is the main goal.\n\n\nExample 5: A funnel shape suggests heteroskedasticity\n\n\n\n\n\n\n\n\n\nResiduals (raw scale)Residuals (log-transformed response)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3 Normality (and when it matters)\nNormality of errors matters most for small-sample \\(t\\)-tests and confidence intervals. In large samples, linear regression often remains useful even when residuals are not perfectly normal, but it is still worth checking for strong departures caused by outliers or skewness.\nSymptoms\n\nStrong curvature or heavy tails in the Q–Q plot.\n\nCommon remedies\n\nCheck for outliers/data errors first.\nConsider a transformation of the response.\nConsider robust or resampling-based inference.\n\n\n\n4.5.4 Independence (autocorrelation)\nThe independence assumption is most commonly violated when observations are collected over time (or space). If residuals are correlated, standard errors can be too small, and \\(p\\)-values can look “better” than they should.\nSymptoms\n\nResiduals show runs/clustering when plotted against time/order.\nResiduals have clear autocorrelation (e.g., via an ACF plot).\n\nCommon remedies\n\nAdd time/seasonality terms if they explain structure.\nUse models designed for correlated errors (time series, mixed models, GLS).\n\n\nExample 6: Residuals over time can reveal dependence\n\n\n\n\n\n\n\n\n\nResiduals vs timeACF of residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.5 Standardised residuals, leverage, and Cook’s distance\nInfluence diagnostics answer a different question than “is the relationship real?” They ask: are my conclusions stable if I perturb the data slightly?\n\nKey-term: Standardised residuals and influence diagnostics\n\nStandardised residuals scale residuals by an estimate of their variability.\nLeverage identifies observations with unusual predictor values.\nCook’s distance combines leverage and residual size to measure influence.\n\n\n\n\nNoteRules of thumb (use with caution): observations with |standardised residual| &gt; 2 may be unusual; Cook’s distance values that stand out relative to the rest merit investigation.\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.6 Extrapolation and high-leverage prediction\nExtrapolation means predicting for predictor values outside the range you observed. It is risky because the relationship itself may change outside the observed region, and because uncertainty grows rapidly when you move away from the data cloud (often showing up as high leverage).\n\nKey-pointBefore you trust a prediction, check whether it is an extrapolation. A simple first step is to compare each new predictor value to the observed range.\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.7 Transformations including Box–Cox\nTransformations can help when residual plots show curvature or nonconstant variance. A systematic way to explore power transformations of the response is the Box–Cox transformation.\n\nNoteBox–Cox requires the response to be strictly positive.\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.8 Multicollinearity\nMulticollinearity occurs when predictors are strongly correlated. It does not necessarily make predictions bad, but it can make coefficient estimates unstable and inflate standard errors—so interpretation and “which predictor matters?” questions become fragile.\nSymptoms\n\nLarge standard errors and coefficient sign changes when you add/remove a correlated predictor.\nLarge variance inflation factors (VIFs).\n\nCommon remedies\n\nDrop or combine redundant predictors (guided by scientific context).\nCollect more data (especially with more diverse predictor combinations).\nUse dimension reduction or penalised regression when prediction is the goal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.9 Blind model selection\nAutomated selection procedures (stepwise, “try everything”, repeatedly adding/removing predictors until something is significant) can be useful for exploration, but they are easy to misuse.\nPitfalls\n\n\\(p\\)-values and confidence intervals become hard to interpret after extensive searching.\nSelected models can overfit and can be unstable to small data changes.\n\nRemedies\n\nUse scientific context to limit the candidate set.\nUse out-of-sample validation (Chapter 03) and always re-check residual diagnostics on the final model.\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html#sec-chapter04-exercises",
    "href": "ch4-pitfalls-diagnostics-remedies.html#sec-chapter04-exercises",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\nThe goal of these exercises is to practise diagnosing patterns and choosing a sensible next step.\n\n4.6.1 Exercise 1: Identify heteroskedasticity\nRun the simulation below and look at the residual plot. Then answer: what issue is the plot mainly suggesting?\n\n\n\n\n\n\n\n\n\nExercise 1\nWrite your answer as a short phrase (e.g., \"heteroskedasticity\" or \"nonconstant variance\").\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nissue_4_1 &lt;- \"heteroskedasticity\"\nissue_4_1 &lt;- \"heteroskedasticity\"\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n4.6.2 Exercise 2: Use VIF to spot multicollinearity\nFit the model and compute VIFs. Which term has the largest VIF?\n\n\n\n\n\n\n\n\n\nExercise 2\nSet worst_term equal to the term with the largest VIF.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nworst_term &lt;- vif_from_lm(ex2_mod)$term[1]\nworst_term &lt;- vif_from_lm(ex2_mod)$term[1]\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\n4.6.3 Exercise 3: Interpret a Box–Cox suggestion\nIn the Box–Cox plot earlier, we computed lambda_hat. Based on its value, which option is the best rough interpretation?\n\nExercise 3\nChoose one: \"no transform\", \"log transform\", or \"square-root transform\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nboxcox_choice &lt;- if (abs(lambda_hat) &lt; 0.25) \"log transform\" else if (abs(lambda_hat - 0.5) &lt; 0.25) \"square-root transform\" else \"no transform\"\nboxcox_choice &lt;- if (abs(lambda_hat) &lt; 0.25) \"log transform\" else if (abs(lambda_hat - 0.5) &lt; 0.25) \"square-root transform\" else \"no transform\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch4-pitfalls-diagnostics-remedies.html#sec-handling-outliers-and-influential-observations",
    "href": "ch4-pitfalls-diagnostics-remedies.html#sec-handling-outliers-and-influential-observations",
    "title": "4  Pitfalls, Diagnostics, and Remedies",
    "section": "4.7 Handling outliers and influential observations",
    "text": "4.7 Handling outliers and influential observations\nWhen diagnostics flag influential observations, slow down and investigate rather than deleting points automatically.\n\nCheck data quality first (entry errors, unusual units, miscoding).\nIf influential points are real, fit with and without them to assess robustness; report how conclusions change.\nPrefer model improvements (functional form, additional predictors, variance stabilisation) over deletion.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Pitfalls, Diagnostics, and Remedies</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html",
    "href": "ch5-case-naplan.html",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "5.1 Defining the research question\nIn Chapters 00–04 we built up the pieces of a regression analysis: what a linear model is (Chapter 00), how to fit and interpret regression models (Chapters 01–02), how to build and compare candidate models (Chapter 03), and how to diagnose problems (Chapter 04). In this final chapter, we put those pieces together in an end-to-end analysis.\nOur workflow follows the model-building approach in Section 3.3:\nThis chapter is designed as a hands-on walkthrough. The webR chunks are the analysis: run each one as you read. Only use “Skip exercise” if you’ve already completed that step (e.g. in your IDE) and just want to keep moving.\nThis case study uses naplan_reading.csv (in this project: Data/naplan_reading.csv), which contains NAPLAN reading achievement data for 3,000 Australian students across Years 3, 5, 7, and 9, drawn from 60 schools (school_id). Each row is one student.\nThe outcome variable is naplan_reading_score. Because scores are on different ranges across grades (roughly 100–600 in Year 3 vs 400–900 in Year 9), we include grade as a covariate throughout.\nAlongside the outcome, the dataset includes student and family predictors such as weekly reading time at home (in minutes), parent education, school type, gender, a socioeconomic status index, birth month, and number of siblings.\nResearch question: How are students’ socioeconomic circumstances, reading practice at home, and demographic characteristics associated with NAPLAN Reading scores?\nIn this dataset:\nAlso note that students come from 60 schools (school_id), so the independence assumption may be imperfect. Handling clustering rigorously would typically use multilevel modelling, which is beyond the scope of this short course.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-defining-the-research-question",
    "href": "ch5-case-naplan.html#sec-defining-the-research-question",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "The outcome is naplan_reading_score.\nCandidate predictors include:\n\nreading practice: reading_time_home (minutes per week),\nsocioeconomic measures: ses_index and parent_education,\ndemographics: gender, birth_months, n_siblings,\nschooling context: school_type,\nschooling level: grade (Year 3/5/7/9, included as a covariate).\n\n\n\nNoteThis is an observational dataset. Regression estimates here describe associations, not “effects”. For reminders about modelling assumptions, see Section 4.1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-importing-data",
    "href": "ch5-case-naplan.html#sec-importing-data",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "5.2 Importing and preparing the data",
    "text": "5.2 Importing and preparing the data\nWe start by importing the dataset and setting factor levels so coefficient interpretations are clear (see Section 2.5 and Section 2.5.4).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nRun the chunk below to import and prepare the data. The glimpse() output shows all available variables, and the small dictionary table summarises what each variable represents.\nWhen you’re done, click Check to continue. If you’ve already done this step (e.g. in your IDE), click Skip exercise instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnaplan &lt;- read_csv(\"Data/naplan_reading.csv\", show_col_types = FALSE) |&gt;\n  mutate(\n    grade = factor(grade, levels = c(\"Year 3\", \"Year 5\", \"Year 7\", \"Year 9\")),\n    parent_education = factor(\n      parent_education,\n      levels = c(\n        \"Year 10 or below\",\n        \"Year 12\",\n        \"Certificate/Diploma\",\n        \"Bachelor degree\",\n        \"Postgraduate\"\n      )\n    ),\n    school_type = factor(school_type, levels = c(\"Government\", \"Catholic\", \"Independent\")),\n    gender = factor(gender, levels = c(\"Female\", \"Male\")),\n    birth_months = factor(\n      birth_months,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    )\n  )\n\nglimpse(naplan)\n\nnaplan_dictionary &lt;- tribble(\n  ~variable, ~description,\n  \"student_id\", \"Student identifier\",\n  \"school_id\", \"School identifier (60 schools)\",\n  \"grade\", \"Year level (Year 3/5/7/9)\",\n  \"reading_time_home\", \"Minutes of reading at home per week\",\n  \"parent_education\", \"Parents' highest education level\",\n  \"school_type\", \"School sector (Government/Catholic/Independent)\",\n  \"gender\", \"Student gender\",\n  \"birth_months\", \"Birth month (Jan–Dec)\",\n  \"n_siblings\", \"Number of siblings\",\n  \"ses_index\", \"Socioeconomic status index (continuous)\",\n  \"naplan_reading_score\", \"NAPLAN reading score (outcome)\"\n)\n\nnaplan_dictionary\n\nnaplan |&gt;\n  select(\n    student_id, school_id, grade, naplan_reading_score, ses_index, reading_time_home,\n    parent_education, school_type, gender, birth_months, n_siblings\n  ) |&gt;\n  slice_head(n = 8)\n\nnaplan |&gt;\n  summarise(\n    n = n(),\n    min_score = min(naplan_reading_score),\n    max_score = max(naplan_reading_score),\n    mean_score = mean(naplan_reading_score),\n    sd_score = sd(naplan_reading_score),\n    n_schools = n_distinct(school_id)\n  )\n\nnaplan |&gt;\n  count(grade) |&gt;\n  arrange(grade)\nnaplan &lt;- read_csv(\"Data/naplan_reading.csv\", show_col_types = FALSE) |&gt;\n  mutate(\n    grade = factor(grade, levels = c(\"Year 3\", \"Year 5\", \"Year 7\", \"Year 9\")),\n    parent_education = factor(\n      parent_education,\n      levels = c(\n        \"Year 10 or below\",\n        \"Year 12\",\n        \"Certificate/Diploma\",\n        \"Bachelor degree\",\n        \"Postgraduate\"\n      )\n    ),\n    school_type = factor(school_type, levels = c(\"Government\", \"Catholic\", \"Independent\")),\n    gender = factor(gender, levels = c(\"Female\", \"Male\")),\n    birth_months = factor(\n      birth_months,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n    )\n  )\n\nglimpse(naplan)\n\nnaplan_dictionary &lt;- tribble(\n  ~variable, ~description,\n  \"student_id\", \"Student identifier\",\n  \"school_id\", \"School identifier (60 schools)\",\n  \"grade\", \"Year level (Year 3/5/7/9)\",\n  \"reading_time_home\", \"Minutes of reading at home per week\",\n  \"parent_education\", \"Parents' highest education level\",\n  \"school_type\", \"School sector (Government/Catholic/Independent)\",\n  \"gender\", \"Student gender\",\n  \"birth_months\", \"Birth month (Jan–Dec)\",\n  \"n_siblings\", \"Number of siblings\",\n  \"ses_index\", \"Socioeconomic status index (continuous)\",\n  \"naplan_reading_score\", \"NAPLAN reading score (outcome)\"\n)\n\nnaplan_dictionary\n\nnaplan |&gt;\n  select(\n    student_id, school_id, grade, naplan_reading_score, ses_index, reading_time_home,\n    parent_education, school_type, gender, birth_months, n_siblings\n  ) |&gt;\n  slice_head(n = 8)\n\nnaplan |&gt;\n  summarise(\n    n = n(),\n    min_score = min(naplan_reading_score),\n    max_score = max(naplan_reading_score),\n    mean_score = mean(naplan_reading_score),\n    sd_score = sd(naplan_reading_score),\n    n_schools = n_distinct(school_id)\n  )\n\nnaplan |&gt;\n  count(grade) |&gt;\n  arrange(grade)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nAs a first quick EDA summary, create a table called score_by_grade with the mean NAPLAN reading score for each grade, sorted from highest to lowest.\nWhen it looks right, click Check to continue (or Skip exercise if you did this in your IDE).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscore_by_grade &lt;- naplan |&gt;\n  group_by(grade) |&gt;\n  summarise(mean_score = mean(naplan_reading_score)) |&gt;\n  arrange(desc(mean_score))\n\nscore_by_grade\nscore_by_grade &lt;- naplan |&gt;\n  group_by(grade) |&gt;\n  summarise(mean_score = mean(naplan_reading_score)) |&gt;\n  arrange(desc(mean_score))\n\nscore_by_grade",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-exploratory-plot",
    "href": "ch5-case-naplan.html#sec-exploratory-plot",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "5.3 Exploratory data analysis",
    "text": "5.3 Exploratory data analysis\nBefore we fit any model, we look for:\n\nobvious trends (linear vs curved),\ndifferences across groups (categorical predictors),\npotential multicollinearity (predictors that tell us almost the same thing),\noutliers or unusual values that might affect the model.\n\nThis is exactly the “EDA first” mindset in Section 3.3.1.\n\n5.3.1 Outcome distribution (by grade)\nNAPLAN scores are on different ranges across grades (Year 3 scores are typically lower than Year 9 scores), so it helps to look grade-by-grade.\n\n\n\n\n\n\n\n\n\n\n5.3.2 Key bivariate relationships\n\n5.3.2.1 Reading score vs SES index\n\n\n\n\n\n\n\n\n\n\n5.3.2.2 Reading score vs time spent reading at home\nBecause reading_time_home is reported in minutes per week and has a lot of small values (including zeros), we should be open to nonlinearity here (Chapter 03: Section 3.1.2).\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Categorical predictors\nBoxplots make it easy to compare distributions across categories (see Section 2.5 and Section 2.5.2).\n\n\n\n\n\n\n\n\n\n\n5.3.4 Correlations among continuous variables\nWe don’t have GGally::ggpairs() available in this project, but we can still compute a correlation matrix and look at a basic pairs plot (see Section 3.3.1.1).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3\nRecreate one of the key EDA plots: reading time at home vs NAPLAN reading score, with a quadratic smoother and separate panels for each grade.\nAfter it runs, try experimenting (e.g., set se = FALSE, remove facet_wrap(~ grade), or change the smoother to linear).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_readtime &lt;- ggplot(naplan, aes(x = reading_time_home, y = naplan_reading_score)) +\n  geom_point(alpha = 0.25, size = 1) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2, raw = TRUE), se = TRUE) +\n  facet_wrap(~ grade) +\n  labs(x = \"Reading time at home (minutes/week)\", y = \"NAPLAN reading score\")\n\np_readtime\np_readtime &lt;- ggplot(naplan, aes(x = reading_time_home, y = naplan_reading_score)) +\n  geom_point(alpha = 0.25, size = 1) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2, raw = TRUE), se = TRUE) +\n  facet_wrap(~ grade) +\n  labs(x = \"Reading time at home (minutes/week)\", y = \"NAPLAN reading score\")\n\np_readtime",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-fitting-the-least-squares-model",
    "href": "ch5-case-naplan.html#sec-fitting-the-least-squares-model",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "5.4 Fitting the least-squares model",
    "text": "5.4 Fitting the least-squares model\nWe now fit regression models using ordinary least squares (Chapter 01: Section 5). We start with a first-order additive model and then consider targeted increases in complexity when the EDA suggests they are useful (Chapter 03: Section 3.1).\n\n5.4.1 A first-order screening model\nThis model includes all candidate predictors, including birth_months. Think of it as a starting point for refinement (not the final model).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Model comparison: does birth month help?\nWe compare the model with and without birth_months using AIC/BIC (Chapter 03: Section 3.2.2.2) and an ANOVA comparison (Chapter 03: Section 3.2.2.4).\n\n\n\n\n\n\n\n\nThe comparison suggests birth_months doesn’t meaningfully improve the model here, so we drop it.\n\nExercise 4\nFit the first-order “screening” model yourself, and store it as my_full_mod.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmy_full_mod &lt;- lm(\n  naplan_reading_score ~ grade + ses_index + reading_time_home + n_siblings +\n    parent_education + school_type + gender + birth_months,\n  data = naplan\n)\n\nglance(my_full_mod)\nmy_full_mod &lt;- lm(\n  naplan_reading_score ~ grade + ses_index + reading_time_home + n_siblings +\n    parent_education + school_type + gender + birth_months,\n  data = naplan\n)\n\nglance(my_full_mod)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-model-refinement-and-interpretation",
    "href": "ch5-case-naplan.html#sec-model-refinement-and-interpretation",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "5.5 Model refinement and selection",
    "text": "5.5 Model refinement and selection\nNow we refine the model in a controlled way, guided by what we saw in the EDA and by the tools in Chapter 03.\n\nNoteAutomated methods like stepwise regression can be useful as screening tools, but can be misleading if used blindly (see Section 3.3.3 and Section 4.5.9). In this case study we prefer small, interpretable comparisons.\n\n\n\n5.5.1 (Optional) Stepwise AIC screening\nIf you want to see what a purely automated AIC search does, we can run a stepwise procedure on a first-order model (no interactions or polynomial terms). Treat this as a starting point for thought, not a final answer.\n\n\n\n\n\n\n\n\n\n\n5.5.2 Add a quadratic term for reading time\nThe scatterplot suggested possible curvature in the relationship between reading_time_home and naplan_reading_score. A natural next step is a quadratic term (Chapter 03: Section 3.1.2).\n\n\n\n\n\n\n\n\n\n\n5.5.3 Allow the reading-time curve to differ by grade (interaction)\nBecause grade is strongly related to score, it’s plausible the reading-time pattern differs across grades. This is a continuous × categorical interaction (Chapter 03: Section 3.1.4.1).\n\n\n\n\n\n\n\n\n\n\n5.5.4 Two competing “SES” choices: SES index vs parent education\nThe dataset contains two SES-related variables:\n\nses_index (a numeric socioeconomic index),\nparent_education (a categorical proxy related to SES).\n\nIncluding both can be redundant: they are different ways of measuring similar underlying circumstances. This is a good moment to practise the “partial effects” idea from Section 2.3, and to remember the warning about multicollinearity in Section 4.5.8.\nWe fit two versions:\n\nmod_ses_index: includes ses_index (but not parent_education),\nfinal_mod: includes parent_education (but not ses_index).\n\n\n\n\n\n\n\n\n\nIn this dataset, the parent-education version fits better by AIC/BIC, so we use it as our final model for reporting. But we still report what the SES index suggests later, as a sensitivity check.\nDoes the SES index add anything once parent education is already in the model?\n\n\n\n\n\n\n\n\n\nExercise 5\nUse an ANOVA comparison to test whether adding birth_months improves the first-order model. Store the p-value in p_birth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirth_cmp &lt;- anova(mod_no_birth, mod_full_first_order)\np_birth &lt;- birth_cmp$`Pr(&gt;F)`[2]\np_birth\nbirth_cmp &lt;- anova(mod_no_birth, mod_full_first_order)\np_birth &lt;- birth_cmp$`Pr(&gt;F)`[2]\np_birth",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-chapter05-diagnostics",
    "href": "ch5-case-naplan.html#sec-chapter05-diagnostics",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "5.6 Diagnostics (does the model look reasonable?)",
    "text": "5.6 Diagnostics (does the model look reasonable?)\nAs emphasized in Chapter 04, we always re-check residual diagnostics on the final model (see Section 4.4 and Section 4.5).\n\n5.6.1 A small set of diagnostic plots\n\n\n\n\n\n\n\n\nThe residual-vs-fitted plot is the main place to look for nonlinearity and changing variance (Chapter 04: Section 4.5.1 and Section 4.5.2). In this case the pattern looks broadly centred around 0, with at most mild changes in spread.\n\n\n5.6.2 Influence and unusual points\n\n\n\n\n\n\n\n\nThese points are worth checking, but the Cook’s distances are not extreme. This is the sort of “small number of influential points” situation discussed in Section 4.5.5.\n\n\n5.6.3 Multicollinearity check (VIF) on a simpler model\nBecause VIFs can be inflated by raw polynomials and interactions, we compute VIFs on the simpler first-order model (no quadratic or interaction terms) to get a clearer sense of redundancy among predictors (Chapter 04: Section 4.5.8).\n\n\n\n\n\n\n\n\n\nExercise 6\nRecreate the core diagnostic plot from Chapter 04: residuals vs fitted values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_resid &lt;- ggplot(final_aug, aes(x = .fitted, y = .resid)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", colour = \"gray40\") +\n  geom_point(alpha = 0.25, size = 1) +\n  labs(x = \"Fitted value\", y = \"Residual\")\n\np_resid\np_resid &lt;- ggplot(final_aug, aes(x = .fitted, y = .resid)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", colour = \"gray40\") +\n  geom_point(alpha = 0.25, size = 1) +\n  labs(x = \"Fitted value\", y = \"Residual\")\n\np_resid",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-reporting-the-results",
    "href": "ch5-case-naplan.html#sec-reporting-the-results",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "5.7 Reporting the results",
    "text": "5.7 Reporting the results\nWe now summarise the final model in a way that a reader can understand (Chapter 02: interpreting coefficients, Section 2.3; and Chapter 01: confidence/prediction intervals, Section 1.5.2).\n\n5.7.1 Model summary\nWe fitted a multiple linear regression model to 3000 students. The overall model fit was:\n\nAdjusted \\(R^2 \\approx\\) 0.638\nResidual standard error \\(\\approx\\) 56.48\nOverall \\(F\\)-test: $F(\\(18\\), \\(2981\\)) = 294.7, p &lt;1e-16\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Interpreting key coefficients\nFor categorical predictors, coefficients are interpreted relative to their reference levels (Chapter 02: Section 2.5). In this model the references are:\n\ngrade: Year 3\nparent_education: Year 10 or below\nschool_type: Government\ngender: Female\n\n\n\n\n\n\n\n\n\nFrom the table:\n\nHigher parent education is associated with higher reading scores (e.g., Bachelor vs Year 10 or below: about 54.8 points higher, holding other variables constant).\nIndependent schools score higher than Government schools by about 44.5 points on average (Catholic vs Government is much smaller).\nMales score about -7.9 points lower than females, on average.\n\n\n\n5.7.3 Interpreting the grade × reading-time pattern\nBecause the model includes an interaction between grade and a quadratic reading-time term, it’s often easiest to interpret with predicted curves rather than trying to “read” raw polynomial coefficients (Chapter 03: Section 3.1.4.1).\n\n\n\n\n\n\n\n\n\n\n5.7.4 A concrete prediction (with a prediction interval)\nConsider a Year 9 female student in a Government school, with parent education = Bachelor degree, who reports 30 minutes of reading at home per week. The fitted mean is:\n\n\n\n\n\n\n\n\n\n\n5.7.5 Sensitivity check: what if we use the SES index instead?\nIf we swap parent_education for the numeric SES index, the SES index is strongly associated with score.\n\n\n\n\n\n\n\n\nThis illustrates an important modelling lesson: how you operationalise a construct like “SES” can change both interpretation and model fit. In this dataset, parent education seems to capture SES-related variation particularly well.\n\nExercise 7\nCreate a prediction interval for a specific student profile.\nUse a Year 9 student with:\n\nreading_time_home = 30\nparent_education = \"Bachelor degree\"\nschool_type = \"Government\"\ngender = \"Female\"\n\nStore the result of predict(..., interval = \"prediction\") in pred_pi.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmy_profile &lt;- tibble(\n  grade = factor(\"Year 9\", levels = levels(naplan$grade)),\n  reading_time_home = 30,\n  parent_education = factor(\"Bachelor degree\", levels = levels(naplan$parent_education)),\n  school_type = factor(\"Government\", levels = levels(naplan$school_type)),\n  gender = factor(\"Female\", levels = levels(naplan$gender))\n)\n\npred_pi &lt;- predict(final_mod, newdata = my_profile, interval = \"prediction\")\npred_pi\nmy_profile &lt;- tibble(\n  grade = factor(\"Year 9\", levels = levels(naplan$grade)),\n  reading_time_home = 30,\n  parent_education = factor(\"Bachelor degree\", levels = levels(naplan$parent_education)),\n  school_type = factor(\"Government\", levels = levels(naplan$school_type)),\n  gender = factor(\"Female\", levels = levels(naplan$gender))\n)\n\npred_pi &lt;- predict(final_mod, newdata = my_profile, interval = \"prediction\")\npred_pi",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "ch5-case-naplan.html#sec-chapter05-conclusion",
    "href": "ch5-case-naplan.html#sec-chapter05-conclusion",
    "title": "5  Regression Case Study: NAPLAN Reading Scores",
    "section": "5.8 Conclusion",
    "text": "5.8 Conclusion\nUsing 3000 students across Years 3, 5, 7, and 9, we fitted a multiple linear regression model relating NAPLAN reading scores to reading time at home, grade, parent education, school type, and gender. The model explained about 63.8% of the variation in scores (adjusted \\(R^2 \\approx\\) 0.638), with a residual standard error of about 56.5 points.\nAfter adjusting for other variables, higher parent education was associated with higher reading scores (e.g., Bachelor vs Year 10 or below: roughly 54.8 points higher). Independent schools scored higher than Government schools by about 44.5 points on average, and males scored about 7.9 points lower than females.\nThe relationship between reading time at home and score was not purely linear and differed by grade (a grade × quadratic reading-time interaction). Predicted curves suggested that increases in reading time are associated with higher predicted scores, but the shape of that association varies across schooling levels.\nLimitations: These are associations from observational data. Potential clustering within schools and omitted variables (e.g., prior achievement, classroom factors) may affect estimates. Always interpret results in context and re-check diagnostics, as in Chapter 4.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Statistical model\nA probabilistic description of how a response relates to predictors, combining a systematic component with random variation.\n\nCommon form\n\\[Y = f(X) + \\varepsilon.\\]\nIn R\n\nFit models with lm() (linear regression) and glm() (generalised linear models).\nUse summary(), coef(), and predict() to inspect fitted models.\n\n\n\n\nRandom variable\nA quantity whose value varies across observations, described by a probability distribution.\n\nIn R\n\nSimulate random variables with functions like rnorm(), runif(), and rbinom().\n\n\n\n\nDistribution\nA description of how the values of a random variable are spread across possible outcomes.\n\nIn R\n\nMany distributions use the d/p/q/r pattern, e.g. dnorm(), pnorm(), qnorm(), rnorm().\n\n\n\n\nNormal distribution\nA symmetric, bell-shaped distribution often used to model random variation.\n\nFormula\n\\[X \\sim N(\\mu, \\sigma^2).\\]\nIn R\n\nDensity/CDF/quantiles/simulation: dnorm(), pnorm(), qnorm(), rnorm().\n\n\n\n\nVariance\nA measure of spread; larger variance means values tend to be further from their mean.\n\nFormula\n\\[\\mathrm{Var}(X) = E[(X - E[X])^2].\\]\nIn R\n\nSample variance: var(x).\n\n\n\n\nAssumption\nA condition we adopt to justify a model or an inference procedure.\n\nIn this course\n\nFor linear regression, key assumptions are about the error term (mean zero, constant variance, normality, independence).\n\n\n\n\nResponse variable\nThe variable you aim to explain or predict (sometimes called the outcome or dependent variable).\n\nIn R\n\nIn a formula like y ~ x1 + x2, the response is on the left-hand side.\n\n\n\n\nPredictor variable\nA variable used to explain, adjust for, or predict changes in the response (sometimes called a covariate).\n\nIn R\n\nIn a formula like y ~ x1 + x2, predictors are on the right-hand side.\n\n\n\n\nContinuous variable\nA numeric variable that can (in principle) take any value on an interval.\n\nIn R\n\nStored as numeric (dbl/int).\nHelpful checks: is.numeric(x), summary(x).\n\n\n\n\nLinear function\nA straight-line relationship between a variable and an outcome.\n\nFormula\n\\[f(x) = \\alpha + \\beta x.\\]\nIn R\n\nFit a straight-line mean function with lm(y ~ x).\n\n\n\n\nLinear predictor\nThe systematic (non-random) part of a regression model that combines predictors and coefficients.\n\nFormula\n\\[E[Y\\mid X] = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k.\\]\nIn R\n\nFitted values: fitted(model) or predict(model).\n\n\n\n\nIntercept\nThe baseline level of the response when predictors are at their reference values (often zero).\n\nIn R\n\nIncluded by default in lm(); remove with y ~ x - 1.\n\n\n\n\nSlope\nThe expected change in the response for a one-unit increase in a predictor, holding other predictors constant.\n\nIn R\n\nExtract coefficients with coef(model) or summary(model)$coefficients.\n\n\n\n\nRandom error term\nThe part of the response not explained by the predictors (random variation around the systematic part).\n\nIn a model\n\nOften written as \\(\\varepsilon\\) in \\(Y = f(X) + \\varepsilon\\).\n\n\n\n\nMean-zero errors\nThe assumption that errors average to zero, so the model is unbiased on average.\n\nFormula\n\\[E[\\varepsilon \\mid X] = 0.\\]\n\n\n\nHomoscedasticity\nThe assumption that the variability of the errors is roughly constant across the predictor range.\n\nFormula\n\\[\\mathrm{Var}(\\varepsilon \\mid X) = \\sigma^2.\\]\nIn R\n\nCheck residuals vs fitted: plot(model, which = 1) (or with broom::augment()).\nScale–location plot: plot(model, which = 3).\n\nRemedies (when violated)\n\nTransform the response (e.g., log / square-root), or model the mean-variance relationship.\nUse heteroskedasticity-robust standard errors for inference (where appropriate).\n\n\n\n\nHeteroskedasticity\nNonconstant error variance across predictor values (the opposite of homoscedasticity).\n\nIdea\n\n\\(\\mathrm{Var}(\\varepsilon \\mid X)\\) changes with \\(X\\) (often increasing with the mean).\n\nIn R\n\nLook for a funnel pattern in residuals vs fitted: plot(model, which = 1).\nScale–location plot: plot(model, which = 3).\nOptional formal test (if available): lmtest::bptest(model) (Breusch–Pagan).\n\nRemedies\n\nTransform the response (log / square-root), especially for positive outcomes.\nConsider weighted least squares if you can model the changing variance.\nFor inference, consider robust standard errors (e.g., sandwich + lmtest), but still inspect the fit.\n\n\n\n\nNormal errors\nThe assumption that errors are approximately normally distributed (mainly important for small-sample inference).\n\nFormula\n\\[\\varepsilon \\sim N(0, \\sigma^2).\\]\nIn R\n\nCheck with Q–Q plots: plot(model, which = 2) or qqnorm(resid(model)); qqline(resid(model)).\n\n\n\n\nIndependence\nThe assumption that errors from different observations are not correlated.\n\nIn R\n\nFor time-ordered data, inspect autocorrelation with acf(resid(model)).\nOptional test (if available): lmtest::dwtest(model) (Durbin–Watson).\n\n\n\n\nEstimation\nThe process of using data to infer unknown model parameters (like regression coefficients and error variability).\n\nIn R\n\nFit a linear regression with lm().\nExtract estimates with coef(model).\n\n\n\n\nEstimate\nA value computed from data that approximates an unknown population quantity (a parameter).\n\nNotation\n\nParameters are often written with Greek letters (e.g., \\(\\beta\\)), and estimates with hats (e.g., \\(\\hat{\\beta}\\)).\n\nIn R\n\nCoefficient estimates: coef(model).\n\n\n\n\nOrdinary least squares\nEstimation method that chooses coefficients to minimise the sum of squared residuals.\n\nCriterion\n\\[\\text{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2.\\]\nIn R\n\nStandard linear regression uses OLS via lm().\n\n\n\n\nResidual sum of squares\nThe total squared discrepancy between observed values and fitted values.\n\nFormula\n\\[\\text{RSS} = \\sum_{i=1}^n e_i^2.\\]\nIn R\n\ndeviance(model) (for lm) equals RSS.\nOr compute directly: sum(resid(model)^2).\n\nNotes\n\nNotation varies: many texts use SSE for this quantity; SSR is sometimes reserved for the regression sum of squares.\n\n\n\n\nTotal sum of squares\nThe total variability in the response around its mean (a baseline benchmark for model fit).\n\nFormula\n\\[\\text{TSS} = \\sum_{i=1}^n (y_i - \\bar{y})^2.\\]\nIn R\n\nsum((y - mean(y))^2).\n\n\n\n\nMean squared error\nAn estimate of error variance based on residual size (often RSS divided by residual degrees of freedom).\n\nFormula\n\\[\\text{MSE} = \\frac{\\text{RSS}}{n - p},\\]\nwhere \\(p\\) is the number of estimated parameters (including the intercept).\nIn R\n\nResidual variance: sigma(model)^2.\n\n\n\n\nResidual standard error\nAn estimate of the typical size of residuals (the estimated error standard deviation).\n\nFormula\n\\[\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n - p}} = \\sqrt{\\text{MSE}},\\]\nwhere \\(p\\) is the number of fitted parameters (including the intercept).\nIn R\n\nFor lm, use sigma(model).\n\n\n\n\nDegrees of freedom\nThe amount of independent information remaining after estimating model parameters.\n\nExamples\n\nIn simple linear regression (intercept + slope): residual df is \\(n - 2\\).\nMore generally (linear regression): residual df is \\(n - p\\).\n\nIn R\n\nResidual df: df.residual(model).\n\n\n\n\nStandard error\nAn estimate of how much a statistic (like a coefficient) would vary across repeated samples.\n\nFormula (linear regression)\n\\[\\mathrm{SE}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2\\,(X^\\top X)^{-1}_{jj}}.\\]\nIn R\n\nCoefficient SEs are in summary(model)$coefficients[, \"Std. Error\"].\nThe covariance matrix is vcov(model).\n\n\n\n\nt-test\nTests whether a coefficient differs from zero using a t statistic and an assumed reference distribution.\n\nTest statistic\n\\[t = \\frac{\\hat{\\beta}_j - 0}{\\mathrm{SE}(\\hat{\\beta}_j)}.\\]\nIn R\n\nSee t value and Pr(&gt;|t|) in summary(model).\n\n\n\n\nF-test\nTests whether predictors jointly improve model fit compared with a simpler model (often intercept-only).\n\nCommon form (overall regression test)\n\\[F = \\frac{(\\text{TSS} - \\text{RSS})/(p - 1)}{\\text{RSS}/(n - p)},\\]\nwhere \\(p\\) is the number of fitted parameters (including the intercept).\nIn R\n\nReported in summary(model) for lm.\nCompare nested models with anova(model_small, model_big).\n\n\n\n\np-value\nThe probability, under the null hypothesis, of observing a result at least as extreme as the one observed.\n\nIn R\n\nReported in summary(model) for standard lm coefficient tests.\n\n\n\n\nConfidence interval\nA range of plausible parameter values at a chosen confidence level (e.g., 95%).\n\nCommon form\n\\[\\hat{\\theta} \\pm t^* \\cdot \\mathrm{SE}(\\hat{\\theta}).\\]\nIn R\n\nUse confint(model) for coefficient confidence intervals.\n\n\n\n\nPrediction interval\nAn interval for a future observation at given predictors; wider than a confidence interval.\n\nIn R\n\npredict(model, newdata = df, interval = \"prediction\").\n\n\n\n\nFitted value\nThe model’s predicted mean response for an observation (given its predictor values).\n\nNotation\n\nOften written as \\(\\hat{y}_i\\).\n\nIn R\n\nfitted(model) or predict(model) for fitted values.\nFor new cases: predict(model, newdata = ...).\n\n\n\n\nResidual\nThe difference between an observed value and the value predicted by the fitted model.\n\nFormula\n\\[e_i = y_i - \\hat{y}_i.\\]\nIn R\n\nresid(model); with broom, augment(model) includes .resid and .fitted.\n\n\n\n\nSimple linear model\nA model that relates a response to a single predictor using a straight-line mean function plus random error.\n\nFormula\n\\[Y = \\alpha + \\beta x + \\varepsilon.\\]\nIn R\n\nFit with lm(y ~ x, data = df).\n\n\n\n\nSimple linear regression\nLinear regression with exactly one predictor (a straight-line relationship on average).\n\nFormula\n\\[E[Y\\mid X] = \\beta_0 + \\beta_1 X.\\]\nIn R\n\nFit with lm(y ~ x).\n\n\n\n\nMultiple linear regression\nLinear regression with two or more predictors, allowing adjustment for multiple variables at once.\n\nFormula\n\\[E[Y\\mid X] = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_k X_k.\\]\nIn R\n\nFit with lm(y ~ x1 + x2 + x3).\n\n\n\n\nPartial regression coefficient\nThe effect of a predictor on the response after holding the other predictors constant.\n\nIdea\n\nInterprets the change in \\(E[Y\\mid X]\\) for a one-unit increase in \\(X_j\\), with the other predictors held fixed.\n\nIn R\n\nIn multiple regression, each coefficient in summary(model)$coefficients is a partial effect.\n\n\n\n\nPearson correlation\nMeasures linear association between two variables, ranging from -1 to 1.\n\nFormula\n\\[r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2\\;\\sum_{i=1}^n (y_i - \\bar{y})^2}}.\\]\nIn R\n\ncor(x, y) for Pearson’s correlation (default).\ncor.test(x, y) for a test and confidence interval.\n\n\n\n\nR-squared\nThe proportion of response variability explained by the fitted model (on the training data).\n\nFormula\n\\[R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}}.\\]\nIn R\n\nsummary(model)$r.squared.\n\n\n\n\nAdjusted R-squared\nR-squared penalised for the number of predictors to discourage unnecessary terms.\n\nFormula\n\\[\\bar{R}^2 = 1 - (1 - R^2)\\frac{n - 1}{n - p},\\]\nwhere \\(p\\) is the number of fitted parameters (including the intercept).\nIn R\n\nsummary(model)$adj.r.squared.\n\n\n\n\nPolynomial term\nA transformed predictor (like x²) used to capture curvature while keeping the model linear in coefficients.\n\nIn R\n\nUse I(x^2) for a squared term, or poly(x, degree) for orthogonal polynomials.\n\n\n\n\nInteraction\nA model term that allows the effect of one predictor to depend on another predictor.\n\nIn R\n\nx1:x2 for an interaction only; x1 * x2 for main effects plus interaction.\n\n\n\n\nOverfitting\nWhen a model captures noise in the training data and generalises poorly to new data.\n\nIn practice\n\nSymptoms include overly optimistic fit on training data and poor performance on new data.\nPrefer validation (train/test split or cross-validation) over “fit on everything and hope”.\n\n\n\n\nStepwise regression\nAn automated selection approach that adds/removes predictors based on a criterion (often AIC).\n\nPitfalls\n\nCan be unstable: small data changes may lead to different selected models.\nPost-selection \\(p\\)-values and confidence intervals can be misleading.\n\nIn R\n\nUse step(model) (AIC-based by default).\nAlways re-check diagnostics after selection, and validate performance on new/held-out data.\n\n\n\n\nAkaike Information Criterion\nModel comparison metric that balances fit and complexity; lower values indicate a preferred model among those compared.\n\nFormula\n\\[\\mathrm{AIC} = -2\\log(L) + 2k,\\]\nwhere \\(k\\) is the number of fitted parameters and \\(L\\) is the maximised likelihood.\nIn R\n\nAIC(model); compare multiple models with AIC(m1, m2, m3).\n\n\n\n\nBayesian Information Criterion\nModel comparison metric that penalises complexity more than AIC; lower values indicate a preferred model among those compared.\n\nFormula\n\\[\\mathrm{BIC} = -2\\log(L) + k\\log(n).\\]\nIn R\n\nBIC(model); compare multiple models with BIC(m1, m2, m3).\n\n\n\n\nParsimony\nChoosing the simplest adequate model that answers the scientific question.\n\nIn practice\n\nPrefer simpler models unless extra complexity meaningfully improves interpretation or prediction.\n\n\n\n\nOutlier\nAn observation with an unusually large residual relative to the fitted model.\n\nDiagnostics\n\nLarge standardised/studentised residuals suggest an outlying response value given the predictors.\nRules of thumb (context dependent): \\(|r_i| &gt; 2\\) (flag), \\(|r_i| &gt; 3\\) (strong flag).\n\nIn R\n\nrstandard(model) (standardised residuals) and rstudent(model) (studentised residuals).\nAn outlier is not always influential: check leverage and Cook’s distance as well.\n\n\n\n\nStandardised residual\nA residual scaled by its estimated standard deviation to highlight unusually large deviations.\n\nFormula\n\\[r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}}.\\]\nIn R\n\nrstandard(model) and rstudent(model) (studentised residuals).\n\n\n\n\nInfluential point\nAn observation that substantially changes fitted coefficients or predictions when removed.\n\nIn R\n\nUse cooks.distance(model) and influence.measures(model).\nFor coefficient impact: dfbetas(model) and dffits(model).\n\n\n\n\nLeverage\nA measure of how unusual a case’s predictor values are; high leverage can increase influence.\n\nFormula\n\\[h_{ii} = x_i^\\top (X^\\top X)^{-1} x_i.\\]\nIn R\n\nhatvalues(model) returns leverages (\\(h_{ii}\\)).\n\nRule of thumb\n\nHigh leverage often flagged by \\(h_{ii} &gt; 2p/n\\) (or \\(3p/n\\)), where \\(p\\) is the number of parameters.\n\n\n\n\nCook’s distance\nAn influence measure combining residual size and leverage to flag points that strongly affect the fit.\n\nFormula\n\\[D_i = \\frac{e_i^2}{p\\,\\hat{\\sigma}^2}\\frac{h_{ii}}{(1 - h_{ii})^2}.\\]\nIn R\n\ncooks.distance(model).\n\nRule of thumb\n\nValues above about \\(4/n\\) are often flagged for inspection (not an automatic deletion rule).\n\n\n\n\nExtrapolation\nMaking predictions outside the observed predictor range, where the fitted relationship may not hold.\n\nIn R\n\nCompare new predictor values to range(df$x) (and analogous for other predictors).\n\nPractical note\n\nUncertainty grows quickly as you move away from the data cloud (often reflected in higher leverage).\n\n\n\n\nMulticollinearity\nStrong correlation among predictors that inflates standard errors and destabilises coefficient estimates.\n\nDiagnostics\n\nLarge standard errors and unstable coefficient signs/magnitudes.\nHigh correlations among predictors; large VIFs.\nExact multicollinearity shows up as non-estimable coefficients.\n\nIn R\n\nCorrelations: cor(df[, numeric_cols]).\nExact aliasing (perfect collinearity): alias(model).\nCondition number (rough diagnostic): kappa(model.matrix(model)).\n\nRemedies\n\nRemove or combine redundant predictors (guided by your scientific question).\nCentering can help when you include interactions/polynomials (reduces induced collinearity), but does not “fix” collinearity in general.\nIf prediction is the goal, consider regularisation (ridge/lasso) or dimension reduction (e.g., principal components).\n\n\n\n\nVariance inflation factor\nA diagnostic that summarises how collinearity inflates the uncertainty of a coefficient estimate.\n\nFormula\n\\[\\text{VIF}_j = \\frac{1}{1 - R_j^2},\\]\nwhere \\(R_j^2\\) comes from regressing predictor \\(X_j\\) on the other predictors.\nIn R\n\nThe car::vif() function is commonly used (if the car package is available).\nBase R approach: regress each predictor on the others and plug its \\(R_j^2\\) into the VIF formula.\n\n\n\n\nBox-Cox transformation\nA family of power transformations used to stabilise variance or improve linearity for positive responses.\n\nFormula\nFor \\(\\lambda \\neq 0\\):\n\\[g_\\lambda(y) = \\frac{y^\\lambda - 1}{\\lambda}.\\]\nFor \\(\\lambda = 0\\):\n\\[g_0(y) = \\log(y).\\]\nIn R\n\nMASS::boxcox(model) explores power transformations for positive responses.\n\n\n\n\nFactor\nA categorical predictor with discrete levels; R stores these as factors.\n\nIn R\n\nCreate with factor(x); check levels with levels(x).\n\n\n\n\nDummy variable\nAn indicator (0/1) used to code categories in regression relative to a baseline.\n\nIn R\n\nFactors are expanded into dummy variables automatically in lm().\nInspect the design matrix with model.matrix(model).\n\n\n\n\nReference level\nThe baseline category used to interpret coefficients for a factor.\n\nIn R\n\nChange with relevel(f, ref = \"baseline\").\n\n\n\n\nContrast\nA coding scheme that maps factor levels to numeric columns (e.g., treatment coding).\n\nIn R\n\nView or set with contrasts(f) and functions like contr.treatment().",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "index.html#sec-course-outline",
    "href": "index.html#sec-course-outline",
    "title": "Linear Regression and Model Selection with R",
    "section": "Course outline",
    "text": "Course outline\n\nChapter 0 — Introduction to linear models: what a regression model is, why we need an error term, and what assumptions mean in practice.\nChapter 1 — Simple linear regression (SLR): least squares, fitted values and residuals, inference, and prediction.\nChapter 2 — Multiple linear regression (MLR): partial regression coefficients, categorical predictors, and overall model tests.\nChapter 3 — Model building: interactions, polynomial terms, parsimony, and model comparison/selection.\nChapter 4 — Regression pitfalls & diagnostics: how to recognise assumption violations and influence, and what to do about them.\nChapter 5 — Case study (NAPLAN Reading scores): a guided workflow through a real dataset from research question → model → diagnostics → interpretation.\nGlossary: short definitions plus formulas and useful R functions.",
    "crumbs": [
      "Course Overview"
    ]
  }
]