[
  {
    "objectID": "module03-model-building.html",
    "href": "module03-model-building.html",
    "title": "5  Model Building and Selection",
    "section": "",
    "text": "5.1 Increasing model complexity to model complex relationships\nIn this module, we explore strategies for building effective regression models. We cover: - Increasing model complexity to capture non-linear relationships and interactions, - Assessing model fit while balancing complexity, - Techniques for selecting parsimonious models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#increasing-model-complexity-to-model-complex-relationships",
    "href": "module03-model-building.html#increasing-model-complexity-to-model-complex-relationships",
    "title": "5  Model Building and Selection",
    "section": "",
    "text": "Why increase model complexity?\nWe began this course with the simple linear regression model: a single continuous predictor with a straight-line effect,\n\nKey-point: The ‘first-order’ single predictor modelIs another name for the simple linear model from ?sec-slm \\[\nE[Y] = \\beta_0 + \\beta_1 X.\n\\]\n\n\nThis is termed a ‘first-order model’ because \\(x\\) only appears once, ‘as is’ (you’ll understand why this terminology is used shortly) but the relevant point is that this is the simplest linear model we can make with \\(X\\).\nThis simple model is often a reasonable starting point, and can go a long way to modeling real-world phenomena (especially in the multiple regression case), but real data may also show structure a straight line cannot capture, such as curvature.\n\nExample 1: Non-linear relationships in data\nFor example, fitting a straight line to the following data is not ideal:\n\n\nquadPoints = transpose(quadData)\nquadXDomain = d3.extent(quadData.x)\n\nxRange = d3.extent(quadData.x)\nyRange = d3.extent(quadData.y)\n\nviewof b0adj_1 = Inputs.range([-1,16], {step: 1, label: html`${tex`\\beta_0`}: Intercept `})\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\\beta_1`}: Slope`})\n\nb0_1 = b0adj_1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\hat{Y} = ${b0_1.toFixed(0)} + ${b1_1.toFixed(2)}x`\n\n\n\n\n\n\n\nlineData = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\nlineResiduals = quadPoints.map((d) =&gt; {\n  const fit = b0_1 + b1_1 * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\nlineMaxAbsResid = d3.max(lineResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\nPlot.plot({\n  marks: [\n    Plot.link(lineResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / lineMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(quadPoints, { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: yRange, label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\nSquare, Cubic, and Higher-Order Univariate Models\nWe enrich the basic linear model by adding powers of a predictor, allowing the fitted relationship to bend. In the above case, the “U” shape suggests a quadratic (squared) term may be appropriate\n\nSecond-order (Quadratic) univariate model\n\\[E[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2\\] ::: Example #### Seeing how \\(\\beta_2\\) bends the curve Adjust the slider for the squared term to see how changing \\(\\beta_2\\) adds curvature to the fitted relationship (with optional tweaks to \\(\\beta_0\\) and \\(\\beta_1\\)). Try to find a good fit to the data (can you guess what model generated this data?).\n\n\nviewof b0_quad = Inputs.range([-4, 4], {step: 0.25, value: 0, label: html`${tex`\\beta_0`}: Intercept`})\nviewof b1_quad = Inputs.range([-3, 3], {step: 0.1, value: 0, label: html`${tex`\\beta_1`}: Linear term`})\nviewof b2_quad = Inputs.range([-1, 3], {step: 0.05, value: 0, label: html`${tex`\\beta_2`}: Squared term`})\n\ntex.block`\\hat{Y} = ${b0_quad.toFixed(2)} + ${b1_quad.toFixed(2)}x + ${b2_quad.toFixed(2)}x^2`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquadCurve = d3.range(quadXDomain[0], quadXDomain[1] + 0.05, 0.05).map((x) =&gt; ({\n  x,\n  y: b0_quad + b1_quad * x + b2_quad * x * x\n}))\n\nquadResiduals = quadPoints.map((d) =&gt; {\n  const fit = b0_quad + b1_quad * d.x + b2_quad * d.x * d.x;\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\nquadMaxAbsResid = d3.max(quadResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\n\nPlot.plot({\n  height: 320,\n  marginLeft: 50,\n  x: {label: \"Predictor (x)\", domain: xRange},\n  y: {label: \"Outcome (y)\", domain: yRange},\n  marks: [\n    Plot.link(quadResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / quadMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(quadPoints, {\n      x: \"x\",\n      y: \"y\",\n      r: 3,\n      title: (d) =&gt; `x = ${d.x.toFixed(2)}\\ny = ${d.y.toFixed(2)}`,\n    }),\n    Plot.line(quadCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2\n    })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n\nThird-order (cubic) univariate model\n\\[\nE[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3\n\\]\nallows more complex curvature with two\n\n\nN-th order univariate model\nWe can extend this process of deriving new terms by adding further powers of \\(X\\) - allowing ius to to fit arbitrarily complex curves: \\[\nE[Y] = \\beta_0 + \\beta_1X +  \\dots + \\beta_nX^n.\n\\]\nNote that each term gets its own parameter (\\(\\beta_i\\)). If we have \\(n\\) data points, a (n-1)^th-order model will have a parameter for each point, meaning it will fit the data perfectly!\n\nExample 2: Fitting higher-order polynomial models\n\nreg= require(\"d3-regression\")\npolyData=transpose(nonlinearData)\nviewof degree = Inputs.range([1, 9], {step: 1, label: \"Model Order\", value: 1})\n\npolyRegression = reg.regressionPoly()\n  .x((d) =&gt; d.x)\n  .y((d) =&gt; d.y)\n  .order(degree)\n  .domain(d3.extent(polyData, (d) =&gt; d.x));\n\npolyCurveRaw = polyRegression(polyData)\npolyCurve = polyCurveRaw.map(([x, y]) =&gt; ({ x, y }))\npolyPredict = (x) =&gt; {\n  if (typeof polyRegression.predict === \"function\") return polyRegression.predict(x);\n  if (polyCurveRaw.coefficients) {\n    return polyCurveRaw.coefficients.reduce((acc, coeff, i) =&gt; acc + coeff * x ** i, 0);\n  }\n  return NaN;\n}\npolyResiduals = polyData.map((d) =&gt; {\n  const fit = polyPredict(d.x);\n  return { x: d.x, y: d.y, fit, resid: d.y - fit };\n})\npolyMaxAbsResid = d3.max(polyResiduals, (d) =&gt; Math.abs(d.resid)) || 1;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolyTerms = Array.from({length: degree + 1}, (_, k) =&gt; {\n  if (k === 0) return `\\\\beta_{0}`;\n  if (k === 1) return `\\\\beta_{1} x`;\n  return `\\\\beta_{${k}} x^{${k}}`;\n}).join(\" + \");\n\nhtml`&lt;div style=\"text-align:center; margin: 0.5rem 0;\"&gt;${tex`E[Y] = ${polyTerms}`}&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolyYDomain = d3.extent([\n  ...polyData.map((d) =&gt; d.y),\n  ...polyCurve.map((d) =&gt; d.y),\n  ...polyResiduals.map((d) =&gt; d.fit)\n])\n\nPlot.plot({\n  marginLeft: 50,\n  height: 320,\n  x: {label: \"Predictor (x)\"},\n  y: {label: \"Outcome (y)\", domain: polyYDomain},\n  marks: [\n    Plot.link(polyResiduals, {\n      x1: \"x\",\n      y1: \"y\",\n      x2: \"x\",\n      y2: \"fit\",\n      stroke: (d) =&gt; d.resid &gt;= 0 ? \"positive\" : \"negative\",\n      strokeWidth: (d) =&gt; 1.5 + 3 * Math.abs(d.resid) / polyMaxAbsResid,\n      strokeOpacity: 0.7,\n      title: (d) =&gt; `Residual: ${d.resid.toFixed(2)}`\n    }),\n    Plot.dot(polyData, {\n      x: \"x\",\n      y: \"y\",\n      r: 4,\n    }), \n    Plot.line(polyCurve, {\n      x: \"x\",\n      y: \"y\",\n      strokeWidth: 2,\n    }),\n  ],\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher-order terms increase flexibility, but also increase the risk of fitting random noise rather than meaningful structure. We will return to this important issue later in the module.\n\nNote: The meaning of “linear” in linear modelsEven though higher-order models can model nonlinear relationships between the outcome and predictors, they are still linear models because each parameter enters additively. “Linearity” here refers to the parameters, not the shape of the fitted curve.\n\n\n\n\n\nFitting higher-order univariate models in R\nIn R, higher-order polynomial terms can be included using the I() function to indicate ‘as is’ operations. For example, to fit a quadratic model:\n\nlm(y ~ x + I(x^2), data = nonlinearData)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = nonlinearData)\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n     2.6185      -3.3760       0.8218  \n\n\nor using the poly() function:\n\nlm(y ~ poly(x, 2), data = nonlinearData)\n\n\nCall:\nlm(formula = y ~ poly(x, 2), data = nonlinearData)\n\nCoefficients:\n(Intercept)  poly(x, 2)1  poly(x, 2)2  \n     0.3493       0.3941       3.0208  \n\n\n\nContinue\n\n\n\nInteraction Models with Continuous Predictors\nIn ?sec-slr we saw a multiple regression model with two continuous predictors:\n\\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2.\n\\]\nThis again is a first-order model - each predictor appears only once, ‘as is’. Of course, we can add higher-order terms for each predictor separately (e.g., \\(X_1^2\\), \\(X_2^3\\)) to capture curvature in their individual effects as we did above.\nNow however, we can also consider a different form of higher order term: what happens when we multiply predictors together? This gives us an interaction term - a term that combines two (or more) predictors.\nFor two continuous predictors, the second-order interaction model is:\n\nKey-point: Second-order interaction model\\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2.\n\\]\n\n\nIf \\(\\beta_3 = 0\\) (i.e. as in the first-order model), predictors act independently of one another. If \\(\\beta_3 \\neq 0\\), however, the effect on \\(E[Y]\\) of changes in \\(X_1\\) varies with \\(X_2\\), and vice versa.\n\nExample 3: Fitting an interaction model to the advertising dataset.\nIf we recall our plot of the advertising dataset from ?sec-mlr, our fist order linear model did not fit the data in particular ways:\n\nads &lt;- read.csv(\"Data/Advertising.csv\")\nfit &lt;- lm(Sales ~ TV + Radio, data = ads)\n\n\n\n\n\n\n\nThe plane does not fit the data at the edges - it overestimates sales when either Radio or TV advertising is low (separately), but underestimates sales when both are high. This suggests that the effect of increasing one type of advertising depends on the level of the other type - an interaction effect.\nWe can fit a second-order interaction model to capture this: \\[\nE[Sales] = \\beta_0 + \\beta_1TV + \\beta_2Radio + \\beta_3(TV \\times Radio).\n\\]\n\nfit_int &lt;- lm(Sales ~ TV * Radio, data = ads)\n\n\n\n\n\n\n\nThe interaction effect can also be visualised by fixing one predictor and plotting the relationship between the other predictor and the outcome. In this case, we can plot the lines representing the relationship between TV advertising and Sales at different when Radio advertising is low (e.g. 0 units), average(i.e. ), and high:\n\n\n\nContinue\n\nWhen our first order model has more than two predictors, we can include interaction terms between any pair (or more) of predictors. For example, with three predictors \\(X_1\\), \\(X_2\\), and \\(X_3\\), a second-order interaction model would include interactions between each pair: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_1X_2 + \\beta_5X_1X_3 + \\beta_6X_2X_3.\n\\]\n\nHigher-order interaction models\nWe can also consider higher-order interaction models that include products of three or more predictors. For example, a third-order interaction model with three predictors would include the three-way interaction term: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_1X_2 + \\beta_5X_1X_3 + \\beta_6X_2X_3 + \\beta_7X_1X_2X_3.\n\\] This term captures how the interaction between two predictors changes depending on the level of the third predictor.\nAs in the case of higher-order univariate models, we can extend this idea to include both higher-order interaction terms and higher-order univariate terms for each predictor, allowing for very flexible modeling of complex relationships.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html",
    "href": "module04-regression-pitfalls-diagnostics.html",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "",
    "text": "6.1 Common pitfalls to avoid",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#common-pitfalls-to-avoid",
    "href": "module04-regression-pitfalls-diagnostics.html#common-pitfalls-to-avoid",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "",
    "text": "Ignoring model misspecification: linear fits to nonlinear patterns inflate residual structure.\nExtrapolating far beyond observed predictor ranges can explode predictions.\nBlind stepwise selection can drop theory-driven terms; keep scientific context in view.\nUnchecked multicollinearity or leverage points can destabilise inference; diagnose before concluding.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#residual-plots-vs-predictors-and-fitted-values",
    "href": "module04-regression-pitfalls-diagnostics.html#residual-plots-vs-predictors-and-fitted-values",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "6.2 Residual plots vs predictors and fitted values",
    "text": "6.2 Residual plots vs predictors and fitted values\n\nPlot residuals against fitted values to check linearity and constant variance; add predictor-specific plots to spot functional-form issues (residual).\n\n\ndiag_mod &lt;- lm(mpg ~ wt + hp + am, data = mtcars)\npar(mfrow = c(1, 2))\nplot(diag_mod, which = 1)      # residuals vs fitted\nplot(mtcars$wt, resid(diag_mod), xlab = \"wt\", ylab = \"Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#diagnosing-nonconstant-variance-and-nonlinearity",
    "href": "module04-regression-pitfalls-diagnostics.html#diagnosing-nonconstant-variance-and-nonlinearity",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "6.3 Diagnosing nonconstant variance and nonlinearity",
    "text": "6.3 Diagnosing nonconstant variance and nonlinearity\n\nFunnel shapes or curved trends in residual plots suggest heteroskedasticity or missing curvature; consider transformations or adding interactions/polynomials.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#assessing-normality-of-residuals",
    "href": "module04-regression-pitfalls-diagnostics.html#assessing-normality-of-residuals",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "6.4 Assessing normality of residuals",
    "text": "6.4 Assessing normality of residuals\n\nUse Q-Q plots and compare \\(t\\)- and \\(p\\)-values to Normal reference.\n\n\nqqnorm(resid(diag_mod)); qqline(resid(diag_mod))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#standardised-residuals-leverage-cooks-distance",
    "href": "module04-regression-pitfalls-diagnostics.html#standardised-residuals-leverage-cooks-distance",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "6.5 Standardised residuals, leverage, Cook’s distance",
    "text": "6.5 Standardised residuals, leverage, Cook’s distance\n\nStandardised (or studentised) residuals scale by estimated variance and flag unusual outcomes (|r| &gt; 2 as a heuristic).\nLeverage (hatvalues()) flags unusual predictor combinations (leverage).\nCook’s distance combines leverage and residual size to assess influence (Cook’s distance).\n\n\ncbind(rstudent(diag_mod),\n      hatvalues(diag_mod),\n      cooks.distance(diag_mod))[1:5, ]\n\n                        [,1]       [,2]        [,3]\nMazda RX4         -1.4433354 0.09320650 0.051538041\nMazda RX4 Wag     -1.1371729 0.12316145 0.044939134\nDatsun 710        -1.3051372 0.08856401 0.040365321\nHornet 4 Drive     0.3121886 0.07518198 0.002046732\nHornet Sportabout  0.4688483 0.07867173 0.004827051",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#transformations-including-boxcox",
    "href": "module04-regression-pitfalls-diagnostics.html#transformations-including-boxcox",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "6.6 Transformations including Box–Cox",
    "text": "6.6 Transformations including Box–Cox\n\nTransformations can stabilise variance or linearise relationships; for strictly positive \\(Y\\), consider Box–Cox transformations to guide power choices.\n\n\nMASS::boxcox(diag_mod, plotit = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module04-regression-pitfalls-diagnostics.html#handling-outliers-and-influential-observations",
    "href": "module04-regression-pitfalls-diagnostics.html#handling-outliers-and-influential-observations",
    "title": "6  Regression Pitfalls and Diagnostics",
    "section": "6.7 Handling outliers and influential observations",
    "text": "6.7 Handling outliers and influential observations\n\nInvestigate data quality first (entry errors, unusual units).\nIf influential points are real, fit with and without them to assess robustness; report how conclusions change.\nPrefer model adjustments (functional form, variance stabilisation) over automatic deletion; document any exclusions explicitly.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Pitfalls and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html",
    "href": "module05-case-naplan.html",
    "title": "7  Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "7.1 Defining the research question",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#defining-the-research-question",
    "href": "module05-case-naplan.html#defining-the-research-question",
    "title": "7  Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "Investigate how student and school-level predictors relate to NAPLAN Reading scores. Clarify outcome, units, and any grouping structure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#importing-data",
    "href": "module05-case-naplan.html#importing-data",
    "title": "7  Case Study: NAPLAN Reading Scores",
    "section": "7.2 Importing data",
    "text": "7.2 Importing data\n\n# Replace path with the appropriate CSV or RDS from the Resources/naplan reading folder\n# naplan &lt;- read.csv(\"Resources/naplan reading/naplan_reading.csv\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#exploratory-plot",
    "href": "module05-case-naplan.html#exploratory-plot",
    "title": "7  Case Study: NAPLAN Reading Scores",
    "section": "7.3 Exploratory plot",
    "text": "7.3 Exploratory plot\n\nStart with scatterplots and boxplots for key predictors (e.g., study time, SES, gender) against Reading scores.\n\n\n# Example placeholder using mtcars structure; swap to naplan data\nlibrary(ggplot2)\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Reading score\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#fitting-the-least-squares-model",
    "href": "module05-case-naplan.html#fitting-the-least-squares-model",
    "title": "7  Case Study: NAPLAN Reading Scores",
    "section": "7.4 Fitting the least-squares model",
    "text": "7.4 Fitting the least-squares model\n\nBegin with a first-order additive model; consider interactions or polynomials if exploratory plots suggest curvature.\n\n\n# model &lt;- lm(Reading ~ predictor1 + predictor2, data = naplan)\n# summary(model)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#model-refinement-and-interpretation",
    "href": "module05-case-naplan.html#model-refinement-and-interpretation",
    "title": "7  Case Study: NAPLAN Reading Scores",
    "section": "7.5 Model refinement and interpretation",
    "text": "7.5 Model refinement and interpretation\n\nCheck residual diagnostics (see Module 6). Address nonlinearity, heteroskedasticity, or influential points.\nInterpret coefficients, including categorical contrasts and any interaction terms.\n\n\n# plot(model, which = 1:2)\n# coef(summary(model))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module05-case-naplan.html#reporting-the-results",
    "href": "module05-case-naplan.html#reporting-the-results",
    "title": "7  Case Study: NAPLAN Reading Scores",
    "section": "7.6 Reporting the results",
    "text": "7.6 Reporting the results\n\nPresent fitted effects with confidence intervals, and provide a prediction interval for a meaningful scenario (e.g., a student profile of interest).\nSummarise key findings in plain language and note any limitations (e.g., omitted variables, sample size).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Simple linear regression\nA linear model with one predictor: \\(E[Y] = \\beta_0 + \\beta_1 X\\) with independent, mean-zero errors.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-mlr",
    "href": "glossary.html#gloss-mlr",
    "title": "Glossary",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nA linear model with two or more predictors; each coefficient is a partial effect holding the others fixed.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-residual",
    "href": "glossary.html#gloss-residual",
    "title": "Glossary",
    "section": "Residual",
    "text": "Residual\nAn observed value minus its fitted value from the model: \\(e_i = y_i - \\hat y_i\\).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-rse",
    "href": "glossary.html#gloss-rse",
    "title": "Glossary",
    "section": "Residual standard error",
    "text": "Residual standard error\nThe estimated standard deviation of the residuals (square root of the residual variance).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-adjusted-r-squared",
    "href": "glossary.html#gloss-adjusted-r-squared",
    "title": "Glossary",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\n\\(R^2\\) penalised for the number of predictors to discourage unnecessary terms.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-aic",
    "href": "glossary.html#gloss-aic",
    "title": "Glossary",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\nModel comparison metric that balances fit and complexity; lower values indicate a preferred model among those compared.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-multicollinearity",
    "href": "glossary.html#gloss-multicollinearity",
    "title": "Glossary",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nStrong correlation among predictors that inflates standard errors and destabilises estimates.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-interaction",
    "href": "glossary.html#gloss-interaction",
    "title": "Glossary",
    "section": "Interaction",
    "text": "Interaction\nA term that allows the effect of one predictor to depend on the level of another (e.g., \\(X_1 X_2\\)).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-dummy",
    "href": "glossary.html#gloss-dummy",
    "title": "Glossary",
    "section": "Dummy variable",
    "text": "Dummy variable\nAn indicator (0/1) used to code categories in regression relative to a baseline.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-leverage",
    "href": "glossary.html#gloss-leverage",
    "title": "Glossary",
    "section": "Leverage",
    "text": "Leverage\nA measure of how far a case’s predictor values are from the predictor means; high leverage can increase influence.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-cooks-distance",
    "href": "glossary.html#gloss-cooks-distance",
    "title": "Glossary",
    "section": "Cook’s distance",
    "text": "Cook’s distance\nInfluence measure combining residual size and leverage to flag points that change fitted values when omitted.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-box-cox",
    "href": "glossary.html#gloss-box-cox",
    "title": "Glossary",
    "section": "Box–Cox transformation",
    "text": "Box–Cox transformation\nA family of power transformations used to stabilise variance or improve linearity for positive responses.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-parsimony",
    "href": "glossary.html#gloss-parsimony",
    "title": "Glossary",
    "section": "Parsimony",
    "text": "Parsimony\nChoosing the simplest adequate model that answers the scientific question.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Regression and Model Selection with R",
    "section": "",
    "text": "1 Course Overview",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Overview</span>"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Linear Regression and Model Selection with R",
    "section": "1.1 What you will learn",
    "text": "1.1 What you will learn\n\nFormulate simple and multiple linear regression models and articulate their assumptions.\nInterpret regression coefficients, including partial, interaction, and categorical effects.\nEvaluate model utility using hypothesis tests, R-squared metrics, and prediction accuracy.\nIdentify when additive models fail and implement interaction or polynomial terms.\nIncorporate qualitative predictors using indicator (dummy) variables.\nApply principles of parsimony and evidence-based model building.\nUse AIC, multicollinearity diagnostics, and stepwise procedures for model selection.\nPerform residual analysis, identify influential observations, and apply transformations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Overview</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Linear Regression and Model Selection with R",
    "section": "1.2 How to use this book",
    "text": "1.2 How to use this book\n\nEach chapter corresponds to a module in the syllabus and ends with short practice prompts.\nR examples use base lm() and standard diagnostics; you can run code chunks directly if you enable execution.\nThe Assessments chapter outlines optional exercises and a capstone project that integrate exploration, model selection, and diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Overview</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#higher-order-linear-models",
    "href": "module03-model-building.html#higher-order-linear-models",
    "title": "5  Model Building and Selection",
    "section": "5.2 Higher-order linear models",
    "text": "5.2 Higher-order linear models\n\nSquare, Cubic, and Higher-Order Univariate Models\nWe enrich the basic linear model by adding powers of a predictor, allowing the fitted relationship to bend:\n\nSecond-order (Quadratic) univariate model\n\\[E[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2\\] captures one turning point.\n\n\nThird-order (cubic) univariate model\n\\[\nE[Y] = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3\n\\] allows more complex curvature.\n\n\nN-th order univariate model\nWe can extend this process of deriving new terms by multiplying the variable \\(x\\)\n\nreg= require(\"d3-regression\")\npolyData=transpose(nonlinearData)\nviewof degree = Inputs.range([1, 10], {step: 1, label: \"Model Order\", value: 1})\n\nfunction fittedCurve(order) {\n  const regression = reg.regressionPoly()\n    .x((d) =&gt; d.x)\n    .y((d) =&gt; d.y)\n    .order(degree)\n    .domain(d3.extent(polyData, (d) =&gt; d.x));\n  const curve = regression(polyData);\n  return curve.map(([x, y]) =&gt; ({x, y}));\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npolyTerms = Array.from({length: degree + 1}, (_, k) =&gt; {\n  if (k === 0) return `\\\\beta_{0}`;\n  if (k === 1) return `\\\\beta_{1} x`;\n  return `\\\\beta_{${k}} x^{${k}}`;\n}).join(\" + \");\n\nhtml`&lt;div style=\"text-align:center; margin: 0.5rem 0;\"&gt;${tex`E[Y] = ${polyTerms}`}&lt;/div&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marginLeft: 50,\n  height: 320,\n  x: {label: \"Predictor (x)\"},\n  y: {label: \"Outcome (y)\"},\n  marks: [\n    Plot.dot(polyData, {\n      x: \"x\",\n      y: \"y\",\n      r: 4,\n      fill: \"#1f77b4\",\n      opacity: 0.8,\n      title: (d) =&gt; `x = ${d.x.toFixed(2)}\\ny = ${d.y.toFixed(2)}`,\n    }),\n    Plot.line(fittedCurve(degree), {\n      x: \"x\",\n      y: \"y\",\n      stroke: \"#d62728\",\n      strokeWidth: 2,\n    }),\n  ],\n});\n\n\n\n\n\n\nNote that each term gets its own parameter (\\(\\beta_i\\)).\nHigher-order terms increase flexibility, but also increase the risk of fitting random noise rather than meaningful structure.\n\nNoteEven when terms like \\(X^2\\) or \\(X_1X_2\\) appear, these are still linear models because the parameters enter additively. “Linearity” refers to the parameters, not the shape of the fitted curve.\n\n\n\n\n\nInteraction Models with Continuous Predictors\nWith two continuous predictors, a first-order model is: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2.\n\\]\nTo let predictors influence each other’s effects, we add an interaction term: \\[\nE[Y] = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2.\n\\]\n\nIf \\(\\beta_3 = 0\\), predictors act independently.\nIf \\(\\beta_3 \\neq 0\\), the slope of \\(X_1\\) changes with \\(X_2\\), and vice versa.\n\nHigher-order interactions such as \\(X_1^2X_2\\) arise when curvature interacts with another predictor.\n\n\nInteraction Models with Categorical Predictors\nA categorical variable with \\(k\\) levels is represented by \\(k-1\\) dummy variables: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma_1D_1 + \\dots + \\gamma_{k-1}D_{k-1}.\n\\]\n\nEach \\(\\gamma_j\\) measures the difference between level \\(j\\) and the baseline.\nChanging the baseline changes interpretations but not fitted values.\n\nCategorical predictors may also interact with continuous predictors: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma_1D_1 + \\delta_1(XD_1),\n\\] allowing each group to have its own slope.\n\nNoteEarlier tools such as t-tests and one-way ANOVA are special cases of linear models with categorical predictors. Regression provides a unified framework that handles categorical, continuous, and interaction effects together.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#assessing-model-fit-and-model-complexity",
    "href": "module03-model-building.html#assessing-model-fit-and-model-complexity",
    "title": "5  Model Building and Selection",
    "section": "5.3 Assessing model fit and model complexity",
    "text": "5.3 Assessing model fit and model complexity\n\nWhy not increase model complexity?\nAdding terms always reduces residual error numerically, but may:\n\nIntroduce unnecessary noise,\nCreate unstable estimates,\nReduce interpretability,\nIncrease sensitivity to outliers,\nEncourage overfitting.\n\nA good model is no more complex than necessary to describe the main structure of the data.\n\n\n\nMulticollinearity\nMulticollinearity occurs when predictors are highly correlated, leading to:\n\nInflated standard errors,\nUnstable coefficient estimates,\nHigh sensitivity to small data changes.\n\nDiagnostics include:\n\nCorrelation matrices\nVariance Inflation Factors (VIF)\nCondition numbers\n\n\nExample 4If weight and engine displacement in a dataset are nearly perfectly correlated, including both may cause unstable coefficient signs and large standard errors.\n\n\n\n\n\nFit Metrics\n\n\\(R^2\\)\nProportion of variation explained by the model. Always increases when predictors are added.\n\n\nAdjusted \\(R^2\\)\nPenalises extra predictors. Increases only when a new term improves explanatory power beyond chance.\n\n\nInformation Criteria: AIC, BIC\nAIC balances fit and complexity: \\[\n\\text{AIC} = -2\\ell + 2k.\n\\] BIC penalises complexity more heavily: \\[\n\\text{BIC} = -2\\ell + k\\log(n).\n\\] Lower values indicate better trade-offs.\n\nNoteAIC is often preferred for prediction-oriented modelling; BIC tends to select simpler models.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#methods-for-building-parsimonious-models",
    "href": "module03-model-building.html#methods-for-building-parsimonious-models",
    "title": "5  Model Building and Selection",
    "section": "5.4 Methods for building parsimonious models",
    "text": "5.4 Methods for building parsimonious models\n\nStepwise Regression\nStepwise methods provide automated ways to search for simpler models.\n\nForward Selection\nBegin with a minimal model (often the intercept). Add predictors one at a time when they improve AIC or adjusted \\(R^2\\).\n\n\nBackward Selection\nBegin with a saturated model containing all candidate predictors. Remove predictors that do not meaningfully contribute.\n\nNoteStepwise procedures should be treated as screening tools, not definitive modelling strategies. Final model decisions should consider diagnostics, interpretability, and subject-matter knowledge.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#exercises",
    "href": "module03-model-building.html#exercises",
    "title": "5  Model Building and Selection",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nExercise 1\nA scatterplot of \\(Y\\) vs. \\(X\\) shows curvature.\n\nFit a straight-line model and inspect residuals.\nFit a model including \\(X^2\\).\nCompare AIC and adjusted \\(R^2\\) between the models.\nDiscuss whether the added complexity is justified.\n:::",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module03-model-building.html#however-as-before-increasing-model-complexity-in-this-way-raises-the-risk-of-overfitting-and-interpretability-challenges-which-we-will-discuss-later-in-this-module.",
    "href": "module03-model-building.html#however-as-before-increasing-model-complexity-in-this-way-raises-the-risk-of-overfitting-and-interpretability-challenges-which-we-will-discuss-later-in-this-module.",
    "title": "5  Model Building and Selection",
    "section": "5.2 However, as before, increasing model complexity in this way raises the risk of overfitting and interpretability challenges, which we will discuss later in this module.",
    "text": "5.2 However, as before, increasing model complexity in this way raises the risk of overfitting and interpretability challenges, which we will discuss later in this module.\n\nContinue\n\n\nFitting interaction models in R\nIn R, interaction terms can be included using the * operator in the formula. For example, to fit a second-order interaction model with two predictors:\n\nlm(Y ~ X1*X2*X3, data = mydata)\n\nThis expands to include both main effects and the interaction term. To include only the interaction term without main effects, use the : operator. For example, if we only wanted the interaction between X1 and X2, along with the linear terms for X1, X2, and X3:\n\nlm(Y ~ X1+X2+X3+X1:X2, data = mydata)\n\n\nContinue\n\n\n\nInteraction Models with Categorical Predictors\nA categorical variable with \\(k\\) levels is represented by \\(k-1\\) dummy variables: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma_1D_1 + \\dots + \\gamma_{k-1}D_{k-1}.\n\\]\n\nEach \\(\\gamma_j\\) measures the difference between level \\(j\\) and the baseline.\nChanging the baseline changes interpretations but not fitted values.a\n\nCategorical predictors may also interact with continuous predictors: \\[\nE[Y] = \\beta_0 + \\beta_1X + \\gamma_1D_1 + \\delta_1(XD_1),\n\\] allowing each group to have its own slope.\n\nNoteEarlier tools such as t-tests and one-way ANOVA are special cases of linear models with categorical predictors. Regression provides a unified framework that handles categorical, continuous, and interaction effects together.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  }
]