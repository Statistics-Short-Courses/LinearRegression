[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linear Regression and Model Selection with R",
    "section": "",
    "text": "1 Course Overview",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Overview</span>"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Linear Regression and Model Selection with R",
    "section": "1.1 What you will learn",
    "text": "1.1 What you will learn\n\nFormulate simple and multiple linear regression models and articulate their assumptions.\nInterpret regression coefficients, including partial, interaction, and categorical effects.\nEvaluate model utility using hypothesis tests, R-squared metrics, and prediction accuracy.\nIdentify when additive models fail and implement interaction or polynomial terms.\nIncorporate qualitative predictors using indicator (dummy) variables.\nApply principles of parsimony and evidence-based model building.\nUse AIC, multicollinearity diagnostics, and stepwise procedures for model selection.\nPerform residual analysis, identify influential observations, and apply transformations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Overview</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Linear Regression and Model Selection with R",
    "section": "1.2 How to use this book",
    "text": "1.2 How to use this book\n\nEach chapter corresponds to a module in the syllabus and ends with short practice prompts.\nR examples use base lm() and standard diagnostics; you can run code chunks directly if you enable execution.\nThe Assessments chapter outlines optional exercises and a capstone project that integrate exploration, model selection, and diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Overview</span>"
    ]
  },
  {
    "objectID": "module01-slr.html",
    "href": "module01-slr.html",
    "title": "2  Simple Linear Regression (SLR)",
    "section": "",
    "text": "2.1 When to use SLR",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "module01-slr.html#sec-LinearFit",
    "href": "module01-slr.html#sec-LinearFit",
    "title": "2  Simple Linear Regression (SLR)",
    "section": "2.2 Fitting models to data",
    "text": "2.2 Fitting models to data\nin the previous section, you were given a linear model - we knew the values of \\(\\alpha\\), \\(\\beta\\) and even \\(\\sigma\\). However, in most contexts we don’t know the true relationship between \\(X\\) and \\(Y\\) in advance. Instead, we are given data and try to infer the values of \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\) by analysing the data.\nIndeed, at the end of the last session we generated data from a linear model. In preparation for this chapter, we’ve generated data from a similar such linear model. Here it is:\n\nPlotData\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            x          y\n1   6.3760079  0.6455654\n2   6.6370954  0.4035721\n3  -3.0352540 -3.7999662\n4  -8.2389353 -9.5449309\n5  -3.0513450 -2.5410170\n6  -0.5572173 -4.7276965\n7  -2.2347887 -6.3122787\n8   7.1978811 -0.8794810\n9   3.9163960  1.7113658\n10  2.2322167 -6.8158073\n11  4.2098264 -0.1668024\n12  2.1158231 -4.0352427\n13  6.9788314 -1.2737527\n14  1.1704133 -2.5711720\n15 -2.0981967 -3.9098972\n16  1.7438984 -1.5709797\n17 -0.5355881 -2.4107105\n18 -2.1431666 -4.6993037\n19  8.2798920  1.7387922\n20 -1.3959860 -5.0201586\n\n\n\n\n\nyour task for the remainder of the chapter (and in regression generally) is to try an make a (educated) guess about what model produced this data.\n\nContinue\n\n\nEstimating parameters\nWe call the process of trying to guess the parameters in the data generating model (i.e. \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\)), estimation, and our guesses are estimates. To avoid confusion, we’ll denote the estimated intercept by \\(a\\) and the estimated slope by \\(b\\). So, we have\n\\[\na: \\text{Estimated intercept}\n\\] \\[\nb: \\text{Estimated slope}\n\\] \\[\n\\hat{\\sigma}: \\text{Estimated standard deviation}.\n\\] We also define \\(\\hat{Y}\\) as the predicted value of \\(Y\\) given our estimated model: \\[\n\\hat{Y}=a+bX\n\\] Looking at some data, we try to find the values of our parameters that best ‘fit’ its distribution. Adjust the sliders below to find a line that you think fits the data:\n\n\nviewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept (asjust)`})\nviewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})\n\nb0_1 = (-3 - 0.5*b1_1) + b0adj_1\n\ntex.block`\\hat{Y} = ${b0_1.toFixed(2)} + ${b1_1.toFixed(2)}x`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxRange = d3.extent(linearData.x)\n\nlineData_1 = xRange.map(x =&gt; ({x, y: b1_1 * x + b0_1}))\n\nPlot.plot({\n  marks: [\n    Plot.line(lineData_1, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10, 5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\nWhat is your best estimate for the values of \\(\\alpha\\) and \\(\\beta\\) that best fit the given data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOk great, so we have an estimate for our line-of-best-fit:\n\ntex.block`\\hat{Y} = ${my_a} + ${my_b}\\cdot X`\n\n\n\n\n\n\nbut what do we mean by ‘best’ fit here? How do we evaluate this estimate and maybe compare it to other estimates?\n\nContinue\n\n\n\nResiduals\nResiduals are the difference between observed values of \\(Y\\), and the value predicted by our estimated linear predictor \\(\\hat{Y}\\). We denote residuals with the letter \\(e\\): \\[\ne=Y-\\hat{Y} = Y - (a + bX).\n\\]\nResiduals, \\(e\\), are similar to the errors, \\(\\varepsilon\\), that we encountered in chapter 1 - but they are distinct. In many situations, we do not know the ‘acutual’ values of \\(\\alpha\\) and \\(\\beta\\) - so we cannot calculate the errors, \\(\\varepsilon= Y- E[Y]= Y-\\alpha + \\beta X\\). However, we do have our estimates, \\(a\\) and \\(b\\), so we can calculate residuals.\nIndeed - calculating residuals gives us a way to assess how well our estimated model fits the data. We can think of the residuals as being the ‘mismatch’ between our estimated linear predictor and each data point. By minimizing the size of residuals (minimisiong the mismatch of our model to the data), we can get a better fit of our line to data.\n\nContinue\n\n\n\nEstimating coefficients\n\nOptimising model fit by minimising (squared) residuals\nBelow, the plot now also displays residuals for each data point. Underneath the model equation is the Sum of Squared Residuals (SSR), which gives a measure of the absolute difference between our linear predictor and the observed outcomes.\n\\[\nSSR=\\sum_{i=1}^{n} e_i^2\n\\]\nwhere \\(e_i = Y_i - \\hat{Y}_i\\). This gives us a numerical measure of how well the line fits the data - see how low you can get the SSR by adjusting slope and intercept.\n\n\nviewof b0adj = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept`})\nviewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})\n\nb0 = -3 + b0adj\n\nresidualData = transpose(linearData).map(d =&gt; {\n  const yhat = b1 * d.x + b0;\n  const res  = d.y - yhat;\n  return {\n    ...d,\n    yhat,\n    residual: res,\n    sign: res &gt;= 0 ? \"positive\" : \"negative\",\n    absRes: Math.abs(res)\n  };\n})\n  \nSSRes = d3.sum(residualData, d =&gt; d.residual ** 2)\n\ntex.block`\\hat{Y} = ${b0} + ${b1}X`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntex.block`\\sum_{i} e_i^{2} = ${SSRes.toFixed(2)}`\n\n\n\n\n\n\n\n\n\n\n\nlineData = xRange.map(x =&gt; ({x, y: b1 * x + b0}))\n\nPlot.plot({\n  marks: [\n    Plot.ruleX(residualData, {x: \"x\",\n      y1: \"y\",\n      y2: \"yhat\",\n      stroke: \"sign\", \n      strokeOpacity: 0.75,\n      strokeWidth: d =&gt; 1 + d.absRes}),\n\n    Plot.line(lineData, { x: \"x\", y: \"y\" }),\n\n    Plot.dot(transpose(linearData), { x: \"x\", y: \"y\", r: 3 })\n  ],\n  x: { domain: xRange },\n  y: { domain: [-10,5], label: \"y\" },\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2\nBased on this visualisation, what do you think is the minimum possible \\(SSR\\) achievable?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want you can update your \\(a\\) and \\(b\\) estimates too (otherwise just proceed):\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe least-squares estimate\nWe call the estimates \\(a\\) and \\(b\\) which minimise the sum of squares the least squares estimates. Some nice mathematics tells us that the least squares esimates for a simple linear model are unique - that is, there is one set of values for \\(a\\) and \\(b\\) which satisfy this property. Moreover, we don’t have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute \\(a\\) and \\(b\\) very efficiently. As a statistical programming language, this is something R does very easily..\n\n\nFitting a model with the lm() function\nIn R the lm() function computes the least squares estimates \\(a\\) and \\(b\\) for our simple linear model (among other things) in a single command:\n\n\n\n\n\n\n\n\nWe can extract the coefficients from the lm by indexing:\n\n\n\n\n\n\n\n\nor by the coef() function:\n\n\n\n\n\n\n\n\n\nHow do these estimates compare with yours?\n\nLets compare the estimated linear fits graphically:\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinue\n\n\n\nEstimating variance\nOnce the line has been fitted (i.e. \\(\\alpha\\) and \\(\\beta\\) have been estimated as \\(a\\) and \\(b\\)), we also have to estimate the distribution of error terms (remember, as per ?sec-varAssumption, the error distribution has a constant variance defined by \\(\\sigma^2\\)). As you might expect, we again utilise the residuals from our least squares linear predictor to estimate \\(\\sigma\\). The spread of the error terms will be estimated by the spread of the residuals around our line of best fit.\nWe can extract the individual residual values from the fitted lm object by indexing (e.g. my_lm$residuals) or with the residuals() function.\n\n\n\n\n\n\n\n\nThe resulting R object is a vector of the residual for each observation. i.e. residuals(lm_1)[[3]] \\(= e_3\\)\n\nExercise 3: Extracting residuals from an lm object\nExtract the residuals from your least squares fitted model lm_1by indexing and assign them to the variable e\n\n\n\n\n\n\n\n\n\n\n\ne &lt;- lm_1$residuals\ne &lt;- lm_1$residuals\n\n\n\n\n\n\n\n\n\n\nResidual Standard Error\nDividing the residual sum of squares by \\(n-2\\) (one degree of freedom lost per fitted coefficient) gives the mean squared error, and taking the square root yields the residual standard deviation (aka. the Residual-Standard-Error) \\[\ns^2 = \\frac{1}{n-2}\\sum_{i=1}^{n} e_i^2= \\frac{SSE}{n-2}, \\quad s=\\sqrt{s^2}.\n\\] Interpreting \\(\\hat \\sigma\\) is often easier on the original \\(y\\) scale: a typical observation falls about \\(\\hat \\sigma\\) units away from the fitted line.\n\\(s^2\\) and \\(s\\) are our estimators for \\(\\sigma^2\\) and \\(\\sigma\\), respectively. ::: Exercise #### Calculate the Residual Standard Error using your e object defined in the previous exercise, calculate the residual standard error of the least squares model (hint - there are 20 observations in the linearData dataset)\n\n\n\n\n\n\n\n\n\n\n\nrse &lt;- sqrt(sum(e^2)/(nrow(linearData)-2))\nrse &lt;- sqrt(sum(e^2)/(nrow(linearData)-2))\n\n\n\n\n\n\n:::\n\n\nCorrelation and \\(R^2\\)\nThe Pearson-Correlation coefficient, \\(r\\), measures how strongly \\(X\\) and \\(Y\\) move together along a straight line, taking values between \\(-1\\) (perfect negative linear relationship) and \\(1\\) (perfect positive linear relationship).\nIn simple linear regression with an intercept, the R-Squared value can be written as \\[\nR^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = r^2,\n\\] so \\(R^2\\) captures the proportion of the total variation in \\(Y\\) that is explained by the fitted line.\n\nExercise 4: Correlation and \\(R^2\\) in our example\nCalculate the correlation between x and y, then compute \\(R^2\\) using the residuals from lm_1.\n\n\n\n\n\n\n\n\n\n\n\nr &lt;- cor(linearData$x, linearData$y)\nr_squared &lt;- 1 - sum(e^2) / sum((linearData$y - mean(linearData$y))^2)\nr &lt;- cor(linearData$x, linearData$y)\nr_squared &lt;- 1 - sum(e^2) / sum((linearData$y - mean(linearData$y))^2)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "module01-slr.html#inference-for-slr",
    "href": "module01-slr.html#inference-for-slr",
    "title": "2  Simple Linear Regression (SLR)",
    "section": "2.3 Inference for SLR",
    "text": "2.3 Inference for SLR\nUp to this point, we have fitted a straight-line model to a sample of data, obtaining estimates \\(a\\) and \\(b\\) for the intercept and slope. These estimates describe the pattern visible in the sample, but they do not tell us how closely they reflect the true population parameters \\(\\alpha\\) and \\(\\beta\\). Because sampling introduces randomness, different samples would produce different fitted lines.\nThis raises the central question:\nHow much confidence can we place in the slope and intercept we estimated, and what do they tell us about the true relationship in the population?\nTo answer this, we rely on statistical inference, which quantifies the uncertainty in the estimates and evaluates whether the predictor genuinely influences the response.\nImportantly, the inferential procedures we use rest on the assumptions of the linear regression model:\n\nThe errors \\(\\varepsilon\\) have mean \\(0\\).\nThey have constant variance \\(\\sigma^2\\) (homoscedasticity).\nThey are independent.\nThey are Normally distributed.\n\nThe Normality assumption is what allows us to derive the sampling distributions of \\(a\\) and \\(b\\), leading directly to the t-tests and confidence intervals used for inference. Without these assumptions—especially Normality—the exact forms of these inferential tools would not hold.\n\nNote 1For a broader introduction to statistical inference, see the Inferential Statistics with R short course. If concepts such as the Normal distribution or t-tests feel unfamiliar, please review that material before continuing.\n\n\n\n\nInference About the Slope, \\(\\beta\\)\nIn simple linear regression, the population relationship is modelled as\n\\[\nY = \\alpha + \\beta x + \\varepsilon.\n\\]\nTo determine whether \\(x\\) is genuinely associated with \\(Y\\), we test:\n\\[\nH_0: \\beta = 0\n\\qquad \\text{vs.} \\qquad\nH_a: \\beta \\ne 0.\n\\]\n\nUnder \\(H_0\\), changes in \\(x\\) do not affect the mean of \\(Y\\) (a change in \\(X\\) will lead to \\(\\beta\\cdot X = 0\\cdot X = 0\\) change in \\(Y\\)).\nUnder \\(H_a\\), there is evidence of a real linear effect (i.e. a change in \\(X\\) will lead to a non-zero change in \\(Y\\)).\n\nBecause the Normality assumption implies that the estimator \\(b\\) has a Normal sampling distribution (and hence a \\(t\\) distribution once \\(\\sigma\\) is estimated), we are able to quantify how unusual our observed slope would be if \\(H_0\\) were correct.\n\nExercise 5: Hypothesis testing revision (p-values)\n\n\n\nThe t-Test for the Slope\nThe hypothesis test is carried out using the statistic\n\\[\nt = \\frac{b}{\\text{SE}(b)},\n\\]\nwhich follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom.\nInterpretation:\n\nA large value of \\(|t|\\) (small p-value) indicates evidence that \\(\\beta \\ne 0\\).\nA small value of \\(|t|\\) suggests the data are consistent with no linear effect.\n\nThe validity of this test relies on the Normality of the errors, which guarantees that this \\(t\\) statistic follows the appropriate reference distribution.\n\nNote 2While the assumption of Normality is what theoretically grounds the use of the t-test for parameter estimates, SLR is quite robust to violations of this assumption and inferences about our slope parameters may be of interest even with non-normal residual distributions.\n\n\n\nExample 1: t-test for slope\n\n\n\nExercise 6: t-test for slope\n\n\n\n\nConfidence Interval for the Slope\nA \\((1-\\alpha)100%\\) confidence interval for \\(\\beta\\) is\n\\[\nb ;\\pm; t_{\\alpha/2,,n-2},\\text{SE}(b).\n\\]\nInterpretation:\n\nAn interval excluding zero indicates a likely genuine relationship.\nAn interval including zero suggests weaker evidence.\n\nConfidence intervals complement hypothesis tests by communicating both the direction and plausible magnitude of the effect.\n\n\n\nInference About the Response, \\(Y\\)\nOnce we have fitted a regression model, we often want to make statements about the value of the response at a given predictor value \\(x_0\\). There are two distinct quantities of interest:\n\nThe mean (average) response at \\(x_0\\): \\[\n\\mu_Y(x_0) = \\alpha + \\beta x_0.\n\\]\nA new individual response at \\(x_0\\): \\[\nY_{\\text{new}} = \\alpha + \\beta x_0 + \\varepsilon.\n\\]\n\nThese involve different uncertainties, and therefore require different intervals.\n\nConfidence Interval for the Mean Response\nLet \\(\\hat{y}_0 = a + b x_0\\) be the fitted value at \\(x_0\\). A confidence interval for the mean response is:\n\\[\n\\hat{y}*0\n;\\pm;\nt*{\\alpha/2,,n-2} ,\ns\\sqrt{\n\\frac{1}{n} +\n\\frac{(x_0 - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n}.\n\\]\nThis interval quantifies uncertainty in the average value of \\(Y\\) for units with predictor value \\(x_0\\).\npredict(fit, newdata = new_point, interval = \"confidence\")\n\n\nPrediction Interval for a New Observation\nTo predict an individual outcome at \\(x_0\\), we must include the additional uncertainty from the random error \\(\\varepsilon\\):\n\\[\n\\hat{y}*0\n;\\pm;\nt*{\\alpha/2, , n-2} ,\ns\\sqrt{\n1 +\n\\frac{1}{n} +\n\\frac{(x_0 - \\bar{x})^2}{\\sum (x_i - \\bar{x})^2}\n}.\n\\]\nBecause of the extra “1” term, prediction intervals are always wider than confidence intervals.\npredict(fit, newdata = new_point, interval = \"prediction\")\n\n\nSummary\n\nConfidence interval → uncertainty in the expected value at \\(x_0\\)\nPrediction interval → uncertainty in a new outcome at \\(x_0\\)\n\n\n\nContinue\n\nShould I include a (brief) section on residuals diagnostics here or save that for the dedicated chapter??",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression (SLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html",
    "href": "module02-mlr.html",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "4.1 Introduction\nMultiple linear regression (MLR) extends simple linear regression by allowing \\(Y\\) to depend on more than one predictor variable. This enables richer models and allows us to estimate the partial contribution of each predictor while accounting for others.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#introduction",
    "href": "module02-mlr.html#introduction",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "",
    "text": "When to Extend SLR\nSLR is limited to one predictor. MLR becomes appropriate when:\n\nMultiple factors plausibly influence the response.\nExcluding predictors may bias results.\nYou wish to measure the unique contribution of each predictor while controlling for others.\n\n\n\nA graphical motivation\nLets consider a simple example:\nWe might look at each bivariate (i.e. two variables) relationship separately:\n\n\\(Y \\sim X_1\\)\\(Y\\sim X_2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever we might also look at the multivariate relationship\n\n\n\n\n\n\n\n\n\nin the case of 3 variables we can also extend our visualisation to 3 dimensions:\n\n\n\n\n\n\n\nContinue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#the-mlr-model",
    "href": "module02-mlr.html#the-mlr-model",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "4.2 The MLR Model",
    "text": "4.2 The MLR Model\nMultiple linear model extends the simple linear model straightforwardly by adding the extra variables to the deterministic linear predictor, each with its own parameter.  \n\\[\nY = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\varepsilon, \\quad \\varepsilon \\sim N(0,\\sigma^2)\n\\]\nNow, instead of a single predictor \\(X\\), we may have several (\\(k\\)) predictors \\(x_1, x_2, \\ldots, x_k\\), each corresponding to a column in our dataset. We use an index to distinguish these predictors, and each predictor \\(x_i\\) has a corresponding parameter \\(\\beta_i\\) governing its effect on the response. Note that we have updated the intercept parameter \\(\\alpha\\) from the simple linear regression section to \\(\\beta_0\\), because it is simply another parameter in the model!\nThe random error term, \\(\\varepsilon\\) stays the same, so the assumptions of MLR are very similar to those of SLR:\n\nAssumption: Assumptions of the MLR model\n\nA linear predictor, e.g. \\(E[Y]=\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k\\).\nIndependent and identically distributed random error terms that\n\nHave a mean \\(\\mu=0\\), and a constant variance \\(Var(\\varepsilon)=\\sigma^2\\)\nFollow a normal distribution: \\(N(0,\\sigma^2)\\)\n\n\n\n\n\nNote: Multivariate Linearity\nAlthough this is still a linear model, the term “linear” no longer refers to a literal straight line in two dimensions as it did in simple linear regression. Instead, “linear” means that the model is linear in its parameters: each \\(\\beta_i\\) multiplies a predictor and enters additively. This linear model may live in a high-dimensional predictor space and cannot be visualised as a single line. For example, for the three dimensional data presented above (one outcome: \\(Y\\) + two predictors: \\(X_1\\), \\(X_2\\)), a linear model will instead look like a three dimensional plane\n\n\n\n\n\n\n\n\n\nPartial Regression Coefficients\nEven though our model is no longer just a straight line, the familiar slope interpretation from simple linear regression still applies to each individual parameter: \\(\\beta_i\\) now describes the expected change in the mean response for a one-unit increase in \\(x_i\\), holding all other predictors constant.\n\nExample 1\nInterpreting a partial coefficient If the fitted model for life expectancy includes Internet usage and BirthRate,\nInternet: 0.112\nBirthRate: -0.594\nThen:\n\nA 1% increase in internet usage is associated with an expected 0.11-year increase in life expectancy, holding other predictors fixed.\nA one‑unit increase in birth rate corresponds to an expected 0.59‑year decrease in life expectancy, controlling for all other predictors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#fitting-mlr-models-in-r",
    "href": "module02-mlr.html#fitting-mlr-models-in-r",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "4.3 Fitting MLR Models in R",
    "text": "4.3 Fitting MLR Models in R\nMLR uses the same lm() function as SLR.\nmod &lt;- lm(LifeExp ~ Population + Health + Internet + BirthRate,\n          data = countries.df)\nsummary(mod)\nThe coefficient table includes:\n\nEstimate (_i)\nStandard error\nt‑statistic and p‑value for testing (H_0 : _i = 0)\n\n\nIndividual Coefficient Inference\nAs in SLR, individual t‑tests assess whether predictors are useful, but now these tests reflect conditional usefulness.\nconfint(mod)\n\nExercise 1\nUsing a provided regression summary:\n\nIdentify which predictors are useful at the 5% level.\nInterpret the adjusted \\(R^2\\).\nState the conclusion of the global F‑test.\nInterpret one coefficient in context.\n:::\n\n\n\n4.4 Global Model Usefulness: The F-Test\nThe global F-test evaluates whether the model as a whole is useful. Rather than looking at each coefficient separately, it asks whether any of the predictors contribute to explaining variation in the response.\nWe test:\n\n\\(H_0\\): all slope coefficients are zero (no linear relationship between \\(Y\\) and the predictors),\n\\(H_a\\): at least one slope coefficient is non-zero (at least one predictor is useful).\n\nThe F-statistic and its p-value are reported in the summary(lm) output. A small p-value (for example, less than 0.05) provides evidence that the model is useful overall.\n\nExampleIn the life expectancy example, the output might report\nF-statistic: 28.2 on 4 and 44 DF,  p-value: 1.19e-11\nThe very small p-value indicates strong evidence against \\(H_0\\). We conclude that at least one of the predictors (Population, Health, Internet, BirthRate) is useful for predicting life expectancy, and that the model is useful overall.\n\n\n\nExerciseGiven an F-statistic of 12.3 with p = 0.0004, state the null and alternative hypotheses and conclude whether the model is useful at the 5% level.\n\n\n\n\n\n4.5 Multicollinearity (Brief Overview)\nIn multiple regression, some predictors are often correlated with each other. This is known as multicollinearity.\nWhen multicollinearity is present:\n\nPredictors share overlapping information about the response.\nStandard errors of the affected coefficients can become large.\nCoefficients can appear non-significant even when the variables are important.\nCoefficient signs may be unstable and difficult to interpret.\n\nA simple descriptive check is to look at correlations between predictors, or a pairs plot. More formal diagnostics (such as variance inflation factors) and strategies for dealing with multicollinearity are covered in Module 6.\n\nExerciseSuppose two predictors in a regression model are very strongly positively correlated. Briefly explain how this might affect (a) the standard errors of their coefficients and (b) your ability to interpret their partial effects.\n\n\n\n\n\n4.6 Measures of Model Fit: \\(R^2\\) and Adjusted \\(R^2\\)\nAs in simple linear regression, \\(R^2\\) measures the proportion of variation in \\(Y\\) explained by the model.\nHowever, in MLR \\(R^2\\) always increases (or at least does not decrease) when we add more predictors, even if the new predictors are not truly useful.\nAdjusted \\(R^2\\) is a modified version of \\(R^2\\) that includes a penalty for the number of predictors. It increases only when a new predictor improves the model more than would be expected by chance.\nUse adjusted \\(R^2\\) when comparing models with different numbers of predictors.\n\n\n\n4.7 Summary\nThis chapter introduced the multiple regression model, the interpretation of partial coefficients, and the core inferential tools used to assess model usefulness. Later modules will extend these ideas to interactions, categorical predictors, model selection, and diagnostic analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#global-model-usefulness-the-f-test",
    "href": "module02-mlr.html#global-model-usefulness-the-f-test",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "4.4 Global Model Usefulness: The F-Test",
    "text": "4.4 Global Model Usefulness: The F-Test\nThe global F-test evaluates whether the model as a whole is useful. Rather than looking at each coefficient separately, it asks whether any of the predictors contribute to explaining variation in the response.\nWe test:\n\n\\(H_0\\): all slope coefficients are zero (no linear relationship between \\(Y\\) and the predictors),\n\\(H_a\\): at least one slope coefficient is non-zero (at least one predictor is useful).\n\nThe F-statistic and its p-value are reported in the summary(lm) output. A small p-value (for example, less than 0.05) provides evidence that the model is useful overall.\n\nExampleIn the life expectancy example, the output might report\nF-statistic: 28.2 on 4 and 44 DF,  p-value: 1.19e-11\nThe very small p-value indicates strong evidence against \\(H_0\\). We conclude that at least one of the predictors (Population, Health, Internet, BirthRate) is useful for predicting life expectancy, and that the model is useful overall.\n\n\n\nExerciseGiven an F-statistic of 12.3 with p = 0.0004, state the null and alternative hypotheses and conclude whether the model is useful at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#multicollinearity-brief-overview",
    "href": "module02-mlr.html#multicollinearity-brief-overview",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "4.5 Multicollinearity (Brief Overview)",
    "text": "4.5 Multicollinearity (Brief Overview)\nIn multiple regression, some predictors are often correlated with each other. This is known as multicollinearity.\nWhen multicollinearity is present:\n\nPredictors share overlapping information about the response.\nStandard errors of the affected coefficients can become large.\nCoefficients can appear non-significant even when the variables are important.\nCoefficient signs may be unstable and difficult to interpret.\n\nA simple descriptive check is to look at correlations between predictors, or a pairs plot. More formal diagnostics (such as variance inflation factors) and strategies for dealing with multicollinearity are covered in Module 6.\n\nExerciseSuppose two predictors in a regression model are very strongly positively correlated. Briefly explain how this might affect (a) the standard errors of their coefficients and (b) your ability to interpret their partial effects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#measures-of-model-fit-r2-and-adjusted-r2",
    "href": "module02-mlr.html#measures-of-model-fit-r2-and-adjusted-r2",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "4.6 Measures of Model Fit: \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "4.6 Measures of Model Fit: \\(R^2\\) and Adjusted \\(R^2\\)\nAs in simple linear regression, \\(R^2\\) measures the proportion of variation in \\(Y\\) explained by the model.\nHowever, in MLR \\(R^2\\) always increases (or at least does not decrease) when we add more predictors, even if the new predictors are not truly useful.\nAdjusted \\(R^2\\) is a modified version of \\(R^2\\) that includes a penalty for the number of predictors. It increases only when a new predictor improves the model more than would be expected by chance.\nUse adjusted \\(R^2\\) when comparing models with different numbers of predictors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#summary",
    "href": "module02-mlr.html#summary",
    "title": "4  Multiple Linear Regression (MLR)",
    "section": "4.7 Summary",
    "text": "4.7 Summary\nThis chapter introduced the multiple regression model, the interpretation of partial coefficients, and the core inferential tools used to assess model usefulness. Later modules will extend these ideas to interactions, categorical predictors, model selection, and diagnostic analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module03-interaction-nonlinearity.html",
    "href": "module03-interaction-nonlinearity.html",
    "title": "4  Interaction and Nonlinearity",
    "section": "",
    "text": "4.1 Interaction models with quantitative predictors\nint_mod &lt;- lm(mpg ~ wt * hp, data = mtcars)\ncoef(int_mod)[c(\"wt\", \"hp\", \"wt:hp\")]\n\n         wt          hp       wt:hp \n-8.21662430 -0.12010209  0.02784815",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction and Nonlinearity</span>"
    ]
  },
  {
    "objectID": "module03-interaction-nonlinearity.html#interaction-models-with-quantitative-predictors",
    "href": "module03-interaction-nonlinearity.html#interaction-models-with-quantitative-predictors",
    "title": "4  Interaction and Nonlinearity",
    "section": "",
    "text": "Allow the effect of one predictor to depend on another (interaction): \\[E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2.\\]\n\\(\\beta_3\\) shifts the slope of \\(X_1\\) per-unit change in \\(X_2\\) (and vice versa).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction and Nonlinearity</span>"
    ]
  },
  {
    "objectID": "module03-interaction-nonlinearity.html#graphical-interpretation-of-interaction-effects",
    "href": "module03-interaction-nonlinearity.html#graphical-interpretation-of-interaction-effects",
    "title": "4  Interaction and Nonlinearity",
    "section": "4.2 Graphical interpretation of interaction effects",
    "text": "4.2 Graphical interpretation of interaction effects\n\nPlot fitted lines across a grid to see slope changes.\n\n\nlibrary(ggplot2)\ngrid &lt;- expand.grid(wt = seq(2, 4, 0.5), hp = c(90, 150))\ngrid$fit &lt;- predict(int_mod, grid)\nggplot(grid, aes(wt, fit, colour = factor(hp))) +\n  geom_line() + labs(colour = \"HP level\", y = \"Fitted mpg\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction and Nonlinearity</span>"
    ]
  },
  {
    "objectID": "module03-interaction-nonlinearity.html#polynomial-models-quadratic-and-cubic",
    "href": "module03-interaction-nonlinearity.html#polynomial-models-quadratic-and-cubic",
    "title": "4  Interaction and Nonlinearity",
    "section": "4.3 Polynomial models: quadratic and cubic",
    "text": "4.3 Polynomial models: quadratic and cubic\n\nCapture curvature by adding powers: \\[E[Y] = \\beta_0 + \\beta_1 X +\n\\beta_2 X^2 \\;(+\\; \\beta_3 X^3).\\]\nUse poly() or explicit powers; center \\(X\\) to reduce collinearity.\n\n\npoly_mod &lt;- lm(mpg ~ wt + I(wt^2), data = mtcars)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction and Nonlinearity</span>"
    ]
  },
  {
    "objectID": "module03-interaction-nonlinearity.html#when-and-how-to-model-curvature",
    "href": "module03-interaction-nonlinearity.html#when-and-how-to-model-curvature",
    "title": "4  Interaction and Nonlinearity",
    "section": "4.4 When and how to model curvature",
    "text": "4.4 When and how to model curvature\n\nUse scatterplots and residual-vs-fitted plots to spot nonlinearity.\nPrefer low-order polynomials for interpretability; consider splines for flexible shapes if allowed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction and Nonlinearity</span>"
    ]
  },
  {
    "objectID": "module03-interaction-nonlinearity.html#extrapolation-risks-and-overfitting-concerns",
    "href": "module03-interaction-nonlinearity.html#extrapolation-risks-and-overfitting-concerns",
    "title": "4  Interaction and Nonlinearity",
    "section": "4.5 Extrapolation risks and overfitting concerns",
    "text": "4.5 Extrapolation risks and overfitting concerns\n\nPolynomial terms can explode outside the data range—avoid predicting far beyond observed \\(X\\).\nGuard against overfitting with cross-validation or an independent validation set when sample size permits.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction and Nonlinearity</span>"
    ]
  },
  {
    "objectID": "module04-qualitative-predictors.html",
    "href": "module04-qualitative-predictors.html",
    "title": "5  Qualitative Predictors",
    "section": "",
    "text": "5.1 Dummy-variable coding\ncat_mod &lt;- lm(mpg ~ factor(cyl), data = mtcars)\nmodel.matrix(cat_mod)[1:5, ]\n\n                  (Intercept) factor(cyl)6 factor(cyl)8\nMazda RX4                   1            1            0\nMazda RX4 Wag               1            1            0\nDatsun 710                  1            0            0\nHornet 4 Drive              1            1            0\nHornet Sportabout           1            0            1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Qualitative Predictors</span>"
    ]
  },
  {
    "objectID": "module04-qualitative-predictors.html#dummy-variable-coding",
    "href": "module04-qualitative-predictors.html#dummy-variable-coding",
    "title": "5  Qualitative Predictors",
    "section": "",
    "text": "Represent \\(k\\)-level categorical predictors with \\(k-1\\) indicator variables (dummy variables); the omitted level is the baseline.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Qualitative Predictors</span>"
    ]
  },
  {
    "objectID": "module04-qualitative-predictors.html#baseline-category-interpretation",
    "href": "module04-qualitative-predictors.html#baseline-category-interpretation",
    "title": "5  Qualitative Predictors",
    "section": "5.2 Baseline category interpretation",
    "text": "5.2 Baseline category interpretation\n\nEach indicator coefficient compares its level to the baseline.\nRe-level with relevel() for more meaningful comparisons.\n\n\nmtcars$cyl &lt;- relevel(factor(mtcars$cyl), ref = \"6\")\nrelevel_mod &lt;- lm(mpg ~ cyl, data = mtcars)\ncoef(relevel_mod)\n\n(Intercept)        cyl4        cyl8 \n  19.742857    6.920779   -4.642857",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Qualitative Predictors</span>"
    ]
  },
  {
    "objectID": "module04-qualitative-predictors.html#regression-with-multi-level-factors",
    "href": "module04-qualitative-predictors.html#regression-with-multi-level-factors",
    "title": "5  Qualitative Predictors",
    "section": "5.3 Regression with multi-level factors",
    "text": "5.3 Regression with multi-level factors\n\nFit models with multiple factors and quantitative predictors; ensure design matrix is full rank (no redundant indicators).\n\n\nmix_mod &lt;- lm(mpg ~ wt + factor(carb), data = mtcars)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Qualitative Predictors</span>"
    ]
  },
  {
    "objectID": "module04-qualitative-predictors.html#mixing-categorical-and-continuous-predictors",
    "href": "module04-qualitative-predictors.html#mixing-categorical-and-continuous-predictors",
    "title": "5  Qualitative Predictors",
    "section": "5.4 Mixing categorical and continuous predictors",
    "text": "5.4 Mixing categorical and continuous predictors\n\nCombine factors and continuous terms; interaction terms allow different slopes by group (e.g., wt * cyl).\n\n\ngroup_slope &lt;- lm(mpg ~ wt * factor(gear), data = mtcars)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Qualitative Predictors</span>"
    ]
  },
  {
    "objectID": "module04-qualitative-predictors.html#connection-to-anova-style-hypotheses",
    "href": "module04-qualitative-predictors.html#connection-to-anova-style-hypotheses",
    "title": "5  Qualitative Predictors",
    "section": "5.5 Connection to ANOVA-style hypotheses",
    "text": "5.5 Connection to ANOVA-style hypotheses\n\nANOVA table for a factor tests whether any level differs from the baseline (joint \\(H_0\\) on all indicators).\nIn R, compare models with anova() or read the factor-level F-test in summary() output when using treatment coding.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Qualitative Predictors</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html",
    "href": "module05-model-building-selection.html",
    "title": "6  Model Building and Selection",
    "section": "",
    "text": "6.1 Exploratory analysis for model formulation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#exploratory-analysis-for-model-formulation",
    "href": "module05-model-building-selection.html#exploratory-analysis-for-model-formulation",
    "title": "6  Model Building and Selection",
    "section": "",
    "text": "Start with plots (pairs, scatterplots, boxplots) to understand ranges, outliers, and plausible functional forms.\nUse subject-matter knowledge to posit candidate predictors and interactions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#systematic-vs-random-variation",
    "href": "module05-model-building-selection.html#systematic-vs-random-variation",
    "title": "6  Model Building and Selection",
    "section": "6.2 Systematic vs random variation",
    "text": "6.2 Systematic vs random variation\n\nDistinguish signal (systematic trend with predictors) from noise (unexplained scatter).\nResidual SD estimates random variation; large unexplained scatter may indicate missing predictors or wrong functional form.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#choosing-first--vs-second-order-functional-forms",
    "href": "module05-model-building-selection.html#choosing-first--vs-second-order-functional-forms",
    "title": "6  Model Building and Selection",
    "section": "6.3 Choosing first- vs second-order functional forms",
    "text": "6.3 Choosing first- vs second-order functional forms\n\nStart with additive, first-order (linear) terms; add interactions or low-order polynomials when plots or theory suggest them.\nPrefer centered predictors to stabilise estimates when adding higher-order terms.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#model-adequacy-and-interpretability",
    "href": "module05-model-building-selection.html#model-adequacy-and-interpretability",
    "title": "6  Model Building and Selection",
    "section": "6.4 Model adequacy and interpretability",
    "text": "6.4 Model adequacy and interpretability\n\nAdequate models fit the data (diagnostics pass) and support the scientific question.\nAvoid models that obscure interpretation with unnecessary complexity or unidentifiable effects.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#parsimony-as-a-guiding-principle",
    "href": "module05-model-building-selection.html#parsimony-as-a-guiding-principle",
    "title": "6  Model Building and Selection",
    "section": "6.5 Parsimony as a guiding principle",
    "text": "6.5 Parsimony as a guiding principle\n\nFavor the simplest model that explains the data and meets assumptions.\nRemove immaterial terms when they do not improve fit or align with theory; compare nested models via F-tests or information criteria.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#multicollinearity-detection-and-implications",
    "href": "module05-model-building-selection.html#multicollinearity-detection-and-implications",
    "title": "6  Model Building and Selection",
    "section": "6.6 Multicollinearity: detection and implications",
    "text": "6.6 Multicollinearity: detection and implications\n\nSymptoms: unstable coefficients, inflated standard errors, signs flipping with small data changes (multicollinearity).\nQuick checks: pairwise correlations, variance inflation factors (VIF), or condition numbers.\n\n\ncor(mtcars[, c(\"wt\", \"hp\", \"disp\", \"drat\")])\n\n             wt         hp       disp       drat\nwt    1.0000000  0.6587479  0.8879799 -0.7124406\nhp    0.6587479  1.0000000  0.7909486 -0.4487591\ndisp  0.8879799  0.7909486  1.0000000 -0.7102139\ndrat -0.7124406 -0.4487591 -0.7102139  1.0000000\n\nkappa(model.matrix(~ wt + hp + disp, data = mtcars))\n\n[1] 1639.418",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#akaike-information-criterion-aic",
    "href": "module05-model-building-selection.html#akaike-information-criterion-aic",
    "title": "6  Model Building and Selection",
    "section": "6.7 Akaike Information Criterion (AIC)",
    "text": "6.7 Akaike Information Criterion (AIC)\n\nBalances fit and complexity: \\(\\text{AIC} = -2\\ell + 2k\\); lower is better (AIC).\nCompare non-nested models with AIC(model1, model2, ...).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#forward-backward-and-stepwise-selection",
    "href": "module05-model-building-selection.html#forward-backward-and-stepwise-selection",
    "title": "6  Model Building and Selection",
    "section": "6.8 Forward, backward, and stepwise selection",
    "text": "6.8 Forward, backward, and stepwise selection\n\nForward: start simple, add terms that reduce AIC or improve fit.\nBackward: start saturated, remove weak terms.\nStepwise: alternate add/drop using step() (AIC by default).\n\n\nfull_mod &lt;- lm(mpg ~ ., data = mtcars)\nstep(full_mod, direction = \"both\", trace = 0)\n\n\nCall:\nlm(formula = mpg ~ wt + qsec + am, data = mtcars)\n\nCoefficients:\n(Intercept)           wt         qsec           am  \n      9.618       -3.917        1.226        2.936",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#limitations-and-cautions-for-stepwise-methods",
    "href": "module05-model-building-selection.html#limitations-and-cautions-for-stepwise-methods",
    "title": "6  Model Building and Selection",
    "section": "6.9 Limitations and cautions for stepwise methods",
    "text": "6.9 Limitations and cautions for stepwise methods\n\nData-driven searches can overfit and inflate Type I error.\nSelected models depend on starting set and may ignore theory; always validate with diagnostics and, if possible, new data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module05-model-building-selection.html#balancing-prediction-and-explanation",
    "href": "module05-model-building-selection.html#balancing-prediction-and-explanation",
    "title": "6  Model Building and Selection",
    "section": "6.10 Balancing prediction and explanation",
    "text": "6.10 Balancing prediction and explanation\n\nFor explanation, prioritise interpretability and scientific plausibility; for prediction, prioritise out-of-sample performance.\nConsider cross-validation or a hold-out set when sample size permits; report uncertainty from the final, diagnostically-sound model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Model Building and Selection</span>"
    ]
  },
  {
    "objectID": "module06-diagnostics.html",
    "href": "module06-diagnostics.html",
    "title": "7  Residual Analysis and Diagnostics",
    "section": "",
    "text": "7.1 Residual plots vs predictors and fitted values\ndiag_mod &lt;- lm(mpg ~ wt + hp + am, data = mtcars)\npar(mfrow = c(1, 2))\nplot(diag_mod, which = 1)      # residuals vs fitted\nplot(mtcars$wt, resid(diag_mod), xlab = \"wt\", ylab = \"Residuals\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residual Analysis and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module06-diagnostics.html#residual-plots-vs-predictors-and-fitted-values",
    "href": "module06-diagnostics.html#residual-plots-vs-predictors-and-fitted-values",
    "title": "7  Residual Analysis and Diagnostics",
    "section": "",
    "text": "Plot residuals against fitted values to check linearity and constant variance; add predictor-specific plots to spot functional-form issues (residual).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residual Analysis and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module06-diagnostics.html#diagnosing-nonconstant-variance-and-nonlinearity",
    "href": "module06-diagnostics.html#diagnosing-nonconstant-variance-and-nonlinearity",
    "title": "7  Residual Analysis and Diagnostics",
    "section": "7.2 Diagnosing nonconstant variance and nonlinearity",
    "text": "7.2 Diagnosing nonconstant variance and nonlinearity\n\nFunnel shapes or curved trends in residual plots suggest heteroskedasticity or missing curvature; consider transformations or adding interactions/polynomials.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residual Analysis and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module06-diagnostics.html#assessing-normality-of-residuals",
    "href": "module06-diagnostics.html#assessing-normality-of-residuals",
    "title": "7  Residual Analysis and Diagnostics",
    "section": "7.3 Assessing normality of residuals",
    "text": "7.3 Assessing normality of residuals\n\nUse Q-Q plots and compare \\(t\\)- and \\(p\\)-values to Normal reference.\n\n\nqqnorm(resid(diag_mod)); qqline(resid(diag_mod))",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residual Analysis and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module06-diagnostics.html#standardised-residuals-leverage-cooks-distance",
    "href": "module06-diagnostics.html#standardised-residuals-leverage-cooks-distance",
    "title": "7  Residual Analysis and Diagnostics",
    "section": "7.4 Standardised residuals, leverage, Cook’s distance",
    "text": "7.4 Standardised residuals, leverage, Cook’s distance\n\nStandardised (or studentised) residuals scale by estimated variance and flag unusual outcomes (|r| &gt; 2 as a heuristic).\nLeverage (hatvalues()) flags unusual predictor combinations (leverage).\nCook’s distance combines leverage and residual size to assess influence (Cook’s distance).\n\n\ncbind(rstudent(diag_mod),\n      hatvalues(diag_mod),\n      cooks.distance(diag_mod))[1:5, ]\n\n                        [,1]       [,2]        [,3]\nMazda RX4         -1.4433354 0.09320650 0.051538041\nMazda RX4 Wag     -1.1371729 0.12316145 0.044939134\nDatsun 710        -1.3051372 0.08856401 0.040365321\nHornet 4 Drive     0.3121886 0.07518198 0.002046732\nHornet Sportabout  0.4688483 0.07867173 0.004827051",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residual Analysis and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module06-diagnostics.html#transformations-including-boxcox",
    "href": "module06-diagnostics.html#transformations-including-boxcox",
    "title": "7  Residual Analysis and Diagnostics",
    "section": "7.5 Transformations including Box–Cox",
    "text": "7.5 Transformations including Box–Cox\n\nTransformations can stabilise variance or linearise relationships; for strictly positive \\(Y\\), consider Box–Cox transformations to guide power choices.\n\n\nMASS::boxcox(diag_mod, plotit = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residual Analysis and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module06-diagnostics.html#handling-outliers-and-influential-observations",
    "href": "module06-diagnostics.html#handling-outliers-and-influential-observations",
    "title": "7  Residual Analysis and Diagnostics",
    "section": "7.6 Handling outliers and influential observations",
    "text": "7.6 Handling outliers and influential observations\n\nInvestigate data quality first (entry errors, unusual units).\nIf influential points are real, fit with and without them to assess robustness; report how conclusions change.\nPrefer model adjustments (functional form, variance stabilisation) over automatic deletion; document any exclusions explicitly.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Residual Analysis and Diagnostics</span>"
    ]
  },
  {
    "objectID": "module07-case-naplan.html",
    "href": "module07-case-naplan.html",
    "title": "8  Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "8.1 Defining the research question",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module07-case-naplan.html#defining-the-research-question",
    "href": "module07-case-naplan.html#defining-the-research-question",
    "title": "8  Case Study: NAPLAN Reading Scores",
    "section": "",
    "text": "Investigate how student and school-level predictors relate to NAPLAN Reading scores. Clarify outcome, units, and any grouping structure.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module07-case-naplan.html#importing-data",
    "href": "module07-case-naplan.html#importing-data",
    "title": "8  Case Study: NAPLAN Reading Scores",
    "section": "8.2 Importing data",
    "text": "8.2 Importing data\n\n# Replace path with the appropriate CSV or RDS from the Resources/naplan reading folder\n# naplan &lt;- read.csv(\"Resources/naplan reading/naplan_reading.csv\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module07-case-naplan.html#exploratory-plot",
    "href": "module07-case-naplan.html#exploratory-plot",
    "title": "8  Case Study: NAPLAN Reading Scores",
    "section": "8.3 Exploratory plot",
    "text": "8.3 Exploratory plot\n\nStart with scatterplots and boxplots for key predictors (e.g., study time, SES, gender) against Reading scores.\n\n\n# Example placeholder using mtcars structure; swap to naplan data\nlibrary(ggplot2)\nggplot(mtcars, aes(wt, mpg)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Reading score\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module07-case-naplan.html#fitting-the-least-squares-model",
    "href": "module07-case-naplan.html#fitting-the-least-squares-model",
    "title": "8  Case Study: NAPLAN Reading Scores",
    "section": "8.4 Fitting the least-squares model",
    "text": "8.4 Fitting the least-squares model\n\nBegin with a first-order additive model; consider interactions or polynomials if exploratory plots suggest curvature.\n\n\n# model &lt;- lm(Reading ~ predictor1 + predictor2, data = naplan)\n# summary(model)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module07-case-naplan.html#model-refinement-and-interpretation",
    "href": "module07-case-naplan.html#model-refinement-and-interpretation",
    "title": "8  Case Study: NAPLAN Reading Scores",
    "section": "8.5 Model refinement and interpretation",
    "text": "8.5 Model refinement and interpretation\n\nCheck residual diagnostics (see Module 6). Address nonlinearity, heteroskedasticity, or influential points.\nInterpret coefficients, including categorical contrasts and any interaction terms.\n\n\n# plot(model, which = 1:2)\n# coef(summary(model))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "module07-case-naplan.html#reporting-the-results",
    "href": "module07-case-naplan.html#reporting-the-results",
    "title": "8  Case Study: NAPLAN Reading Scores",
    "section": "8.6 Reporting the results",
    "text": "8.6 Reporting the results\n\nPresent fitted effects with confidence intervals, and provide a prediction interval for a meaningful scenario (e.g., a student profile of interest).\nSummarise key findings in plain language and note any limitations (e.g., omitted variables, sample size).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Case Study: NAPLAN Reading Scores</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Simple linear regression\nA linear model with one predictor: \\(E[Y] = \\beta_0 + \\beta_1 X\\) with independent, mean-zero errors.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-mlr",
    "href": "glossary.html#gloss-mlr",
    "title": "Glossary",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nA linear model with two or more predictors; each coefficient is a partial effect holding the others fixed.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-residual",
    "href": "glossary.html#gloss-residual",
    "title": "Glossary",
    "section": "Residual",
    "text": "Residual\nAn observed value minus its fitted value from the model: \\(e_i = y_i - \\hat y_i\\).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-rse",
    "href": "glossary.html#gloss-rse",
    "title": "Glossary",
    "section": "Residual standard error",
    "text": "Residual standard error\nThe estimated standard deviation of the residuals (square root of the residual variance).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-adjusted-r-squared",
    "href": "glossary.html#gloss-adjusted-r-squared",
    "title": "Glossary",
    "section": "Adjusted R-squared",
    "text": "Adjusted R-squared\n\\(R^2\\) penalised for the number of predictors to discourage unnecessary terms.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-aic",
    "href": "glossary.html#gloss-aic",
    "title": "Glossary",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\nModel comparison metric that balances fit and complexity; lower values indicate a preferred model among those compared.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-multicollinearity",
    "href": "glossary.html#gloss-multicollinearity",
    "title": "Glossary",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nStrong correlation among predictors that inflates standard errors and destabilises estimates.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-interaction",
    "href": "glossary.html#gloss-interaction",
    "title": "Glossary",
    "section": "Interaction",
    "text": "Interaction\nA term that allows the effect of one predictor to depend on the level of another (e.g., \\(X_1 X_2\\)).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-dummy",
    "href": "glossary.html#gloss-dummy",
    "title": "Glossary",
    "section": "Dummy variable",
    "text": "Dummy variable\nAn indicator (0/1) used to code categories in regression relative to a baseline.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-leverage",
    "href": "glossary.html#gloss-leverage",
    "title": "Glossary",
    "section": "Leverage",
    "text": "Leverage\nA measure of how far a case’s predictor values are from the predictor means; high leverage can increase influence.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-cooks-distance",
    "href": "glossary.html#gloss-cooks-distance",
    "title": "Glossary",
    "section": "Cook’s distance",
    "text": "Cook’s distance\nInfluence measure combining residual size and leverage to flag points that change fitted values when omitted.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-box-cox",
    "href": "glossary.html#gloss-box-cox",
    "title": "Glossary",
    "section": "Box–Cox transformation",
    "text": "Box–Cox transformation\nA family of power transformations used to stabilise variance or improve linearity for positive responses.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#gloss-parsimony",
    "href": "glossary.html#gloss-parsimony",
    "title": "Glossary",
    "section": "Parsimony",
    "text": "Parsimony\nChoosing the simplest adequate model that answers the scientific question.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "module02-mlr.html#y-x_1",
    "href": "module02-mlr.html#y-x_1",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "3.2 \\(Y\\) ~ \\(X_1\\)",
    "text": "3.2 \\(Y\\) ~ \\(X_1\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  },
  {
    "objectID": "module02-mlr.html#y-x_2",
    "href": "module02-mlr.html#y-x_2",
    "title": "3  Multiple Linear Regression (MLR)",
    "section": "3.3 \\(Y\\) ~ \\(X_2\\)",
    "text": "3.3 \\(Y\\) ~ \\(X_2\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression (MLR)</span>"
    ]
  }
]