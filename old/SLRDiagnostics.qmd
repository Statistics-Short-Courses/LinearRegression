---
format: 
  live-html:
    toc: true
resources:
  - Data

execute:
  echo: true
  warning: false
  message: false

embed-resources: true

filters:
  - custom-numbered-blocks

custom-numbered-blocks:
  classes:
    Assumption:
      colors: [FFCDD2, F44336]
      boxstyle: foldbox.simple
      collapse: false
      numbered: false
    Example:
      colors: [BBDEFB, 2196F3]
      boxstyle: foldbox.simple
      collapse: false
    Exercise:
      colors: [C8E6C9, 4CAF50]
      boxstyle: foldbox.simple
      collapse: false
    Technical-point:
      colors: [FFF9C4, FFEB3B]
      boxstyle: foldbox.simple
      collapse: true
webr:
  render-df: gt-interactive
  packages: 
    - dplyr
    - readr
    - ggplot2
  cell-options:
    echo: true
    warning: false
    message: false
---
{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}
{{< include ./_extensions/r-wasm/live/_gradethis.qmd >}}


```{r}
#| include: false
library(tidyverse)
```

```{webr}
#| include: false
theme_set(theme_bw())
```

# Fit diagnostics and assumption checks 
In @sec-chap1, we introduced the simple linear model by making assumptions about how our outcome variable, $Y$, could be expressed in terms of a predictor variable, $X$, and a random error term, $\varepsilon$:

::: Assumption
## The simple linear model {.unnumbered}
$$
Y_i = \alpha + \beta X_i + \varepsilon_i, \quad{\varepsilon_i \sim N(0,\sigma^2)}.
$$
:::

When performing simple linear regression, the assumption we make is that our data were generated by a simple linear model. 

In particular, we assume

- A *linear* relationship between the expected value of our outcome and the predictor, $E[Y]=\alpha+\beta \cdot X$. 
- *Independent* and identically distributed random error terms that
  - Have a mean $\mu=0$, and a constant variance $Var(\varepsilon)=\sigma^2$
  - Follow a normal distribution: $N(0,\sigma^2)$

If any of these conditions does not hold, the simple linear model may not be an appropriate model for our data and we may need to re-evaluate our modelling choices. We can use our fitted model -in particular, it's residuals- to 'check' the appropriateness of the simple linear model for our data.

-  [ ] Continue

## Using residuals to diagnose fit and checks
While we might be able to assess our model fit base just on a scatter plot of $X$ vs $Y$ and the overlayed linear fit (as we have been doing so far), we can use the residuals we have calculated to great effect for diagnosing how and where our model does not fit the data. Remember that a residual $e_i = Y_i - \hat{Y}_i$ is literally the difference between our model (predictions) and the actual value of our outcome variable, so by looking at residuals we get a  much clearer picture of where our fit might be failing. 

### Using `plot(lm)` for residual plots

Because residual plots are so useful for checking linear models, the default behaviour for base R's `plot()` function when given an `lm` object is to output a selection of 'residual plots' for diagnosing model fit. We will use each of these plots in turn for asessing our model fit and checking that our model assumptions hold. 

-  [ ] Continue

## Linearity 
In @sec-Linearity we made a key assumption about the functional form of our model - that the expected value of the outcome, $E[Y]$, was a linear function of the parameters $\alpha$ and $\beta$ and the predictor $X$:

::: Assumption
### Linearity
$$
E[Y]= \alpha + \beta X
$$
:::

In other words, we expect the scatter plot of $X$ vs $Y$ to follow a straight line like we have saw in @sec-sec-LinearFit. 
### Diagnostics
Because our linear fit can only account for straight-line relationships, any non-linearity  in the relationship will still be 'left-over' in the model residuals. We can look for such non linearity in the 'residuals vs fitted' plot:

::: Example
#### Non-linearity in cholesterol vs fat intake
The `Olympic.txt` dataset reports cholesterol levels (milgrams per liter) and fat intake (milligrams) of 20 olympic athletes:

```{webr}
#| edit: false
olympic <- read_table("Data/OLYMPIC.txt")[1:2]
```

:::: {.panel-tabset}
##### Data
```{webr}
#| edit: false
olympic
```

##### Scatter plot

```{webr}
#| edit: false
ggplot(olympic)+
  aes(FAT, CHOLESTEROL)+
  geom_point()
```
::::

Fitting a simple linear regression to this curved data leaves the obvious non-linear trend remaining in the residuals:
```{webr}
#| edit: false
olympic_lm <- lm(CHOLESTEROL~FAT, data=olympic)
```

:::: {.panel-tabset}
##### Linear fit

```{webr}
#| edit: false
ggplot(olympic)+
  aes(x=FAT, y=CHOLESTEROL)+
  geom_point()+
  geom_abline(aes(intercept= coef(olympic_lm)[[1]], slope=coef(olympic_lm)[[2]]))
```
##### Residuals vs fitted

```{webr}
#| edit: false
plot(olympic_lm, which=1)
```

##### Residuals vs FAT

```{webr}
#| edit: false
plot(olympic_lm$residuals~olympic$FAT)
abline(h=0, col="darkblue")
```
::::
:::

## Independence of error terms

::: Example
Researchers are interested in the relationship between IQ measurement and alcohol consumption. To this end they design a study wherein they visit a nearby pub over several days and measure individuals' IQ and the amount of alcohol they've consumed. 

```{r}
#| include: false
n_part <- 5
n_obs <- 10
mu_00 <- 100
mu_01 <- 5 #between-person effect
mu_10 <- -2 #withinperson effect

nonindependence_df <- tibble(
  participant = c("Anne", "John", "Laura", "Mary", "Steve"),
  intake_mean = seq(from = 1, to = 10, by = 2),
  u0 = rnorm(n_part, 0, 2)
) |>
  mutate(
    IQ_baseline = mu_00 + mu_01 * intake_mean + u0,
    β_within = mu_10
  )|>
  mutate(obs = list(1:n_obs)) |>
  unnest(obs) |>
  mutate(
    intake = intake_mean + rnorm(n_obs, 0, 2), 
    dev_intake = intake - intake_mean,
    IQ = IQ_baseline + β_within * dev_intake + rnorm(n(), 0, 3)
  )
```
Here is a plot of the data they collected:

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()
```
:::
Fitting a simple linear model of IQ on alcohol intake would suggest that higher alcohol intake is associated with higher IQ scores. Indeed, we can visualise a simple linear fit with the `geom_smooth(method="lm")` function.

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
:::
which shows this positive trend. In some sense this is what the data show; however, it is not the whole story. Consider the following plot, with each observation coloured by participant:

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()
```
:::

We can see that multiple observations (points) originate from the same participant and therefore are likely to be associated with one another. Moreover, if we focus on linear trends within each individual, our model tells a different story:

::: {.panel-tabset}
## Plot
```{r}
#| echo: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
## Code

```{r}
#| eval: false
nonindependence_df |>
  ggplot() +
  aes(x = IQ, y = intake, colour = participant) +
  geom_point()+
  geom_smooth(method="lm", se=FALSE)
```
:::
:::

## Constant Variance
### What it means
Also called homoscedasticity, the variability of residuals should stay roughly constant across fitted values.

### Diagnostics
Residual-versus-fitted plots should show an even vertical spread without a trend in spread. We can also look at the 

### Warning signs
Funnel or fan shapes (narrow for small $\hat y$ but wide for large $\hat y$) reveal heteroscedasticity.

### Possible remedies
Transform the response, use weighted least squares, or model the variance explicitly if certain groups have larger spread.


::: Example
**Constant variance**

When variability grows with the fitted value, the residuals form a funnel shape. In this simulated data the spread of the errors increases with the predictor, so the residual-versus-fitted plot fans out instead of forming an even band.

```{r}
set.seed(2024)
hetero_example <- tibble(
  x = seq(0, 10, length.out = 80),
  y = 2 + 0.5 * x + rnorm(80, 0, 0.4 + 0.25 * x)
)

hetero_model <- lm(y ~ x, data = hetero_example)
hetero_diag <- hetero_example |>
  mutate(
    fitted = fitted(hetero_model),
    residuals = resid(hetero_model)
  )

ggplot(hetero_diag, aes(fitted, residuals)) +
  geom_hline(yintercept = 0, colour = "grey70") +
  geom_point() +
  geom_smooth(method = "loess", se = FALSE, colour = "#542788") +
  labs(x = "Fitted value", y = "Residual")
```
:::


## Normality of error
### What it means
Least squares inference (confidence intervals, hypothesis tests) assumes residuals are approximately normal.

### Diagnostics (slides 8-19 to 8-20)
Use a normal Q-Q plot to compare residual quantiles with the theoretical normal line; formal tests like `shapiro.test()` exist but have low power in larger samples.

### Warning signs
Systematic bends, S-shapes, or heavy tail departures from the diagonal indicate skewness or heavy tails.

### Possible remedies
Transform the response, refit with robust methods, or move to a different error distribution if normality is clearly untenable.


::: Example
**Normality**

Heavy-tailed errors lead to extreme residuals that stray from the reference line on a Q-Q plot. The simulated model below uses Student-$t$ noise with only two degrees of freedom, producing thicker tails than the normal distribution.

```{webr}
set.seed(2024)
heavy_tail_example <- tibble(
  hours = runif(100, 0, 8),
  satisfaction = 4 + 0.8 * hours + rt(100, df = 2)
)

heavy_tail_model <- lm(satisfaction ~ hours, data = heavy_tail_example)
heavy_tail_resid <- tibble(residuals = resid(heavy_tail_model))

ggplot(heavy_tail_resid, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(colour = "#1b9e77") +
  labs(x = "Theoretical quantiles", y = "Sample quantiles")
```
:::
