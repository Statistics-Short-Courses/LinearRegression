---
format: 
  live-html:
    toc: true

execute:
  echo: true
  warning: false
  message: false

embed-resources: true

filters:
  - custom-numbered-blocks

custom-numbered-blocks:
  classes:
    Assumption:
      colors: [FFCDD2, F44336]
      boxstyle: foldbox.simple
      collapse: false
    Example:
      colors: [BBDEFB, 2196F3]
      boxstyle: foldbox.simple
      collapse: false
    Exercise:
      colors: [C8E6C9, 4CAF50]
      boxstyle: foldbox.simple
      collapse: false
    Note:
      colors: [FFF9C4, FFEB3B]
      boxstyle: foldbox.simple
      collapse: false
webr:
  render-df: gt-interactive
---
{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}
{{< include ./_extensions/r-wasm/live/_gradethis.qmd >}}

```{webr module01-slr-setup}
#| include: false
library(tidyverse)
theme_set(theme_bw())
```

```{r module01-slr-setup-2}
#| include: false
library(tidyverse)
theme_set(theme_bw())
```

```{r module01-slr-setup-simulate}
#| echo: false
#| define:
#|  - linearData
n <- 20
x <- rnorm(n,0,4)
y <- 0.5 * x - 3 + rnorm(n,0, 2)

linearData <- data.frame(x,y)
ojs_define(linearData)
```
# Simple Linear Regression (SLR) {#sec-slr}

## When to use SLR

## Fitting models to data {#sec-LinearFit}
in the previous section, you were given a linear model - we knew the values of $\alpha$, $\beta$ and even $\sigma$. However, in most contexts we don't know the true relationship between $x$ and $Y$ in advance. Instead, we are given *data* and try to *infer* the values of $\alpha$, $\beta$, and $\sigma$ by analysing the data.

Indeed, at the end of the last session we generated data from a linear model. In preparation for this chapter, we've generated data from a similar such linear model. Here it is:

::: {.panel-tabset}
### Plot
```{r module01-slr-fitting-models-to-data-plot}
#| echo: false
#| input:
#|   - linearData
ggplot(linearData)+
  aes(x=x, y=y)+
  geom_point()
```

### Data
```{r module01-slr-fitting-models-to-data}
#| echo: false
#| input:
#|   - linearData
linearData
```
:::

 your task for the remainder of the chapter (and in regression generally) is to try an make a (educated) guess about what model produced this data. 

- [ ] Continue

### Estimating parameters

We call the process of trying to guess the parameters in the data generating model (i.e. $\alpha$, $\beta$ and $\sigma$), *estimation*, and our guesses are *estimates*. To avoid confusion, we'll denote the estimated intercept by $a$ and the estimated slope by $b$. So, we have

$$
a: \text{Estimated intercept}
$$
$$
b: \text{Estimated slope}
$$
$$
\hat{\sigma}: \text{Estimated standard deviation}.
$$
We also define $\hat{Y}$ as the predicted value of $Y$ given our estimated model:
$$
\hat{Y}=a+bx
$$
Looking at some data, we try to find the values of our parameters that best 'fit' its distribution. Adjust the sliders below to find a line that you think fits the data:

```{ojs module01-slr-estimating-parameters-view}
//| panel: sidebar
//| echo: false
viewof b0adj_1 = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept (asjust)`})
viewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})

b0_1 = (-3 - 0.5*b1_1) + b0adj_1

tex.block`\hat{Y} = ${b0_1.toFixed(2)} + ${b1_1.toFixed(2)}x`
```

```{ojs module01-slr-estimating-parameters-ojs}
//| panel: fill
//| echo: false

xRange = d3.extent(linearData.x)

lineData_1 = xRange.map(x => ({x, y: b1_1 * x + b0_1}))

Plot.plot({
  marks: [
    Plot.line(lineData_1, { x: "x", y: "y" }),

    Plot.dot(transpose(linearData), { x: "x", y: "y", r: 3 })
  ],
  x: { domain: xRange },
  y: { domain: [-10, 5], label: "y" },
})
```

::: Exercise
What is your best estimate for the values of $\alpha$ and $\beta$ that best fit the given data?

```{webr module01-slr-estimating-parameters}
#| exercise: ex_2.1
#| envir: Ex2
#| define:
#|   - my_a
#|   - my_b
my_a <- ______
my_b <- ______
```
:::: {.solution exercise="ex_1"}
```{webr module01-slr-estimating-parameters-2}
#| exercise: ex_2.1
#| solution: true
```
::::
```{webr module01-slr-estimating-parameters-3}
#| exercise: ex_2.1
#| envir: Ex2
#| check: true
#| class: wait
grade_this({
  if (is.numeric(get("my_a", envir = .envir_result)) && is.numeric(get("my_b", envir = .envir_result))){pass("Good guess!")}
  fail("both $a$ and $b$ should be numbers")
})
```
:::

###

Ok great, so we have an estimate for our line-of-best-fit:
```{ojs module01-slr-estimating-parameters-ojs-2}
//| echo: false
tex.block`\hat{Y} = ${my_a} + ${my_b}\cdot x`
```
but what do we mean by 'best' fit here? How do we evaluate this estimate and maybe compare it to other estimates?

-   [ ] Continue

### Residuals {#sec-residuals}

[Residuals]{.glossary term="Residual"} are the difference between observed values of $Y$, and the value predicted by our estimated linear predictor $\hat{Y}$. We denote residuals with the letter $e$:
$$
e=Y-\hat{Y} = Y - (a + bx). 
$$

Residuals, $e$, are similar to the errors, $\varepsilon$, that we encountered in chapter 1 - but they are distinct. In many situations, we do not know the 'acutual' values of $\alpha$ and $\beta$ - so we cannot calculate the errors, $\varepsilon= Y- E[Y]= Y-\alpha + \beta x$. However, we do  have our estimates, $a$ and $b$, so we can calculate residuals. 

Indeed - calculating residuals gives us a way to assess how well our estimated model fits the data.  We can think of the residuals as being the 'mismatch' between our estimated linear predictor and each data point. By *minimizing* the size of residuals (minimisiong the mismatch of our model to the data), we can get a better fit of our line to data.

- [ ] Continue 

### Estimating coefficients 

#### Optimising model fit by minimising (squared) residuals
Below, the plot now also displays residuals for each data point.
Underneath the model equation is the *[Sum of squared residuals]{.glossary}* (SSR), which gives a measure of the absolute difference between our linear predictor and the observed outcomes. 

$$
SSR=\sum_{i=1}^{n} e_i^2
$$

where $e_i = Y_i - \hat{Y}_i$.  This gives us a numerical measure of how well the line fits the data - see how low you can get the SSR by adjusting slope and intercept.


```{ojs module01-slr-optimising-model-fit-by-minimising-squared-resid-view}
//| panel: sidebar
//| echo: false
viewof b0adj = Inputs.range([-5, 5], {step: 1, label: html`${tex`a`}: Estimated Intercept`})
viewof b1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`b`}: Estimated Slope`})

b0 = -3 + b0adj

residualData = transpose(linearData).map(d => {
  const yhat = b1 * d.x + b0;
  const res  = d.y - yhat;
  return {
    ...d,
    yhat,
    residual: res,
    sign: res >= 0 ? "positive" : "negative",
    absRes: Math.abs(res)
  };
})
  
SSRes = d3.sum(residualData, d => d.residual ** 2)

tex.block`\hat{Y} = ${b0} + ${b1}x`

tex.block`\sum_{i} e_i^{2} = ${SSRes.toFixed(2)}`
```

```{ojs module01-slr-optimising-model-fit-by-minimising-squared-resid-ojs}
//| panel: fill
//| echo: false
lineData = xRange.map(x => ({x, y: b1 * x + b0}))

Plot.plot({
  marks: [
    Plot.ruleX(residualData, {x: "x",
      y1: "y",
      y2: "yhat",
      stroke: "sign", 
      strokeOpacity: 0.75,
      strokeWidth: d => 1 + d.absRes}),

    Plot.line(lineData, { x: "x", y: "y" }),

    Plot.dot(transpose(linearData), { x: "x", y: "y", r: 3 })
  ],
  x: { domain: xRange },
  y: { domain: [-10,5], label: "y" },
})
```

::: Exercise
Based on this visualisation, what do you think is the minimum possible $SSR$ achievable?
```{webr module01-slr-optimising-model-fit-by-minimising-squared-resid}
#| exercise: Ex_2.2
#| envir: Ex2
#| define:
#|   - SSR
SSR <- ________
```
:::: {.solution exercise="Ex_2.2"}
```{webr module01-slr-optimising-model-fit-by-minimising-squared-resid-2}
#| exercise: Ex_2.2
#| solution: true
```
::::
```{webr module01-slr-optimising-model-fit-by-minimising-squared-resid-3}
#| exercise: Ex_2.2
#| check: true
#| class: wait
grade_this({
  if (is.numeric(get("SSR", envir = .envir_result))){
    pass("Good guess!")}
  fail("SSR should be a number!")
})
```
If you want you can update your $a$ and $b$ estimates too (otherwise just proceed):

```{webr module01-slr-optimising-model-fit-by-minimising-squared-resid-4}
#| exercise: ex_2.3
#| envir: Ex2
my_a <- ______

my_b <- ______
```

:::

### The least-squares estimate {#sec-leastsquares}

We call the estimates $a$ and $b$ which minimise the sum of squares the *[least squares estimates]{.glossary term="Ordinary least squares"}*. Some nice mathematics tells us that the least squares esimates for a simple linear model are *unique* - that is, there is one set of values for $a$ and $b$ which satisfy this property. Moreover, we don't have to manually adjust our parameters to keep lowering SSR - there is a convenient closed form expression which allows us to compute $a$ and $b$ very efficiently. As a statistical programming language, this is something R does very easily..


### Fitting a model with the `lm()` function

In R the `lm()` function computes the least squares estimates $a$ and $b$ for our simple linear model (among other things) in a single command: 

```{webr module01-slr-fitting-a-model-with-the-lm-function-fit-lm}
#| input:
#|   - linearData
lm_1 <- lm(y~x,linearData)
```
We can  extract the coefficients from the `lm` by indexing:

```{webr module01-slr-fitting-a-model-with-the-lm-function}
lm_1$coefficients
```
or by the `coef()` function:

```{webr module01-slr-fitting-a-model-with-the-lm-function-2}
coef(lm_1)
```
-  [ ] How do these estimates compare with yours? 

Lets compare the estimated linear fits graphically:

::: {.panel-tabset}
#### Plot
```{webr module01-slr-fitting-a-model-with-the-lm-function-plot}
#| exercise: comparePlot
#| envir: Ex2
#| echo: false
#| edit: false
ggplot(linearData)+
  aes(x=x,y=y)+
  geom_point()+
  geom_abline( # the least squares estimate
    aes(intercept = lm_1$coefficients[1],
    slope = lm_1$coefficients[2],
    colour = "Least squares fit")) +
  geom_abline( # Your guess
    aes(intercept = my_a,
    slope = my_b,
    colour = "Your guess"))+
  scale_colour_manual(values= c("Least squares fit"="#4CAF50","Your guess"='#2196F3'))
```
#### Code
```{webr module01-slr-fitting-a-model-with-the-lm-function-code}
#| eval: false
ggplot(linearData)+
  aes(x=x,y=y)+
  geom_point()+
  geom_abline( # the least squares estimate
    aes(intercept = lm_1$coefficients[1],
    slope = lm_1$coefficients[2],
    colour = "Least squares fit")) +
  geom_abline( # Your guess
    aes(intercept = my_a,
    slope = my_b,
    colour = "Your guess"))+
  scale_colour_manual(values= c("Least squares fit"="#4CAF50","Your guess"='#2196F3'))
```
:::
-   [ ] Continue


### Estimating variance

Once the line has been fitted (i.e. $\alpha$ and $\beta$ have been estimated as $a$ and $b$), we also have to estimate the distribution of error terms (remember, as per @sec-varAssumption, the error distribution has a constant variance defined by $\sigma^2$). As you might expect, we again utilise the residuals from our least squares linear predictor to estimate $\sigma$.  The spread of the error terms will be estimated by the spread of the residuals around our line of best fit.

We can extract the individual residual values from the fitted `lm` object by indexing (e.g. `my_lm$residuals`) or with the `residuals()` function.

```{webr module01-slr-estimating-variance}
residuals(lm_1)
```

The resulting `R` object is a vector of the residual for each observation. i.e. residuals(lm_1)[[3]] $= e_3$

::: Exercise
#### Extracting residuals from an `lm` object
Extract the residuals from  your least squares fitted model `lm_1`*by indexing* and assign them to the variable `e`

```{webr module01-slr-extracting-residuals-from-an-lm-object}
#| exercise: ex_2.2.1
#| envir: ex_2.2
e <- ______
```
:::: {.solution exercise="ex_2.2.1"}
#### Solution 
```{webr module01-slr-extracting-residuals-from-an-lm-object-solution}
#| exercise: ex_2.2.1
#| solution: true
#| envir: ex_2.2
e <- lm_1$residuals
```
::::

```{webr module01-slr-extracting-residuals-from-an-lm-object-solution-2}
#| exercise: ex_2.2.1
#| envir: ex_2.2
#| check: true
#| class: wait
grade_this_code()
```
:::

### Residual Standard Error


Dividing the residual sum of squares by $n-2$ (one [degree of freedom]{.glossary term="Degrees of freedom"} lost per fitted coefficient) gives the [mean squared error]{.glossary}, and taking the square root yields the residual standard deviation (aka. the [Residual standard error]{.glossary})
$$
s^2 = \frac{1}{n-2}\sum_{i=1}^{n} e_i^2= \frac{SSE}{n-2}, \quad s=\sqrt{s^2}.
$$
Interpreting $\hat \sigma$ is often easier on the original $y$ scale: a typical observation falls about $\hat \sigma$ units away from the fitted line.

$s^2$ and $s$ are our estimators for $\sigma^2$ and $\sigma$, respectively. 

::: Exercise
#### Calculate the Residual Standard Error
using your `e` object defined in the previous exercise, calculate the residual standard error of the least squares model (hint - there are 20 observations in the linearData dataset)
```{webr module01-slr-calculate-the-residual-standard-error}
#| exercise: ex_2.2.3
#| envir: ex_2.2
rse <- ______
```
:::: {.solution exercise="ex_2.2.1"}
#### Solution 
```{webr module01-slr-calculate-the-residual-standard-error-solution}
#| exercise: ex_2.2.3
#| solution: true
#| envir: ex_2.2
rse <- sqrt(sum(e^2)/(nrow(linearData)-2))
```
::::

```{webr module01-slr-calculate-the-residual-standard-error-solution-2}
#| exercise: ex_2.2.1
#| envir: ex_2.2.1
#| check: true
#| class: wait
grade_this_code()
```
:::

### Correlation and $R^2$

The [Pearson correlation]{.glossary} coefficient, $r$, measures how strongly $x$ and $Y$ move together along a straight line, taking values between $-1$ (perfect negative linear relationship) and $1$ (perfect positive linear relationship). 

In simple linear regression with an intercept, the [R-squared]{.glossary} value can be written as
$$
R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = r^2,
$$
so $R^2$ captures the proportion of the total variation in $Y$ that is explained by the fitted line.

::: Exercise
#### Correlation and $R^2$ in our example
Calculate the correlation between `x` and `y`, then compute $R^2$ using the residuals from `lm_1`.
```{webr module01-slr-correlation-and-r-2-in-our-example}
#| exercise: ex_2.2.4
#| envir: ex_2.2
r <- ______
r_squared <- ______
```
:::: {.solution exercise="ex_2.2.4"}
#### Solution 
```{webr module01-slr-correlation-and-r-2-in-our-example-2}
#| exercise: ex_2.2.4
#| solution: true
#| envir: ex_2.2
r <- cor(linearData$x, linearData$y)
r_squared <- 1 - sum(e^2) / sum((linearData$y - mean(linearData$y))^2)
```
::::

```{webr module01-slr-correlation-and-r-2-in-our-example-solution}
#| exercise: ex_2.2.4
#| envir: ex_2.2
#| check: true
#| class: wait
grade_this_code()
```

```{webr module01-slr-correlation-and-r-2-in-our-example-summary}
#| echo: false
#| envir: ex_2.2
c(computed_r = r, computed_r_squared = r_squared, lm_summary_r_squared = summary(lm_1)$r.squared)
```
:::
## Inference for SLR


Up to this point, we have fitted a straight-line model to a *sample* of data, obtaining estimates $a$ and $b$ for the intercept and slope. These estimates describe the pattern visible in the sample, but they do not tell us how closely they reflect the **true population parameters** $\alpha$ and $\beta$. Because sampling introduces randomness, different samples would produce different fitted lines.

This raises the central question:

**How much confidence can we place in the slope and intercept we estimated, and what do they tell us about the true relationship in the population?**

To answer this, we rely on **statistical inference**, which quantifies the uncertainty in the estimates and evaluates whether the predictor genuinely influences the response.

Importantly, the inferential procedures we use rest on the **assumptions of the linear regression model**:

* The errors $\varepsilon$ have mean $0$.
* They have constant variance $\sigma^2$ (homoscedasticity).
* They are independent.
* **They are Normally distributed.**

The Normality assumption is what allows us to derive the sampling distributions of $a$ and $b$, leading directly to the **[t-tests]{.glossary term="t-test"}** and **[confidence intervals]{.glossary term="Confidence interval"}** used for inference. Without these assumptions—especially Normality—the exact forms of these inferential tools would not hold.

::: Note
For a broader introduction to statistical inference, see the *Inferential Statistics with R* short course. If concepts such as the Normal distribution or t-tests feel unfamiliar, please review that material before continuing.
:::

---

### Inference About the Slope, $\beta$ {#sec-beta_inference}

In simple linear regression, the population relationship is modelled as

$$
Y = \alpha + \beta x + \varepsilon.
$$

To determine whether $x$ is genuinely associated with $Y$, we test:

$$
H_0: \beta = 0
\qquad \text{vs.} \qquad
H_a: \beta \ne 0.
$$

* Under $H_0$, changes in $x$ do not affect the mean of $Y$ (a change in $x$ will lead to $\beta\cdot x = 0\cdot x = 0$ change in $Y$).
* Under $H_a$, there is evidence of a real linear effect (i.e. a change in $x$ will lead to a non-zero change in $Y$).

Because the Normality assumption implies that the estimator $b$ has a Normal sampling distribution (and hence a $t$ distribution once $\sigma$ is estimated), we are able to quantify how unusual our observed slope would be if $H_0$ were correct.

::: Exercise

#### Hypothesis testing revision ([p-values]{.glossary term="p-value"})

:::

#### The t-Test for the Slope

The hypothesis test is carried out using the statistic

$$
t = \frac{b}{\text{SE}(b)},
$$

which follows a $t$-distribution with $n - 2$ [degrees of freedom]{.glossary term="Degrees of freedom"}.

Interpretation:

* A **large** value of $|t|$ (small p-value) indicates evidence that $\beta \ne 0$.
* A **small** value of $|t|$ suggests the data are consistent with no linear effect.

The validity of this test relies on the Normality of the errors, which guarantees that this $t$ statistic follows the appropriate reference distribution.

::: Note
While the assumption of Normality is what theoretically grounds the use of the t-test for parameter estimates, SLR is quite robust to violations of this assumption and inferences about our slope parameters may be of interest even with non-normal residual distributions.
:::

::: Example

##### t-test for slope

:::

::: Exercise

##### t-test for slope

:::

#### Confidence Interval for the Slope

A $(1-\alpha)100%$ confidence interval for $\beta$ is

$$
b ;\pm; t_{\alpha/2,,n-2},\text{SE}(b).
$$

Interpretation:

* An interval **excluding zero** indicates a likely genuine relationship.
* An interval **including zero** suggests weaker evidence.

Confidence intervals complement hypothesis tests by communicating both the direction and plausible magnitude of the effect.


### Inference About the Response, $Y$

Once we have fitted a regression model, we often want to make statements about the **value of the response** at a given predictor value $x_0$. There are two distinct quantities of interest:

1. **The mean (average) response** at $x_0$:
   $$
   \mu_Y(x_0) = \alpha + \beta x_0.
   $$

2. **A new individual response** at $x_0$:
   $$
   Y_{\text{new}} = \alpha + \beta x_0 + \varepsilon.
   $$

These involve different uncertainties, and therefore require different intervals.

#### [Confidence interval]{.glossary term="Confidence interval"} for the Mean Response

Let $\hat{y}_0 = a + b x_0$ be the fitted value at $x_0$.
A confidence interval for the mean response is:

$$
\hat{y}*0
;\pm;
t*{\alpha/2,,n-2} ,
s\sqrt{
\frac{1}{n} +
\frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}
}.
$$

This interval quantifies uncertainty in the *average* value of $Y$ for units with predictor value $x_0$.

```r
predict(fit, newdata = new_point, interval = "confidence")
```

#### [Prediction interval]{.glossary term="Prediction interval"} for a New Observation

To predict an individual outcome at $x_0$, we must include the additional uncertainty from the random error $\varepsilon$:

$$
\hat{y}*0
;\pm;
t*{\alpha/2, , n-2} ,
s\sqrt{
1 +
\frac{1}{n} +
\frac{(x_0 - \bar{x})^2}{\sum (x_i - \bar{x})^2}
}.
$$

Because of the extra "1" term, prediction intervals are **always wider** than confidence intervals.

```r
predict(fit, newdata = new_point, interval = "prediction")
```

#### Summary

* Confidence interval → uncertainty in the **expected** value at $x_0$
* Prediction interval → uncertainty in a **new outcome** at $x_0$

---

-  [ ] Continue

Should I include a (brief) section on residuals diagnostics here or save that for the dedicated chapter??
