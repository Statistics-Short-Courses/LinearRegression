---
format:
  live-html:
    toc: true

execute:
  echo: false
  warning: false
  message: false

embed-resources: true

filters:
  - custom-numbered-blocks

custom-numbered-blocks:
  classes:
    Assumption:
      colors: [FFCDD2, F44336]
      boxstyle: foldbox.simple
      collapse: false
      numbered: false
    Example:
      colors: [BBDEFB, 2196F3]
      boxstyle: foldbox.simple
      collapse: false
    Exercise:
      colors: [C8E6C9, 4CAF50]
      boxstyle: foldbox.simple
      collapse: false
    Note:
      colors: [FFF9C4, FFEB3B]
      boxstyle: foldbox.simple
      collapse: false
      numbered: false
    Key-term:
      colors: [D1C4E9, 673AB7]
      boxstyle: foldbox.simple
      collapse: false
      numbered: false
    Key-point:
      colors: [FFF9C4, FFEB3B]
      boxstyle: foldbox.simple
      collapse: false
      numbered: false

webr:
  render-df: gt-interactive

resources:
  Data
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}}
{{< include ./_extensions/r-wasm/live/_gradethis.qmd >}}

```{webr}
#| include: false
library(tidyverse)
library(plotly)
theme_set(theme_bw())
```

```{r}
#| include: false
library(tidyverse)
theme_set(theme_bw())
```


# Model Building and Selection

In this module, we explore strategies for building effective regression models. We cover:
- Increasing model complexity to capture non-linear relationships and interactions,
- Assessing model fit while balancing complexity,
- Techniques for selecting parsimonious models.

## Increasing model complexity to model complex relationships

### Why increase model complexity?

We began this course with the simple linear regression model: a single continuous predictor with a straight-line effect. We then expanded to multiple predictors, where each predictor still had a straight-line effect. For reasons that will become clear, these are examples of so-called **first-order models**:

::: Key-term
#### First-order model
A linear regression where each predictor appears only once and to the first power (no squared terms or interactions). For one predictor:
$$
E[Y] = \beta_0 + \beta_1 X.
$$
With multiple predictors, $X_1, X_2, \dots, X_i$, this extends to 
$$E[Y] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_i X_i$$
:::

This first-order structure is the simplest linear model we can make with $X$. It is often a reasonable starting point, and can go a long way to modeling real-world phenomena (especially in the multiple regression case), but real data are often more complex. Real relationships between predictors and outcomes are often non-linear, or involve interactions between predictors. To capture these more complex relationships, we can increase model complexity by adding 'higher-order' and 'interaction' terms.

::: Example
#### Non-linear relationships in data
For example, fitting a straight line to the following data is not ideal, as the relationship between $X$ and $Y$ is curved (U-shaped), so there is always part of the relationship the line cannot capture: 
```{r}
#| echo: false
#| define:
#|  - quadData
set.seed(12)
n <- 50
x <- runif(n,-4,4)
y <- 1+ x + x^2 + rnorm(n,0, 2)

quadData <- data.frame(x,y)
ojs_define(quadData)
```

```{ojs}
//| panel: sidebar
//| echo: false

quadPoints = transpose(quadData)
quadXDomain = d3.extent(quadData.x)

xRange = d3.extent(quadData.x)
yRange = d3.extent(quadData.y)

viewof b0adj_1 = Inputs.range([-1,16], {step: 1, label: html`${tex`\beta_0`}: Intercept `})
viewof b1_1 = Inputs.range([-2, 2], {step: 0.01, label:  html`${tex`\beta_1`}: Slope`})

b0_1 = b0adj_1

```

```{ojs}
//| panel: fill
//| echo: false

tex.block`\hat{Y} = ${b0_1.toFixed(0)} + ${b1_1.toFixed(2)}x`

lineData = xRange.map(x => ({x, y: b1_1 * x + b0_1}))
lineResiduals = quadPoints.map((d) => {
  const fit = b0_1 + b1_1 * d.x;
  return { x: d.x, y: d.y, fit, resid: d.y - fit };
})
lineMaxAbsResid = d3.max(lineResiduals, (d) => Math.abs(d.resid)) || 1;
Plot.plot({
  marks: [
    Plot.link(lineResiduals, {
      x1: "x",
      y1: "y",
      x2: "x",
      y2: "fit",
      stroke: (d) => d.resid >= 0 ? "positive" : "negative",
      strokeWidth: (d) => 1.5 + 3 * Math.abs(d.resid) / lineMaxAbsResid,
      strokeOpacity: 0.7,
      title: (d) => `Residual: ${d.resid.toFixed(2)}`
    }),
    Plot.line(lineData, { x: "x", y: "y" }),

    Plot.dot(quadPoints, { x: "x", y: "y", r: 3 })
  ],
  x: { domain: xRange },
  y: { domain: yRange, label: "y" },
})
```
:::

- [ ] Continue

### Square, Cubic, and Higher-Order Univariate Models

We enrich the basic linear model by adding **powers of a predictor**, allowing the fitted relationship to bend. In the above case, the "U" shape suggests a quadratic (squared) term may be appropriate.

::: Key-term
#### Second-order (Quadratic) univariate model
  $$E[Y] = \beta_0 + \beta_1X + \beta_2X^2$$
:::
::: Example
#### Seeing how $\beta_2$ bends the curve
Adjust the slider for the squared term to see how changing $\beta_2$ adds curvature to the fitted relationship . Try to find a good fit - can you guess what model generated this data?.

```{ojs}
//| panel: sidebar
//| echo: false

viewof b0_quad = Inputs.range([-4, 4], {step: 1, value: 0, label: html`${tex`\beta_0`}: Intercept`})
viewof b1_quad = Inputs.range([-3, 3], {step: 1, value: 0, label: html`${tex`\beta_1`}: Linear term`})
viewof b2_quad = Inputs.range([-1, 3], {step: 0.05, value: 0, label: html`${tex`\beta_2`}: Squared term`})
```

```{ojs}
//| panel: fill
//| echo: false

tex.block`\hat{Y} = ${b0_quad.toFixed(0)} + ${b1_quad.toFixed(0)}x + ${b2_quad.toFixed(2)}x^2`

quadCurve = d3.range(quadXDomain[0], quadXDomain[1] + 0.05, 0.05).map((x) => ({
  x,
  y: b0_quad + b1_quad * x + b2_quad * x * x
}))

quadResiduals = quadPoints.map((d) => {
  const fit = b0_quad + b1_quad * d.x + b2_quad * d.x * d.x;
  return { x: d.x, y: d.y, fit, resid: d.y - fit };
})
quadMaxAbsResid = d3.max(quadResiduals, (d) => Math.abs(d.resid)) || 1;

Plot.plot({
  height: 320,
  marginLeft: 50,
  x: {label: "Predictor (x)", domain: xRange},
  y: {label: "Outcome (y)", domain: yRange},
  marks: [
    Plot.link(quadResiduals, {
      x1: "x",
      y1: "y",
      x2: "x",
      y2: "fit",
      stroke: (d) => d.resid >= 0 ? "positive" : "negative",
      strokeWidth: (d) => 1.5 + 3 * Math.abs(d.resid) / quadMaxAbsResid,
      strokeOpacity: 0.7,
      title: (d) => `Residual: ${d.resid.toFixed(2)}`
    }),
    Plot.dot(quadPoints, {
      x: "x",
      y: "y",
      r: 3,
      title: (d) => `x = ${d.x.toFixed(2)}\ny = ${d.y.toFixed(2)}`,
    }),
    Plot.line(quadCurve, {
      x: "x",
      y: "y",
      strokeWidth: 2
    })
  ]
})
```
:::

#### Third-order (cubic) univariate model
$$
E[Y] = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3
$$

allows more complex curvature with two changes in direction (an "S"-shape) that a quadratic term cannot capture.

```{r}
#| echo: false
#| define:
#|  - cubicData
set.seed(21)
n <- 60
x <- runif(n, -3.5, 3.5)
y <- 2 - 1 * x + 0.5 * x^2 - 0.1 * x^3 + rnorm(n, 0, 1.4)

cubicData <- data.frame(x, y)
ojs_define(cubicData)
```

::: Example
#### Seeing how $\beta_3$ twists the curve
Adjust the slider for the cubic term to see how changing $\beta_3$ adds an inflection point to the fitted relationship. Can you tune the parameters to recover the curve that generated this data?

```{ojs}
//| panel: sidebar
//| echo: false

cubicPoints = transpose(cubicData)
cubicXDomain = d3.extent(cubicData.x)

viewof b0_cub = Inputs.range([0, 4], {step: 1, value: 2, label: html`${tex`\beta_0`}: Intercept`})
viewof b1_cub = Inputs.range([-2, 2], {step: 1, value: -1, label:  html`${tex`\beta_1`}: Linear term`})
viewof b2_cub = Inputs.range([-1, 1], {step: 0.1, value: 0.5, label: html`${tex`\beta_2`}: Squared term`})
viewof b3_cub = Inputs.range([-0.6, 0.6], {step: 0.1, value: 0.0, label: html`${tex`\beta_3`}: Cubic term`})
```

```{ojs}
//| panel: fill
//| echo: false

tex.block`\hat{Y} = ${b0_cub.toFixed(1)} + ${b1_cub.toFixed(2)}x + ${b2_cub.toFixed(2)}x^2 + ${b3_cub.toFixed(2)}x^3`

cubicCurve = d3.range(cubicXDomain[0], cubicXDomain[1] + 0.05, 0.05).map((x) => ({
  x,
  y: b0_cub + b1_cub * x + b2_cub * x * x + b3_cub * x * x * x
}))

cubicResiduals = cubicPoints.map((d) => {
  const fit = b0_cub + b1_cub * d.x + b2_cub * d.x * d.x + b3_cub * d.x * d.x * d.x;
  return { x: d.x, y: d.y, fit, resid: d.y - fit };
})
cubicMaxAbsResid = d3.max(cubicResiduals, (d) => Math.abs(d.resid)) || 1;
cubicYDomain = d3.extent(cubicData.y)

Plot.plot({
  height: 320,
  marginLeft: 50,
  x: {label: "Predictor (x)", domain: cubicXDomain},
  y: {label: "Outcome (y)", domain: cubicYDomain},
  marks: [
    Plot.link(cubicResiduals, {
      x1: "x",
      y1: "y",
      x2: "x",
      y2: "fit",
      stroke: (d) => d.resid >= 0 ? "positive" : "negative",
      strokeWidth: (d) => 1.5 + 3 * Math.abs(d.resid) / cubicMaxAbsResid,
      strokeOpacity: 0.7,
      title: (d) => `Residual: ${d.resid.toFixed(2)}`
    }),
    Plot.dot(cubicPoints, {
      x: "x",
      y: "y",
      r: 3,
      title: (d) => `x = ${d.x.toFixed(2)}\ny = ${d.y.toFixed(2)}`,
    }),
    Plot.line(cubicCurve, {
      x: "x",
      y: "y",
      strokeWidth: 2
    })
  ]
})
```
:::

#### N-th order univariate model 
We can extend this process of deriving new terms by adding further powers of $X$ - allowing ius to to fit arbitrarily complex curves:
$$
E[Y] = \beta_0 + \beta_1X +  \dots + \beta_nX^n.
$$

Note that each term gets its own parameter ($\beta_i$). If we have $n$ data points, a (n-1)^th-order model will have a parameter for each point, meaning it will fit the data perfectly!

::: Example
#### Fitting higher-order polynomial models
```{r}
#| echo: false
#| define:
#|  - linearData
set.seed(1)
n <- 10
x <- runif(n,0,4)
y <- sin(2.3*x) + rnorm(n,0, 0.8)

nonlinearData <- data.frame(x,y)
ojs_define(nonlinearData)
```

```{ojs}
//| echo: false

reg= require("d3-regression")
polyData=transpose(nonlinearData)
viewof degree = Inputs.range([1, 9], {step: 1, label: "Model Order", value: 1})

polyRegression = reg.regressionPoly()
  .x((d) => d.x)
  .y((d) => d.y)
  .order(degree)
  .domain(d3.extent(polyData, (d) => d.x));

polyCurveRaw = polyRegression(polyData)
polyCurve = polyCurveRaw.map(([x, y]) => ({ x, y }))
polyPredict = (x) => {
  if (typeof polyRegression.predict === "function") return polyRegression.predict(x);
  if (polyCurveRaw.coefficients) {
    return polyCurveRaw.coefficients.reduce((acc, coeff, i) => acc + coeff * x ** i, 0);
  }
  return NaN;
}
polyResiduals = polyData.map((d) => {
  const fit = polyPredict(d.x);
  return { x: d.x, y: d.y, fit, resid: d.y - fit };
})
polyMaxAbsResid = d3.max(polyResiduals, (d) => Math.abs(d.resid)) || 1;
```

```{ojs}
//| echo: false

polyTerms = Array.from({length: degree + 1}, (_, k) => {
  if (k === 0) return `\\beta_{0}`;
  if (k === 1) return `\\beta_{1} x`;
  return `\\beta_{${k}} x^{${k}}`;
}).join(" + ");

html`<div style="text-align:center; margin: 0.5rem 0;">${tex`E[Y] = ${polyTerms}`}</div>`
```

```{ojs}
polyYDomain = d3.extent([
  ...polyData.map((d) => d.y),
  ...polyCurve.map((d) => d.y),
  ...polyResiduals.map((d) => d.fit)
])

Plot.plot({
  marginLeft: 50,
  height: 320,
  x: {label: "Predictor (x)"},
  y: {label: "Outcome (y)", domain: polyYDomain},
  marks: [
    Plot.link(polyResiduals, {
      x1: "x",
      y1: "y",
      x2: "x",
      y2: "fit",
      stroke: (d) => d.resid >= 0 ? "positive" : "negative",
      strokeWidth: (d) => 1.5 + 3 * Math.abs(d.resid) / polyMaxAbsResid,
      strokeOpacity: 0.7,
      title: (d) => `Residual: ${d.resid.toFixed(2)}`
    }),
    Plot.dot(polyData, {
      x: "x",
      y: "y",
      r: 4,
    }), 
    Plot.line(polyCurve, {
      x: "x",
      y: "y",
      strokeWidth: 2,
    }),
  ],
});
```
:::

Higher-order terms increase the model's flexibility, but also increase the risk of fitting random noise rather than meaningful structure. We will return to this important issue later in the module. 

::: Note
#### The meaning of "linear" in linear models
Even though higher-order models can model nonlinear relationships between the outcome and predictors, they are still **linear models** because each *parameter* enters additively. “Linearity” here refers to the parameters, *not* the shape of the fitted curve.
:::

#### Fitting higher-order univariate models in R
In R, higher-order polynomial terms can be included using the `I()` function to indicate 'as is' operations. For example, to fit a quadratic model:
```{r}
#| echo: true
lm(y ~ x + I(x^2), data = nonlinearData)
```
or using the `poly()` function:
```{r}
#| echo: true
lm(y ~ poly(x, 2), data = nonlinearData)
```


- [ ] Continue

### Interaction Models with Continuous Predictors

In @sec-slr we saw a multiple regression model with two continuous predictors: 

$$
E[Y] = \beta_0 + \beta_1X_1 + \beta_2X_2.
$$

This again is a first-order model - each predictor appears only once, 'as is'.  Of course, we can add higher-order terms for each predictor separately (e.g., $X_1^2$, $X_2^3$) to capture curvature in their individual effects as we did above. e.g. A second-order multivariate model with two predictors might look like:
$$
E[Y] = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1^2 + \beta_4X_2^2.
$$

Now however, we can also consider a different form of higher order term: what happens when we multiply predictors together? This gives us an **interaction term** - a term that combines two (or more) predictors.

::: Key-term
#### Interaction term
An interaction is a product of predictors that allows the effect of one predictor to depend on the level of another. For two continuous predictors, the second-order interaction model is:
$$
E[Y] = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2.
$$
If $\beta_3 = 0$ (i.e. as in the first-order model), predictors act independently; if $\beta_3 \neq 0$, the slope for $X_1$ varies with $X_2$ and vice versa.
:::

::: Example
#### Fitting an interaction model to the `advertising` dataset.
If we recall our plot of the `advertising` dataset from @sec-mlr, our fist-order linear model did not fit the data in particular ways:

```{r}
#| echo: true
#| edit: false
ads <- read.csv("Data/Advertising.csv")
ads_mod <- lm(Sales ~ TV + Radio, data = ads)
```

```{r}
#| edit: false
#| echo: false
library(plotly)

# Create grid for the plane
tv_seq <- seq(min(ads$TV), max(ads$TV), length.out = 4)
radio_seq <- seq(min(ads$Radio), max(ads$Radio), length.out = 4)
grid   <- expand.grid(TV = tv_seq, Radio = radio_seq)
grid$sales_hat <- predict(ads_mod, newdata = grid)

z_mat <- matrix(grid$sales_hat, nrow = length(tv_seq), ncol = length(radio_seq))

# Plotly 3D scatter + plane
fig <- plot_ly() |>
  add_markers(
    data = ads,
    x = ~Radio, y = ~TV, z = ~Sales,
    type = "scatter3d",
    mode = "markers") |>
  add_surface(
    x = radio_seq,
    y = tv_seq,
    z = z_mat,
    opacity = 0.6,
    showscale = FALSE,
    name = "LS plane"
  ) |>
  layout(
    scene = list(
      xaxis = list(title = "Radio"),
      yaxis = list(title = "TV"),
      zaxis = list(title = "Sales")
    ),
    showlegend = FALSE
  ) 

fig
```
The plane does not fit the data at the edges - it overestimates sales when either Radio or TV advertising is low (separately), but underestimates sales when both are high. This suggests that *the effect of increasing one type of advertising depends on the level of the other type* - an interaction effect. 

We can fit a second-order interaction model to capture this:
$$
E[Sales] = \beta_0 + \beta_1TV + \beta_2Radio + \beta_3(TV \times Radio).
$$

```{r}
#| echo: false
#| edit: false
ads_mod_int <- lm(Sales ~ TV + Radio+ TV:Radio, data = ads)
```
The fitted interaction model has a charateristic 'saddle' shape, which can better capture the data structure:
```{r}
#| edit: false
#| echo: false

# Fit least squares plane

# Create grid for the plane
tv_seq <- seq(min(ads$TV), max(ads$TV), length.out = 4)
radio_seq <- seq(min(ads$Radio), max(ads$Radio), length.out = 4)
grid   <- expand.grid(TV = tv_seq, Radio = radio_seq)
grid$sales_hat <- predict(ads_mod_int, newdata = grid)

z_mat <- matrix(grid$sales_hat, nrow = length(tv_seq), ncol = length(radio_seq))

# Plotly 3D scatter + plane
fig <- plot_ly() |>
  add_markers(
    data = ads,
    x = ~Radio, y = ~TV, z = ~Sales,
    type = "scatter3d",
    mode = "markers") |>
  add_surface(
    x = radio_seq,
    y = tv_seq,
    z = z_mat,
    opacity = 0.6,
    showscale = FALSE,
    name = "LS plane"
  ) |>
  layout(
    scene = list(
      xaxis = list(title = "Radio"),
      yaxis = list(title = "TV"),
      zaxis = list(title = "Sales")
    ),
    showlegend = FALSE
  ) 

fig
```

The interaction effect can also be visualised by fixing one predictor and plotting the relationship between the other predictor and the outcome. In this case, we can plot  lines representing the relationship between TV advertising and Sales  when Radio advertising is low (\$0 spent), average (i.e. `mean(ads$Radio)`= `r mean(ads$Radio)` thousand dollars ), and high (i.e. `max(ads$Radio)`= \$`r max(ads$Radio)` thousand dollars):


```{r}
#| echo: false
library(ggplot2)
library(scales)

radio_colors <- col_numeric(
palette = "viridis",
domain = range(ads$Radio)
)

ads |>
ggplot(aes(x = TV, y = Sales, colour = Radio)) +
geom_point() +
scale_colour_viridis_c() +
geom_abline(
intercept = coef(ads_mod_int)[1],
slope = coef(ads_mod_int)[2],
colour = radio_colors(min(ads$Radio)), linewidth = 1
) +
geom_abline(
intercept = coef(ads_mod_int)[1] + coef(ads_mod_int)[3] * mean(ads$Radio),
slope = coef(ads_mod_int)[2] + coef(ads_mod_int)[4] * mean(ads$Radio),
colour = radio_colors(mean(ads$Radio)), linewidth = 1
) +
geom_abline(
intercept = coef(ads_mod_int)[1] + coef(ads_mod_int)[3] * max(ads$Radio),
slope = coef(ads_mod_int)[2] + coef(ads_mod_int)[4] * max(ads$Radio),
colour = radio_colors(max(ads$Radio)), linewidth = 1
)
```

We can see that the relationship between TV advertising and Sales (represented by the slope of the regression lines) is different at different levels of Radio advertising. In particular, as radio advertising increases, the slope of the line increases, indicating that TV advertising has a larger effect on Sales when Radio advertising is also high.  
:::

- [ ] Continue

When our first order model has more than two predictors, we can include interaction terms between any pair (or more) of predictors. For example, with three predictors $X_1$, $X_2$, and $X_3$, a second-order interaction model would include interactions between each pair:
$$
E[Y] = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_1X_2 + \beta_5X_1X_3 + \beta_6X_2X_3.
$$


#### Higher-order interaction models
We can also consider higher-order interactions - those that include combinations of three or more predictors. For example, a third-order interaction model with three predictors would include the 'three-way' interaction term:
$$
E[Y] = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_3 + \beta_4X_1X_2 + \beta_5X_1X_3 + \beta_6X_2X_3 + \beta_7X_1X_2X_3.
$$
The $\beta_7$ parameter here captures how the interaction between two predictors changes depending on the level of the third predictor.

As in the case of higher-order univariate models, we can extend this idea to include both higher-order interaction terms and higher-order univariate terms for each predictor, allowing for very flexible modeling of complex relationships.

However, as before, increasing model complexity in this way raises the risk of overfitting and interpretability challenges, which we will discuss later in this module.

---

- [ ] Continue


#### Fitting interaction models in R
In R, interaction terms can be included using the `*` operator in the formula. For example, to fit a second-order interaction model with two predictors:
```{r}
#| echo: true
#| eval: false
lm(Y ~ X1*X2*X3, data = mydata)
```
This expands to include both main effects and the interaction term. To include only the interaction term without main effects, use the `:` operator. For example, if we only wanted the interaction between `X1` and `X2`, along with the linear terms for `X1`, `X2`, and `X3`:

```{r}
#| echo: true
#| eval: false
lm(Y ~ X1+X2+X3+X1:X2, data = mydata)
```


- [ ] Continue

### Interaction Models with Categorical Predictors

A categorical variable with $k$ levels is represented by $k-1$ dummy variables:
$$
E[Y] = \beta_0 + \beta_1X + \gamma_1D_1 + \dots + \gamma_{k-1}D_{k-1}.
$$

* Each $\gamma_j$ measures the difference between level $j$ and the baseline.
* Changing the baseline changes interpretations but not fitted values.

Categorical predictors may also interact with continuous predictors:
$$
E[Y] = \beta_0 + \beta_1X + \gamma_1D_1 + \delta_1(XD_1),
$$
allowing each group to have its own slope.

::: Note
We saw in @sec-mlr_cat that first order models with categorical predictors are equivalent to the familiar t-tests and ANOVA models from earlier statistics courses. Interaction models with both continuous and categorical predictors are similarly analogous to "ANCOVA" models. 
:::

- [ ] Continue

## Assessing model fit and model complexity

### Why not increase model complexity?

In the previous section we saw how adding higher-order terms and interactions can help us capture complex relationships in data. However, increasing model complexity is not always beneficial. While more complex models can fit the *training data* (i.e. the data we use to fit our model) better, they may not generalise well to new data. This is because complex models can start to fit random noise in the training data, rather than the underlying signal. This phenomenon is known as **overfitting**.

In addition, more complex models can be harder to interpret, making it difficult to understand the relationships between predictors and the outcome. They may also be more sensitive to outliers and multicollinearity, leading to unstable parameter estimates.

A good model is **no more complex than necessary** to describe the main structure of the data. Therefore, when building regression models, we need to balance the trade-off between model fit and model complexity. In this section, we discuss some common issues that arise with complex models, and metrics for assessing model fit while accounting for complexity.

::: Key-point
#### Balancing fit and complexity
A good regression model balances:
* Adequate fit to the data,
* Simplicity and interpretability,
* Generalisability to new data.
:::

- [ ] Continue

### Fit Metrics

Fit metrics quantify how well a model captures patterns in the data. We have already seen one such metric: the coefficient of determination, $R^2$. 

::: Key-term
#### Coefficient of Determination ($R^2$)
$$
R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
$$
where $SS_{res}$ is the residual sum of squares and $SS_{tot}$ is the total sum of squares. $R^2$ measures the proportion of variance in the outcome explained by the model, ranging from 0 (no explanatory power) to 1 (perfect fit).
:::

$R^2$ was introduced in @sec-slr as a measure of fit for simple linear regression models based soely on the proportion of variance in $Y$ explained by $X$. We can also calculate $R^2$ for multiple regression models, where it measures the proportion of variance in $Y$ explained by all predictors jointly. However, $R^2$ has a key limitation: it always increases (or at least does not decrease) when additional predictors are added to the model, even if those predictors do not meaningfully improve the model's explanatory power. This can lead to overfitting, where a model appears to fit the training data well but performs poorly on new data - for example, the (n-1)th order polynomial model discussed in @sec-ho_univariate has A SSres of zero and thus an $R^2$ of 1, but is unlikely to generalise well.

#### Adjusted $R^2$
To address this limitation, we can *adjust* $R^2$ to penalise the addition of unnecessary predictors. The *adjusted $R^2$* introduces a penalty term based on the number of predictors and the sample size, providing a more balanced measure of model fit that accounts for complexity.

::: Key-term
#### Adjusted $R^2$
$$
\text{Adjusted } R^2 = 1 - \left(1 - R^2\right) \frac{n - 1}{n - k - 1}
$$
where $n$ is the sample size and $k$ is the number of predictors. Adjusted $R^2$ can decrease when unnecessary predictors are added, making it a more reliable metric than $R^2$ for model selection.
Can be computed in R using `summary(lm_model)$adj.r.squared`. 
:::

Adjusted $R^2$ is particulary useful because of it's standardised scale (0 to 1), and the interpretation of $R^2$ as the proportion of variance explained still holds - now with the added benefit of penalising unnecessary complexity. However, it is important to note that adjusted $R^2$ is just one of many metrics available for assessing model fit while accounting for complexity. 

#### Information Criteria: AIC, BIC

Another common approach to balancing model fit and complexity is to use information criteria, such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These criteria provide a quantitative way to *compare* models, penalising those with more parameters.

::: Key-term
#### Akaike Information Criterion (AIC)
$$
\text{AIC} = 2k - 2\ln(L)
$$
where $k$ is the number of parameters in the model and $L$ is the likelihood of the model. **Lower AIC values indicate a better balance between fit and complexity**.
Can be computed in R using `AIC()` function.
:::

::: Key-term
#### Bayesian Information Criterion (BIC)
$$
\text{BIC} = \ln(n)k - 2\ln(L)
$$
where $n$ is the sample size, $k$ is the number of parameters, and $L$ is the likelihood of the model. Like AIC, **lower BIC values indicate a better balance between fit and complexity**.
Can be computed in R using `BIC()` function.
:::

Both AIC and BIC penalise model complexity, but BIC generally imposes a larger penalty for additional parameters, especially in larger samples. The choice between AIC and BIC often depends on the context and goals of the analysis. AIC is generally preferred for predictive accuracy, while BIC is more conservative and favours simpler models. 

::: Key-point
#### Interpreting AIC and BIC
Unlike adjusted $R^2$, AIC and BIC do not have a fixed scale, so their absolute values are not interpretable on their own. Instead, they are used to *compare* models fitted to the same dataset - the model with the lowest AIC or BIC is considered the best among the candidates.
:::

#### Calculating fit metrics with `broom::glance()`
In R, we can calculate adjusted $R^2$, AIC, and BIC using the `broom` package's `glance()` function, which provides a tidy summary of model fit statistics. For example, using our fitted 2nd-order interaction model of the advertising dataset:
```{r}
#| echo: true
library(broom)
glance(ads_mod_int)
```
Note that `glance()` returns a data frame with the mentioned fit metrics, along with other useful statistics such as the number of observations (`nobs`), residual standard error (`sigma`) and the global F-statistic (`statistic`). Alongside `tidy()`, `glance()` is a powerful tool for summarising and comparing regression models in R.

#### Comparing model fit

When comparing multiple models fitted to the same dataset, we can use adjusted $R^2$, AIC, and BIC to assess which model provides the best balance between fit and complexity: the key points to remember are:

- Higher adjusted $R^2$ values indicate better fit.
- Lower AIC and BIC values indicate better fit.

::: Example
#### Comparing first-order and interaction models
Lets continue our investigation of the `advertising` dataset from @sec-mlr by comparing our first-order model of Sales (`ads_mod`) to our second-order interaction model (`ads_mod_int`).

Lets combine the fit metrics from both models into a single table for easy comparison:

::: {.panel-tabset}
##### Table of fit metrics
```{r}
ads_fit_glance <- rbind(
  glance(ads_mod) |> `rownames<-`("First-order model"),
  glance(ads_mod_int) |> `rownames<-`("Interaction model"))

ads_fit_glance
```

##### Code
```{r}
#| echo: true
#| eval: false
ads_fit_glance <- rbind(
  glance(ads_mod) |> `rownames<-`("First-order model"),
  glance(ads_mod_int) |> `rownames<-`("Interaction model"))

ads_fit_glance
```
:::
Since the interaction model @eq-ads_int differs from the first-order model @eq-ads_mod by the addition of only the $TV \times Radio$ interaction term, we can interpret the change in fit metrics as the change in model fit due to adding this term. Lets go through each fit metric in turn:
- $R^2$: Since the RSE (sigma) decreases, $R^2$ increases from approximately 0.897 to 0.915, indicating that the interaction model explains more variance in Sales than the first-order model (as expected). Moreover,
- Adjusted $R^2$ also increases a similar ammount, suggesting that the improvement in fit is substantial enough to outweigh the penalty for adding an additional parameter.
- AIC decreases from approximately 315.6 to 308.3, indicating that the interaction model provides a better balance between fit and complexity.
- BIC also decreases from approximately 322.2 to 317.4, further supporting the conclusion that the interaction model is preferable.

This seems pretty clearcut: the interaction model provides a much better fit to the data (as we saw in our plots of @sec-ads_int), and the fit metrics all indicate that the improvement in fit justifies the added complexity of the interaction term. But can we improve our model further by adding more terms?

Lets fit the 'full second-order model' including all main effects and the interaction term, as well as quadratic terms for both TV and Radio advertising:
```{r}
#| echo: true
ads_mod_full <- lm(Sales ~ TV + Radio + I(TV^2) + I(Radio^2) + TV:Radio, data = ads)

ads_fit_glance <- ads_fit_glance |> rbind(
  glance(ads_mod_full) |> `rownames<-`("Full second-order model")
)
ads_fit_glance
```
once again, we see that the fit metrics all improve. 
Note that at this stage, the model has become difficult to interpret - we have lost the simple meaning of the parameters as marginal effects of each predictor. Now, the effect of each predictor depends on both its own level, its interaction with the other predictor, and the squared terms all at once.

Nevertheless, we *can* continue to add more terms, lets fit a third-order model including cubic terms for both predictors:
```{r}
#| echo: true
ads_mod_cubic <- lm(Sales ~ TV + Radio + I(TV^2) + I(Radio^2) + I(TV^3) + I(Radio^3) + TV:Radio, data = ads)
ads_fit_glance |> rbind(
  glance(ads_mod_cubic) |> `rownames<-`("Cubic model"))
```
Here, we see that while $R^2$ and adjusted $R^2$ continue
:::


## A model building workflow

### Exploratory data analysis and theory-driven model development

### Automated model selection methods

### Stepwise Regression

Stepwise methods provide automated ways to search for simpler models.

#### Forward Selection

Begin with a minimal model (often the intercept). Add predictors one at a time when they improve AIC or adjusted $R^2$.

#### Backward Selection

Begin with a saturated model containing all candidate predictors. Remove predictors that do not meaningfully contribute.

::: Note
Stepwise procedures should be treated as **screening tools**, not definitive modelling strategies. Final model decisions should consider diagnostics, interpretability, and subject-matter knowledge.
:::

---

## Exercises

::: Exercise
A scatterplot of $Y$ vs. $X$ shows curvature.

1. Fit a straight-line model and inspect residuals.
2. Fit a model including $X^2$.
3. Compare AIC and adjusted $R^2$ between the models.
4. Discuss whether the added complexity is justified.
   :::
