---
format:
  live-html:
    toc: true
---
# Model Building and Variable Screening
## Exploratory analysis for model formulation

- Start with plots (pairs, scatterplots, boxplots) to understand ranges,
  outliers, and plausible functional forms.
- Use subject-matter knowledge to posit candidate predictors and
  interactions.

## Systematic vs random variation

- Distinguish signal (systematic trend with predictors) from noise
  (unexplained scatter).
- Residual SD estimates random variation; large unexplained scatter may
  indicate missing predictors or wrong functional form.

## Choosing first- vs second-order functional forms

- Start with additive, first-order (linear) terms; add interactions or
  low-order polynomials when plots or theory suggest them.
- Prefer centered predictors to stabilise estimates when adding
  higher-order terms.

## Model adequacy and interpretability

- Adequate models fit the data (diagnostics pass) *and* support the
  scientific question.
- Avoid models that obscure interpretation with unnecessary complexity
  or unidentifiable effects.

## Parsimony as a guiding principle

- Favor the simplest model that explains the data and meets assumptions.
- Remove immaterial terms when they do not improve fit or align with
  theory; compare nested models via F-tests or information criteria.

## Interaction models with quantitative predictors

- Allow the effect of one predictor to depend on another
  ([interaction](#gloss-interaction)):
  $$E[Y] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2.$$
- $\beta_3$ shifts the slope of $X_1$ per-unit change in $X_2$ (and
  vice versa).

```{r}
int_mod <- lm(mpg ~ wt * hp, data = mtcars)
coef(int_mod)[c("wt", "hp", "wt:hp")]
```

## Graphical interpretation of interaction effects

- Plot fitted lines across a grid to see slope changes.

```{r}
library(ggplot2)
grid <- expand.grid(wt = seq(2, 4, 0.5), hp = c(90, 150))
grid$fit <- predict(int_mod, grid)
ggplot(grid, aes(wt, fit, colour = factor(hp))) +
  geom_line() + labs(colour = "HP level", y = "Fitted mpg")
```

## Polynomial models: quadratic and cubic

- Capture curvature by adding powers: $$E[Y] = \beta_0 + \beta_1 X +
  \beta_2 X^2 \;(+\; \beta_3 X^3).$$
- Use `poly()` or explicit powers; center $X$ to reduce collinearity.

```{r}
poly_mod <- lm(mpg ~ wt + I(wt^2), data = mtcars)
```

### Explore polynomial degree interactively

An Observable widget lets you adjust the polynomial degree for a simple
one-predictor model and see how the fitted curve responds. Higher-degree
polynomials can capture curvature but may also overfit.

```{ojs}
import * as Plot from "npm:@observablehq/plot";
import {slider} from "npm:@observablehq/inputs";
import {regressionPoly} from "npm:d3-regression";
import {extent} from "npm:d3-array";

let seed = 7;
const random = () => (seed = (seed * 48271) % 2147483647) / 2147483647;

const polyData = Array.from({length: 32}, (_, i) => {
  const x = -3 + (6 / 31) * i;
  const signal = 4 - 1.2 * x + 0.7 * x ** 2 - 0.25 * x ** 3;
  const y = signal + (random() - 0.5) * 2;
  return {x, y};
});

viewof degree = slider({
  min: 1,
  max: 6,
  step: 1,
  value: 3,
  label: "Polynomial degree",
});

function fittedCurve(order) {
  const regression = regressionPoly()
    .x((d) => d.x)
    .y((d) => d.y)
    .order(order)
    .domain(extent(polyData, (d) => d.x));
  const curve = regression(polyData);
  return curve.map(([x, y]) => ({x, y}));
}

Plot.plot({
  marginLeft: 50,
  height: 320,
  x: {label: "Predictor (x)"},
  y: {label: "Outcome (y)"},
  marks: [
    Plot.dot(polyData, {
      x: "x",
      y: "y",
      r: 4,
      fill: "#1f77b4",
      opacity: 0.8,
      title: (d) => `x = ${d.x.toFixed(2)}\ny = ${d.y.toFixed(2)}`,
    }),
    Plot.line(fittedCurve(degree), {
      x: "x",
      y: "y",
      stroke: "#d62728",
      strokeWidth: 2,
    }),
  ],
});
```

## When and how to model curvature

- Use scatterplots and residual-vs-fitted plots to spot nonlinearity.
- Prefer low-order polynomials for interpretability; consider splines
  for flexible shapes if allowed.

## Extrapolation risks and overfitting concerns

- Polynomial terms can explode outside the data rangeâ€”avoid predicting
  far beyond observed $X$.
- Guard against overfitting with cross-validation or an independent
  validation set when sample size permits.

## Variable screening and model selection

- Aim for models that balance predictive accuracy with interpretability.
- Preserve theory-driven terms, but remove noise predictors that do not
  improve fit or align with the research question.

## Multicollinearity: detection and implications

- Symptoms: unstable coefficients, inflated standard errors, signs
  flipping with small data changes ([multicollinearity](#gloss-multicollinearity)).
- Quick checks: pairwise correlations, variance inflation factors (VIF),
  or condition numbers.

```{r}
cor(mtcars[, c("wt", "hp", "disp", "drat")])
kappa(model.matrix(~ wt + hp + disp, data = mtcars))
```

## Akaike Information Criterion (AIC)

- Balances fit and complexity: $\text{AIC} = -2\ell + 2k$; lower is
  better ([AIC](#gloss-aic)).
- Compare non-nested models with `AIC(model1, model2, ...)`.

## Forward, backward, and stepwise selection

- Forward: start simple, add terms that reduce AIC or improve fit.
- Backward: start saturated, remove weak terms.
- Stepwise: alternate add/drop using `step()` (AIC by default).

```{r}
full_mod <- lm(mpg ~ ., data = mtcars)
step(full_mod, direction = "both", trace = 0)
```

## Limitations and cautions for stepwise methods

- Data-driven searches can overfit and inflate Type I error.
- Selected models depend on starting set and may ignore theory; always
  validate with diagnostics and, if possible, new data.

## Balancing prediction and explanation

- For explanation, prioritise interpretability and scientific
  plausibility; for prediction, prioritise out-of-sample performance.
- Consider cross-validation or a hold-out set when sample size permits;
  report uncertainty from the final, diagnostically-sound model.

## Working with qualitative predictors

- Represent $k$-level categorical predictors with $k-1$ indicator
  variables ([dummy variables](#gloss-dummy)); the omitted level is the
  baseline.

```{r}
cat_mod <- lm(mpg ~ factor(cyl), data = mtcars)
model.matrix(cat_mod)[1:5, ]
```

## Baseline category interpretation

- Each indicator coefficient compares its level to the baseline.
- Re-level with `relevel()` for more meaningful comparisons.

```{r}
mtcars$cyl <- relevel(factor(mtcars$cyl), ref = "6")
relevel_mod <- lm(mpg ~ cyl, data = mtcars)
coef(relevel_mod)
```

## Regression with multi-level factors

- Fit models with multiple factors and quantitative predictors; ensure
  design matrix is full rank (no redundant indicators).

```{r}
mix_mod <- lm(mpg ~ wt + factor(carb), data = mtcars)
```

## Mixing categorical and continuous predictors

- Combine factors and continuous terms; interaction terms allow
  different slopes by group (e.g., `wt * cyl`).

```{r}
group_slope <- lm(mpg ~ wt * factor(gear), data = mtcars)
```

## Connection to ANOVA-style hypotheses

- ANOVA table for a factor tests whether any level differs from the
  baseline (joint $H_0$ on all indicators).
- In R, compare models with `anova()` or read the factor-level F-test in
  `summary()` output when using treatment coding.
