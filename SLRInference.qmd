---
format: 
  live-html:
    toc: true

execute:
  echo: true
  warning: false
  message: false

embed-resources: true

filters:
  - custom-numbered-blocks

custom-numbered-blocks:
  classes:
    Assumption:
      colors: [FFCDD2, F44336]
      boxstyle: foldbox.simple
      collapse: false
    Example:
      colors: [BBDEFB, 2196F3]
      boxstyle: foldbox.simple
      collapse: false
    Exercise:
      colors: [C8E6C9, 4CAF50]
      boxstyle: foldbox.simple
      collapse: false
    Technical-point:
      colors: [FFF9C4, FFEB3B]
      boxstyle: foldbox.simple
      collapse: true
webr:
  render-df: gt-interactive
---

{{< include ./_extensions/r-wasm/live/_knitr.qmd >}} {{< include ./_extensions/r-wasm/live/_gradethis.qmd >}}

```{webr}
#| include: false
library(tidyverse)
set_theme(theme_bw())
```

# Inference about population parameters

So far, we have been given sample of data that we assumed came from a linear model, and attemped to *estimate* that model based on the data. However, since we only have a *sample* (and not an not the entire *population*), there is *uncertainty* in our estimates. How confident are we that our estimates, $a$, $b$ are good representations of the 'true' population parameters $\alpha$, $\beta$? Assessing this uncertainty is the subject of *statistical inference,* and will us to draw conclusions about the usefulness of our model and the reliability of the predictions it makes[^1].

[^1]: For a more general introduction to statistical inference, you may refer to the 'Inferential Statistics with R' short course. If any of the assumed content here (e.g. Normal distribution, t-tests) feels unfamiliar, please revise that course before continuing.

## Inference about the slope

In @sec-dataGeneration we generated data from a linear model
$$
y = \alpha + \beta x + \varepsilon,\qquad \varepsilon \sim \mathsf{Normal}(0, \sigma^2),
$$
with $\alpha = -3$, $\beta = 0.5$, $\sigma = 2$, and we plan to collect $n = 20$ observations. The following code defines those "true" values and a helper that can simulate a new sample whenever we need it.

```{r}
set.seed(2024)
alpha <- -3
beta <- 0.5
sigma <- 2
n <- 20

simulate_sample <- function() {
  tibble(
    x = rnorm(n, mean = 0, sd = 4),
    y = alpha + beta * x + rnorm(n, mean = 0, sd = sigma)
  )
}
```

With a single sample we obtain one set of parameter estimates. The plot below compares the fitted line from that sample to the (unknown) population line.

```{r}
sample_1 <- simulate_sample()
fit_1 <- lm(y ~ x, data = sample_1)

sample_1 %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_abline(intercept = coef(fit_1)[1], slope = coef(fit_1)[2], colour = "#1b9e77", linewidth = 1) +
  geom_abline(intercept = alpha, slope = beta, colour = "#d95f02", linetype = "dashed") +
  labs(
    title = "Sample 1: fitted line vs. population line",
    x = "x",
    y = "y"
  )
```

Generating a *second* sample from exactly the same population already gives a noticeably different least squares line. Tabulating the estimates keeps us focused on how $a$ and $b$ change from sample to sample.

```{r}
sample_2 <- simulate_sample()
fit_2 <- lm(y ~ x, data = sample_2)

bind_rows(
  tibble(sample = "Sample 1", intercept = coef(fit_1)[1], slope = coef(fit_1)[2]),
  tibble(sample = "Sample 2", intercept = coef(fit_2)[1], slope = coef(fit_2)[2])
)
```

If we continue drawing samples we eventually build the *sampling distribution* of our estimators. Running the experiment 50 times produces the following empirical distribution for the slope $b$.

```{r}
set.seed(512)
n_sims <- 50
sampling_results <- map_dfr(
  1:n_sims,
  function(sim_id) {
    fit <- lm(y ~ x, data = simulate_sample())
    tibble(sim = sim_id, intercept = coef(fit)[1], slope = coef(fit)[2])
  }
)

sampling_results %>% 
  ggplot(aes(x = slope)) +
  geom_histogram(binwidth = 0.05, fill = "#377eb8", colour = "white") +
  geom_vline(xintercept = beta, linetype = "dashed", colour = "#d95f02") +
  labs(
    title = "Slope estimates from 50 simulated datasets",
    subtitle = "Dashed line marks the true population slope (Î² = 0.5)",
    x = "Estimated slope (b)",
    y = "Count"
  )
```

-   Repeating this process produces the sampling distribution of our parameters.

-   In the real world we will not have the luxury of generating new samples by running R commands, so we have to make do with the data we have. Thankfully, if our assumptions of the underlying model are true then we can expect the same process of convergence to a stable sampling distribution to hold true and our parameter estimates for $b$ to follow a t-distribution.

### Inference about $\beta$

-   **Objective:** Derive sampling distributions for $a$ and $b$ under the model assumptions.
-   **Key results:** Emphasise unbiasedness, variance formulas, and the joint normality of the estimators.
-   **Confidence intervals:** Outline the structure $a \pm t_{n-2,\alpha/2} \cdot \operatorname{SE}(a)$ (and similarly for $b$).
-   **Hypothesis tests:** Summarise how to test linear contrasts of the coefficients using t-statistics.

### Inference about $\sigma$

-   **Distributional result:** With the assumptions above, the scaled residual sum of squares follows a $\chi^2_{n-2}$ distribution.

-   **Confidence interval:** Show how this leads to bounds for $\sigma$ using chi-squared quantiles.

-   **Hypothesis test:** Note the form of tests comparing error variance claims.

-   **Software link:** Point to the `sigma` output and degrees of freedom in R for practical calculation. \### Inference about $\beta$

-   **Hypothesis:** To assess association, test $H_0: \beta = 0$ versus an appropriate alternative.

-   **Test statistic:** Introduce the t-statistic $t = \frac{b - 0}{\operatorname{SE}(b)}$ and its $t_{n-2}$ reference distribution.

-   **Software output:** Interpret R's t-statistic, p-value, and confidence interval for $\beta$.

-   **Practical interpretation:** Translate statistical significance into statements about direction and strength of association.

## Interpreting \`summary()\` output

-   **Purpose:** `summary()` wraps the model fit, assumptions, and inference into a single report.
-   **Coefficients table:** Highlight estimates, standard errors, t-values, and p-values for $\alpha$ and $\beta$.
-   **Residual standard error:** Connect this to $\hat \sigma$ and the degrees of freedom shown in the output.
-   **Model fit metrics:** Explain multiple $R^2$ and adjusted $R^2$ as measures of explained variation.
-   **Overall test:** Describe how the F-statistic in simple regression aligns with the $\beta$ t-test.
-   **Workflow tip:** Encourage students to read the table line by line, linking each quantity back to the modelling steps above.

Interpreting each component in context helps translate the statistical output into practical insight about the data.

## Using the model for prediction

-   **Prerequisite:** Only use the model for prediction after diagnostics suggest the assumptions hold.
-   **Point prediction:** Use `predict()` to obtain fitted values for new $x$.
-   **Interval estimates:** Emphasise the difference between confidence intervals for the mean response and prediction intervals for individual outcomes.
-   **Communicating uncertainty:** Always report the uncertainty associated with predictions.
-   **Scope of application:** Warn about extrapolating beyond the observed range of $x$ and discuss potential pitfalls.

test.

## possible exercises

-   calculate residuals manually from beta coefficients
